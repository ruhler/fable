Pprof
=====
For discussion of the UI that I want.

Here's a concrete use case I feel like I want:
1. View the list of blocks by percent of time the program spent in or under
   that block.
2. Maybe we could merge together similar blocks in the list so there is less
   to look at.
3. Be able to easily 'remove' a block from that list. Which removes any
   samples that have that block anywhere in it.

So I collect together a list of interesting blocks by removing one after the
other. Now I have a high level summary breakdown of where time is going. Then
I want to be able to focus at one of those blocks at a time. Rinse and repeat?

---

I'm concerned that my perf approach is truncating stacks and not including all
the samples. Is it possible for me to write my own perf that does exactly what
I want? Anyway I can review how perf is implemented?

The implementation looks tough. But maybe I can figure out enough to parse the
output of 'perf script' directly. It looks like it is a sequence of traces. We
clearly have the timestamp. I'm just not sure what 'cycles' means. Looks like
it's a timestamp of some sort rather than a sample count. I think it's safe to
ignore for performance purposes.

Yes, so best opportunity at the moment would appear to be to parse each record
from the output of 'perf script'. Read and process the thing however I
prefer. Don't worry about the 128 frame limit for now.

---

Let's figure out the nicest output we can get from perf script.

Today it looks like:

<exe> <pid> <timestamp>.<nanos>:    <n> cycles:u:
              <addr> <name>+<offset> (<so>)
              ...

Any easy way to simplify timestamp perhaps? Or it doesn't matter? We don't
care about the header line anyway, aside from being able to recognize it.

Any way to get rid of the .so? I don't care about that, right?

I don't see any options to turn things off. Let's just parse what we get by
default. That's simplest, right?

We only care about function names, or do we care about where in the function?
Where in the function sounds nice, doesn't it? Or is that too much detail?
Unless I'm going to look into the address, let's strip it for now.

---

The truncated stacks are bothering me. Let's try:

  --call-graph dwarf,65528 

Option to perf record. See if that does anything interesting.

It's still truncating the stacks. Looks like I'm getting 256 at most.

Oh well. I'll have to find some other way to work around that annoyance.

---

Showing the lists by overall time and by self times is a really useful entry
point.

Here's the start of the overall time list:

   60.43%    15997 FbleCall
   45.77%    12116 TailCall
   45.49%    12040 _2f_Core_2f_List_25__2e_Append_21_.001f
   36.54%     9671 FblePopFrame
   34.69%     9183 GcRealloc
   32.32%     8556 NewGcValueRaw
   23.57%     6239 IncrGc
    9.04%     2393 FbleStructValueField

* FbleCall and TailCall should be ignored here
* /Core/List/Append points to a big chunk of time worth investigating.
* FblePopFrame, GcRealloc, NewGcValueRaw, IncrGc all point to same chunk
  of code being run.
* FbleStructValueField points to an interesting thing to look into.

Here's the start of the self time list:

    9.04%     2392 FbleStructValueField
    8.47%     2242 FbleCall
    6.67%     1766 FbleUnionValueTag
    6.21%     1643 Get
    5.26%     1392 FbleUnionValueField
    5.12%     1355 TailCall
    4.91%     1301 _int_free
    4.36%     1155 _2f_Core_2f_List_25__2e_Append_21_.001f
    4.20%     1112 _int_malloc
    3.60%      952 MoveTo

All of these are worth looking into individually. I wonder what 'Get' is?

Anyway, that's really useful top level info. What do we want next level to be
able to better understand? Let's look at /List/Core/Append% for example.

It would be nice to see:
1. Who is calling this. Is it one big path, a single benchmark? Or multiple? If
   it's multiple, how is the time divided between the callers?
2. If I focus on calls starting here, how does time break down into callees?
   What's the expensive part of this call?

For (1), I can break down by stack trace leading up to the append call, but
it's not very enlightening: all except three start in append, because of the
truncated stacks. I really need to get rid of those somehow. It messes
everything up.

The other three are:
* Fbld Result Do
* FannkuchRedux Flips
* Md5 Bits Lit

With no real indication of how much is contributing to each. We should be able
to tell from an fble profile run of the benchmark, because it doesn't truncate
stacks.

For (2), is it a straight up recursive problem? Filter out samples that
include the append call, drop all entries leading up to the first, then show
overall and by self?

* The overall tells us we spend 54% of the time calling FblePopFrame here.
* The self tells us 12% FbleStructValueField, 10% 'Get',
  etc.

I think it would be nice to count sample stacks and show all those too?
Assuming they don't get too long?

I feel like it would be awesome if I could compact a single trace by
recognizing cycles. For example:
  a(bc)*de*f

To cover any kind of trace like:
  abcbcbcdeeeeef

Then list the traces by count.

Thus I propose a single, useful view that you can then zoom in and filter on:
* by overall breakdown
* by self breakdown
* List of sample traces going into.
* List of sample traces starting from.

We can start by looking at the 'root' page.
If you click on an overall entry, it filters to traces with that entry and
focuses there. Multiple subsequent selections filter down more and more.

I would be interested in trying to compress and report all the samples as a
big list. See if anything interesting comes up. How do we do that?

---

Note: the --call-graph dwarf,65528 let me see inside of inlined functions,
which is kind of nice.

---

Where I'm at right now with pprof.py:
* Overall section is very useful.
  Skip over any obvious things, like start, or FbleCall.
  Anything else that stands out as you go down the list is a good candidate
  for investigation. This starts the investigation.
* Self section is very useful.
  Go down the list. All of these are good candidates for investigation.
* Canonicalization of traces.
  I think it works well. No loss of information, makes things more compact.
  Good.
* Listing traces:
  Hard to navigate, but contains some important information about who is
  responsible for calling a frame of interest that we don't get elsewhere.
  Having chains is useful here, because sometimes just one back doesn't give
  enough context.
* Tree of traces:
  Hard to navigate. I'm not convinced it's useful.

So I know how to identify frames of interest. The difficulty comes from there.
Things I want to know after I have a frame of interest:

* Who calls the frame. Are there different cases to consider? If so, which
  case is the biggest case I should consider first.
 - Note that looking a single frame back is not always enough, but looking too
   far back also makes things difficult. Somehow we want to see just enough
   context for the caller to distinguish between interesting cases.

Let's say I magically solve the context question. Is there anything else I
need? Anything else I want?
* I suspect the ability to recursively investigate:
  We start at the root.
  Then we say we are interested in A.
  Within A, we want to say we are interesting in B, but still show that
  relative to A, not too root.

  This should be easy. We just specify a sequence of frames of interest. Each
  frame in tern filters out samples.
* Do we want a breakdown going out of a frame like we want a breakdown going
  into a frame? Or is Overall / Self focused on a frame sufficient for that?

It seems like there are two different ways to view incoming or outcoming
edges:
* The Overall/Self breakdown, doesn't look at direct edges, but focuses on the
  biggest cuts.
* Context traces looks at specific edges.

If both are useful, presumably both are useful both directions: incoming and
outgoing.

I know that just having 'overall/self' doesn't give me information about who
immediately calls a function, which I think is useful in some cases.

---

Idea: Focus on sequences of frames rather than individual frames.
* Keep canonicalization of samples. I see no loss of information there.
* Count occurrences of every observed subsequence of frames.
  Make sure to count each subsequence at most once per sample.
* We can do Overall, Self based on that if we wanted.
  Though I assume the longer the subsequence, the smaller the overall value.
  So we can skip this if it's too expensive.
* We can do a graph view based on that.
  A mini section for each subsequence.
  It shows frames leading into the subsequence, broken down by frame.
  It shows frames going out of the subsequence, broken down by frame.

One more thing we want is filtering. You can partition any profile by a
subsequence into samples with the subsequence and samples without. So, your
filter is a list of subsequence of samples to keep and a list of subsequences
of samples to reject.

The graph might be expensive. If samples are limited to 128 in length, that's
O(128^2) * N possible subsequences (though we expect much less in practice I
hope).

An implementation approach we can try:
1. For each sample:
 i.   Canonicalize the sample.
 ii.  Compute the set of subsequences of the sample.
 iii. Increment overall count of subsequence ==> count mapping by one for each
      subsequence in the set from the sample.
2. For each subsequence in the overall set:
  incoming[subsequence[1:]][subsequence[0]] = count
  outgoing[subsequence[:-2]][subsequence[-1]] = count

Sort subsequences by overall count.

Now we can show:
* Overall by subsequence. Optionally limiting to length 1 subsequences
  depending on what's useful. Maybe better would be limiting to subsequences
  taking up more than 1% of overall time?
* Self by subsequence (meaning when the subsequence is the tail). Maybe
  limiting to subsequences with self time greater than 1%?
* Graph of subsequences like how we do current profile report.

Basically the same as our current profile report, except:
* We canonicalize samples first.
* We track based on subsequence instead of frame.
* We add the option for filtering based on a set of subsequences to accept or
  not.

To get a feel for how useful it is, no need to add filtering to start.

Now, how I expect this to be used:
* Go down list of overall or self to find a subsequence of interest.
* To see who is calling into that subsequence, search in the graph for the
  subsequence, traverse up it. We have as much context available to use as we
  want.
* To see what the subsequence is doing, search the graph for the subsequence,
  traverse down it. Or, consider filtering on the subsequence first, then
  repeat the flow from the beginning.

I think this will be good. I think it will be expensive. We can limit in
various ways based on needs. For example, choose a subsequence first, then
only show graph for nodes starting or ending in that subsequence.

Note that this removes the need for separate canonicalized sample section:
that will show up in 'overall' and 'self' views with subsequences. I think
it's an improvement over the tree view, because it focuses on a single level
at a time in the graph view, which is what I really care about, and it let's
you view the tree from the root, the leafs, or anywhere in between you may be
interested in.

Next step: draft the pprof.py code for this in a small sample profile and see
how it looks.

---

Apparently it's very expensive to do all these subsequences in python. That's
concerning. Except it's not the sequences that are super expensive, it seems
like it's something else. I'm a little confused, because I don't think it was
this slow before.

Yeah, something is definitely wrong. I was able to do the same thing way
faster before.

I see what I did wrong. I forgot to clear the frames after each sample. Okay.
Let's try again then.

Yes, now performance is okay getting counts of all the subsequences.

Next step is to try producing the graph view.

---

The graph view is going to be good I think. Some requested improvements:
* Need an easier way to link to a specific sequence.
* Figure out a better way to highlight the sequence in the graph entry.
* Consider limiting incoming and outgoing edges with N, % like elsewhere?

Honestly, if I can turn this into a webserver with links that shows a single
graph entry at a time, that would be awesome I think.

---

I see three directions I can take this profiling stuff:
1. pprof.py as a webserver. Iron out the interface to make it useful.
2. pprof implementation in fble. Move towards what we have in pprof.py.
3. profiling in runtime. Collect samples instead of aggregated counts.

(1) is presumably most important, because it tells us exactly what we need.
(2) is interesting because it gives us a more meaningful case study to try (1)
out with. (3) can wait. So let's go in order.

---

Proposal for pprof.py website:
* /overall page - gives the list broken down by overall frame.
  Each frame links to the graph node for that single frame.

* /self page - gives list broken down by self frame.
  Each frame links to the graph node for that single frame.

* /graph&id=... - gives a single graph node
  List the stack trace to that point.
  List the incoming edges. Edge links to trace with that incoming added.
  List the outgoing edges. Edge links to trace with that outgoing added.

Try that. See how useful it is.

Oh, also, I feel like I want a kind of self time for a sequence. That would
be: time spent in the sequence not covered by incoming or outgoing? Not sure
that makes sense. Don't worry about it for now.

Step 1: overall page. How do we do http server in python?

---

I got the http server going now, with Overview, Overall, Self, and graph
views. I think this is good. There are some navigation issues to work out,
where I want to go to a specific subsequence without an easy way, and there
are opportunities to provide more information. But it feels good.

I can pretty easily see the tree view I was hoping for, the breakdown by
benchmark within fble-benchmark. Maintaining the context of traversal is what
makes that work, because we can go through FbleCalls without losing the
context.

Cool. It's in good enough shape now to try using it for something for real
before making more changes.

Two things next:
1. Re-implementing pprof in fble.
2. Updating fble profiling to track and output samples instead of aggregated
report.

(1) should be pretty straight forward. I assume we'll instantly run into
performance issues, but that's fine. That's what we need to figure out how to
fix.

(2) is going to be tricky. I think it's doable. The main challenge is I think
we need to support on-the-fly canonicalization in order to handle potentially
unbounded tail recursion. Single frame canonicalization isn't hard. But
multi-frame loops need some way to track where we came from and where we
should go back to when popping frames.

The good news is, (2) isn't blocked on (1). I can switch over entirely to
pprof.py now. Or, at least, once I've used it enough to verify it's not
missing something major we used to have. But, honestly, it's not, right? It
has all the same information we had before in the graph, just more of it.

So, what say you? Time to rewrite pkgs/pprof?

---

I kind of want to throw away everything that's there and start from scratch.
What say you? Shall we?

It's hard to design with such a big fear of performance issues. I propose we
parse a perf script into a map from sequence to number of occurrences of that
sequence. I can output a canonical format basic report, for example, which is
<count> <sequence>, one sequence per line.

I honestly don't believe I'll be able to do that much without running into
significant performance issues. We can play around with different
canonicalizations: none, actual canonicalization, or always return a single
sequence. Just to see what makes a difference for performance.

Okay, so let's think of this, to start, as a conversion tool to convert perf
script output into my text based pprof format. Don't worry about multiple
metrics at this point, just one metric for now.

---

Remember how to parse perf script:
* If line starts with tab, read the frame
  Of the form:  <space><word><space><name>+<addr>...
* If line is empty, save the sample.
* Ignore all other lines.

Let's go the getline then parse route, okay?

---

I can parse perf script now. Let's see how terribly slow it goes. Because if I
can't even get through parsing, there's really no point in working on anything
else.

Time to parse my 11K sample perf script input, without any canonicalization:

We run out of memory after 2 minutes. Why is this running out of memory? There
shouldn't be a memory leak anywhere I don't think? Did I mess something up?

The only thing it could be is the map of samples. Let me try fake
canonicalizing and see if we avoid storing the samples that fixes the memory
leak, and how long it takes to record all the samples in that case.

For baseline, it takes pprof.py 90 seconds to parse and process the input.

The memory growth goes away in fble if we don't save any of the sequence in
the map. It must be that the map takes a lot of space. Hopefully
canonicalization of frames will help. But before we get there, baseline,
performance is looking like: ... not good. Let me let it run for a while.

It got through after just under 12 minutes.

---

First attempt at using pprof.py to debug performance of fble pprof:

Self:
* 20% FbleCall
* 17% IncrGc
* 6% Int Map lookup
* 4% new func value. Is that expected?

Overall:
* 37% in GcRealloc
* 20% GetChar

Nice to see that ParseFrame is really cheap.

I'm thinking we have a slightly pathological case with GcRealloc it would be
good to understand in detail.

Looking at FbleNewFuncValue, that comes from:
* FbleCall? That must be from PartialApply. Which is coming from GetLine
  apparently.

The problematic path looks like GetLine + GetChar + State monad.

So, maybe I should play around with cat and fast cat, and go back to trying to
optimize for performance.

Or, implement canonicalization. See if that fixes the memory issue. Wait out
the slow parse. I mean, 10 minutes isn't too terrible, right?

---

My gut feeling is the problem is with recursive GetLine. Two things I want to
try:

1. Don't use GetLine. Parse char by char.
2. Understand in detail how GetLine looks at runtime.

Trying (1)... I'm hoping it goes twice as fast because we have at least half
as much string being built up and read from the input stream.

5:39.58

That's good. That's twice as fast. Cool.

---

With canonicalization implemented, we can process my input file in 13 minutes
23 seconds. Awesome. It uses maybe 50% memory on my computer.

---

Feature request: when I show a sequence, show cost per frame. But in a
specific way. For example: take the sequence a->b->c->d.

Next to 'd', I want to show number of samples with the sequence a->b->c->d.
That's easy.

But that doesn't show as much time as I expect for the overall sequence. I
think next to c and want to show total time in a->b->c, and next to b all time
from a->b and next to a, all time in a.

That should be pretty easy, right?

Yeah, that's super useful. For example:

   21.45% 7757 _2f_Core_2f_Char_2f_Ascii_25__2e_Chr_21_.0077
   21.03% 7605 FbleCall
   18.56% 6711 TailCall
   4.54%  1641 _2f_Core_2f_Int_2f_IntP_2f_Map_25__2e_Lookup_21_.0021

This is the entire sequence from the Chr function. It tells us of the 21% time
spent in the overall program due to Chr, it's almost entirely dominated by
TailCall. Imagine what we could do if we could somehow directly tail call from
within the Lookup function?

---

Decision is to stop worrying about performance for this release, clean up the
rest:
* Switch things to work on a simple pprof text format.
* Add conversion from perf script to pprof text in python and fble.
* Change pprof.py and fble pprof to load from pprof text format.
* Set up a benchmark for fble pprof.
* Replace pkgs/pprof with pkgs/pprof2.
* Write the server implementation for fble pprof.

---

I think it's time to sit down and have a serious discussion about the pprof
file format.

Today I have a list of lines of <time> <name>;<name>;...;<name>.

The problem with this:
* No way to output count in addition to time.
  - Which is certainly useful for the fble-profile-test.c debugging output.
  - Which may or may not be useful for an actual user.
* No way to output extra info about blocks.
  - Namely line and file number.

Non-problems with this:
* For long names, it's not very compact. Honestly, I don't see that being a
  problem in practice at this point. Maybe some day, but it's not a compelling
  reason for me to do something now.

Possible output formats:
* The one I have now.
* A custom text-based format with support for count and block info.
* google/pprof text proto format.
* google/pprof binary proto format.

It bothers me that I can't figure out any way to generate a google/pprof file.

It looks like there are some sample google/pprof files as testdata when you
check it out, along with a text representation of the contents. That text
representation does not use text proto format.

The google/pprof format is more general than what I need. That's fine for
outputting to that format. But it makes it harder for implementing a viewer
for the format, because there could be things I don't want to support.

Benefits of using google/pprof format:
* I don't have to design my own format. Just go with what's there.
* I could in theory view the profiles with google/pprof implementation for
  comparison. Not that I would, but other people certainly might prefer that.
* I could in theory view google/pprofs from elsewhere with my viewer.
  Depending on how fully featured my viewer is.

Downsides of using google/pprof format:
* I have no control over changes to the format in the future.
* It's more complicated than I need: 
  - Having to generate and parse proto format sounds hard?
  - Supporting additional features in my viewer sounds annoying.
* It's hard for me to test that my interpretation of the google/pprof format
  matches the rest of the world, because I can't figure out how to generate a
  reference pprof.
* It adds a dependency on google/pprof, if only in terms of specification.
* If google/pprof doesn't official support a text format, that means I can't
  generate a text format file, which I much prefer to have so I can view and
  debug things in a text editor.

Let's be honest. The real immediate concern here is how hard it is to generate
and parse google/pprof files. How about we try that. If it's not too hard, use
it. If it's too hard, make up my own format.

To try that...

1. Figure out how to run google/pprof to show something useful.
2. Get google/pprof to show something useful on a text based profile.
3. Try to generate a text based google/pprof from fble and open it in
google/pprof.

I don't mind saying that fble profiles are a subset of google/pprof to start,
to simplify implementation.

Okay? Let's try it.

Well, wait. Let's say I had my own format. It's as easy as I want to generate
and parse. But I keep it aligned with the kind of information google/pprof
needs. Then anyone who wants could write a converter between my format and
google/pprof, and that converter is as hard as dealing with the google/pprof
format. How about that? I get everything I want, without the headache of being
tied in any way to google/pprof?

Both approaches seem worth exploring.

There's a debian package called google-perftools. Is that related? Looks like
it. That installs something called google-pprof. That's cool.

Sample pprof file to try running it on:
  pprof/profile/testdata/java.cpu

It just says 'Did not specify a profile file'. I have to assume my sample
profile is bogus.

Maybe I need gperftools package instead of pprof?
  github.com/gperftools/gperftools

Maybe apt install libgoogle-perftools-dev?

Yeah, that looks like the same as gperftools.
* link with -lprofiler
* run with CPUPROFILE=foo.prof

Doesn't work for me. Hmm...

Okay, manually call ProfilerStart and ProfilerStop, including
gperftools/profiler.h works.

And I can view that with google-pprof. Good. Now, what's the format? How can I
get a text output format?

It's odd that the cpu profiler output format description is completely
different from the pprof format description.

Ah. I see. The cpu profiler generates a legacy format. pprof knows how to
convert that to its proto format if needed. Which is not really useful for me
being able to view proto format pprofs.

I think I need to figure out how to run pprof directly, not google-pprof,
which I assume runs on the old format files?

Cool. I got it:

  ~/go/bin/pprof -text ./profile/testdata/java.cpu

So yeah, can't use google-pprof. Oh well.

pprof -raw generates text output in what looks like legacy format. pprof can't
read that in.

pprof -proto outputs in compressed proto format. How do I convert that to
text? Does pprof take text proto input?

So, not only does pprof expect binary encoded protobuf, it expects gzipped
binary encoded protobuf. Based on looking at the code. It also supports some
legacy formats, but doesn't promise to continue doing so.

No, none of those legacy formats are really what I want, and it seems
pointless if pprof is going to drop support for those. I definitely don't want
to output in compressed encoded binary format.

Which leads me to my own format.

There are two different use cases to consider:
1. fble --profile, where we have counts and file and line number info.
2. fble-perf-profile, where we don't have meaningful counts or file and line
number.

How important are counts? Seems like things could be simplified a bunch if we
didn't have to deal with them. I know they are useful for testing counts, but
we wouldn't need to test them if we didn't have them.

I feel like I very rarely look at counts. Like they are useful in theory, but
not as much in practice. Use cases:
* Code coverage. Just need to know 1 or 0.
  I could probably use time for that, now that I no longer random sample?
* Telling the difference between lots of calls versus one long call.
  Like, is FbleNewFuncValue called a ton of times, or does it just take
  forever? That really ought to be useful information.

If we had counts, how would we want to view them? I think differently from
time. For example, 'overall' counts is not meaningful. You don't want the
number of calls that happened within a function recursively. Counts are only
useful for counting in 'self' mode. Basically counts of suffixes of a
sequence.

So I would want a different view. Not 'seqs' in general, but ends of
sequences. Or have an option to say '$' as the next element in the sequence to
specify the end, so I can force that, then look at counts.

I think counts are useful in theory. I want to give them a chance.

Now, pprof.py could either know about counts and display them along side time,
or treat count as just some other kind of thing that was sampled that you can
switch between.

The information I want in my output format:
* If generalized to counts of all sorts of different things, the names of the
  metrics. e.g. 'time', 'samples', 'count'.
* For each sample:
  The values of the metrics.
  The sequence of blocks.
* For each block:
  The name of the block.
  If available, the file, line number, and column number of the block.

I probably should not call my profile format pprof, to avoid confusion. Maybe
fblp instead? Sounds reasonable, no? Or "fble profile"?

Let's play around with some possibilities.

  metrics: count time
  
Hmm... I still wonder if I could use pprof format, but do a text proto? It has
nice things like units.

Let's try to convert the sample profile I have to text format somehow.
Manually if I need to, worst case.

Looks like protoc can decode into text format. Let me get that installed.
Package: protobuf-compiler?

Yes. Got it:

protoc --decode=perftools.profiles.Profile proto/profile.proto < java.cpu.proto  > java.cpu.proto.txt

Note, this has to be done on the uncompressed proto file, not the compressed
proto file.

Good news is, now I can test generation of profiles, converting into binary
proto, converting into gzipped, opening with pprof.

Let's see a sample of how text proto might look.

  sample_type { type: 1 unit: 2 }
  sample_type { type: 3 unit: 4 }
  sample { location_id: 1 location_id: 3 location_id: 2 value: 1 value: 20 }
  sample { location_id: 1 location_id: 3 location_id: 3 value: 1 value: 30 }
  location { id: 1 line { function_id: 1 line: 12 column: 4 } }
  location { id: 2 line { function_id: 2 line: 13 column: 5 } }
  location { id: 3 line { function_id: 3 line: 14 column: 6 } }
  function { id: 1 name: 5 system_name: 5 filename: 6 }
  function { id: 2 name: 7 system_name: 7 filename: 8 }
  function { id: 3 name: 9 system_name: 9 filename: 10 }
  string_table: ""
  string_table: "count"
  string_table: "count"
  string_table: "time"
  string_table: "instructions"
  string_table: "foo"
  string_table: "Foo.fble"
  string_table: "bar"
  string_table: "Bar.fble"
  string_table: "sludge"
  string_table: "Sludge.fble"

I'm not sure why functions and locations both have line numbers associated
with them.

Yeah, so text based is pretty verbose here. How bad would it be to generate
encoded protobuf directly?

Things involved in protobuf encoding:
* Variable length int representation: uses 7 of 8 bits, with 8th bit saying if
  there is continuation. Sounds straight forward enough.
* Records are: field name + type + value. With a straight forward enough
  encoding.
* Signed integers are packed around 0 rather than use 2's complement. But I
  don't think I need to use signed integers for anything.

So, actually, not that bad. I think that's doable. We'll want a helper for
outputting varints. I'll need to hard code the message field ids. I can have
conventions around how I assign indices to the string table.

I think I would want a separate source file to implement this. Maybe call it
pprof.c. It would be a glorified query over the profile.

Where does this leave us? I think generating uncompressed binary encoded
.pprof format is feasible. The pprof proto format certainly supports
everything I need.

Sounds like we should use absolute line number in location, it's in the file
for the function. The function line number will be set to the same thing in
this case.

If I generate uncompressed pprof encoded proto files...
* You could gzip them and then view in pprof easily.
* I need some way to parse them.

For python code, I could use protoc maybe to generate the proto parser part.
For fble... shouldn't be too hard. Might be a useful utility in general.

So, I guess the proposal is to try to use uncompressed pprof binary encoded
proto as my profile format:
* Implement a pprof.c file to output the proto format. It can handle both the
  pprof and proto parts.
* In pprof.py, probably want to use the output of protoc for parsing. I can
  check in the generated code and inline it maybe? Sounds messy. Or try to
  parse what I need to by hand.
* In fble-pprof, parse by hand. Shouldn't be too bad.

Sounds like a plan. How do we get there?

1. Hack up pprof.py to support proto format making use of protoc generated
code.
2. Try it out on my above hand generated text proto, which I encode with protoc.
3. I can try out pprof on my sample too, after doing gzip of the encoded
value.
4. Implement pprof.c to generate in pprof format.

I should now be able to generate and view profiles usefully. Try it out with
some pprof something or other. For now, don't worry about multiple metrics or
location info in pprof.py. We can extract the 'time' metric explicitly.

Note: this isn't going to be useful for debugging fble-profiles-test. I should
use a custom report generator for that. It doesn't care about blocks.

Otherwise, if I was just going to do a simple text format, I guess I may as
well assume it always has count and time and output a list of samples and a
list of blocks. Something like:

1 20: 1 3 2
1 30: 1 3 3 
1: Foo Foo.fble 12 4
2: Bar Bar.fble 13 5
3: Sludge Sludge.fble 14 6

Basically it's a list of sample lines followed by a list of location lines.
You tell the difference by the number of numbers before the ':'. The file
names and line and column info is optional.

Any more obvious way to write the sample values?

I would say, don't worry about it.

Okay, so now I have a proposal for a simple text format. It's useful for debug
output. It's text readable. It's super easy to generate. It's super easy to
parse. It's fairly compact.

Why would I go to all that trouble of supporting pprof if I can just use this?

The key assumption here is we have 2 metric values to output: count and time.
Is that a bad assumption?

What if we said block definitions come first, samples second? Seems more
useful for the parser if we want one pass, right?

I could require the ids be packed, so we don't need to enumerate them that
way. Except explicit enumeration really helps when looking at the file
manually. Or... we could just use line number? That sounds reasonable. Block 4
is output on line 4. Don't output block 0, because? Well, that's slightly
tedious.

I think it's fine to put the explicit block numbers.

How do we denote end of the list of blocks?

Oh, you know what else is useful to have? Total number of samples / sequences
and number of blocks? I can track total number of samples (not total number of
sequences? though I could track that too) in the profile easily enough. But
total count? Total time? Dunno.

  1 Foo Foo.fble 12 4
  2 Bar Bar.fble 13 5
  3 Sludge Sludge.fble 14 6

  1 20 1 3 2
  1 30 1 3 3 

Aha. That's it. Blocks, with explicit id. Let's say the id could be anything,
doesn't have to be 1, 2, 3, ... if we don't want to. It just has to be unique.

Then a blank line. Then samples.

I just wish it were more obvious where the stack starts and what is count
versus time. Add a header? Add some header lines? As comments perhaps?

  # Blocks: id name file line column
  1 Foo Foo.fble 12 4
  2 Bar Bar.fble 13 5
  3 Sludge Sludge.fble 14 6

  # Samples: count time <block>
  # From outer most to inner most.
  1 20 1 3 2
  1 30 1 3 3 

Yes. I like that. That's nice. Document with comments. Now it's human
readable, we know what's there. Super easy to parse. Super easy to generate.
We keep all information.

Let's say no difference for fble profile versus fble-perf-profile. Use ???:0:0
for file and line and column. Output a count.

Now, do I want to output a bunch of sequences with 0 time? Because every
sub sequence has a non-zero count. The output file could get big, right? Too
big? Let's see. Try on md5.

It's a 34M file, with 34K lines. Since we don't do random sampling, there is
no such thing as a 0 time entry anymore anyway.

I think it's fine. It will be even better when we list block ids instead of
the full names.

So, great. I have everything I need. A nice, compact format that works in all
cases, is clearly documented, easy to generate and parse.

I probably shouldn't call it pprof. I like 'fble profile'. fble-perf-profile
makes sense. fble --profile option makes sense. So what should I call my
profile viewer? pkgs/profile?

What does pprof stand for, anyway? "Performance Profiles".

How about fprof?  fble-profile? Or, simply, pkgs/profile? fblp? Naw. Just call
it 'profile', that's fine. We can do renames later when that becomes
confusing.

Cool. I have a plan. Steps:

1. Update lib/profile.c to generate the new output format.
2. Update pprof.py to parse the new format. Focus on time.
3. Update pkgs/pprof2 to parse the new format. Focus on time.

That's it. Easy.

I suppose one other thing is to decide on my terminology. 'sample' isn't
really right for --profile option to fble.

pprof uses 'Sample', or 'call stack'.
gdb has backtrace.
wikipedia redirects backtrace to stack trace.
 alias: stack backtrace, stack traceback
man backtrace: is what libc uses.

Perhaps 'back' refers to the order of the trace: inner most to outer most?

So I shouldn't use 'back'. But 'trace' is not at all obvious. Maybe 'site'?
Like, call site? I use 'sequence' some times. I dunno. Think on it.

---

I think call stack is a good term. Because this isn't a sample, or a back
anything. I don't like the word trace. That leaves us with call stack.

What do you call a partial call stack though? Like an intermediate? How about
a call sequence? "Call sequence" is an interesting term. Maybe it makes more
sense than 'stack' in the case of canonicalization?

I like call sequence. We can have a full call sequence or a call subsequence.
How can I shorten that for variable names though? 'calls'? 'seqs'? 'call'?
'seq'?

I don't know. Maybe let it stew some more.

Onwards now to my new fble profile format.

---

Well, it's not very human readable, to be honest. The indirection for blocks
makes it just a bunch of data.

There are problems with whitespace in names too. Hmm...

Maybe we want a binary format instead? What would that look like?

uint64_t num_blocks;
for each num_blocks:
{
  string name;
  string file;
  uint64_t line;
  uint64_t column;
}
for each sequence:
{
  uint64_t count;
  uint64_t time;
  uint64_t length;
  for each length {
    uint64_t block;
  }
}

Where string is: { uint64_t length; char[length] }

Let's see what that looks like?

It takes up more space if I'm using uint64_t for everything. May as well just
use ascii. That way people at least can see what's there.

I suppose I can see why gzip makes sense for pprof proto format: lots of call
stacks will share similar sequences. And string names of things too. I don't
want to take a dependency on gzip though.

Let's stick with my proposed text based fble profile format. It may not look
pretty, but it is decipherable just by looking at the file in question.

Oh, but what do I do about whitespace in names? That's going to mess
everything up.

Sigh. I'm feeling more like I should try pprof format. It doesn't have these
issues. I can special case fble-profile-test to output things pretty like.

Blarg. Let's sleep on things. See how I feel.

---

I want to try generating pprof output. This is exploratory. Things may get a
little messy for a while. That's okay. We have everything it revision control
and can go back as needed.

Focus on generating pprof output first. Do not worry about reading it. Focus
on implementing the code, don't worry about API or testing so much. The
primary goal at this point is to figure out if it's feasible to go the pprof
route. Worry about the rest later.

Proposed plan of attack:
1. Make blocks a public field of FbleProfile. It will just make everything
easier.
2. Add a todo to remove no longer needed APIs for accessing the blocks data of
a profile.
3. Add a todo to change the FbleOutputProfile function to take a file name
instead of a FILE*. We will not be wanting to concatenate or output to stdout
these pprof files.
4. Draft the implementation of the pprof.c.

At this point I should be able to run the fble-perf-profile test to generate a
simple profile with known values. Check if I can decode it with protoc. Check
if pprof can handle it.

Once we get that working, we can move on to the next phase:
* Implement the todos from the first phase.
* Ditch the old pkgs/pprof code.
* Start working on the new pkgs/pprof code based on the pprof format.

I think I don't care about pprof.py anymore. It would need to be rewritten for
both input format and performance sake. I don't fancy parsing pprof in python
or using the generated python from protoc. I say go straight to fble for the
implementation. We can resurrect pprof.py later if needed for performance, if
fble can't keep up. I think there's a decent change fble can keep up, but also
would not be at all surprised if it can't.

---

Sketch for how pprof.c implementation might look:

* Add helpers for things as needed. Like outputting a varint, outputting a
  string, outputting a record. No need to plan ahead on those.
* hard code the two sample types.
* location id = block id, or plus 1 if needed.
* function_id = block id, or plus 1 if needed.
* strings are: "", metrics, followed by block strings: 2 per block: name and
  filename.

It really sounds quite straight forward with that mapping.

Step 1, figure out how to output sample_type:
* field id is 1
* value is ValueType
* it's a repeated field.
* ValueType is int64 type = 1, int64 unit = 2

How to express this as a sequence of records? Let's try to figure it out on my
own first, then we can see what protoc does for comparison.

I would guess:

  sample_type { type: 1 unit: 2 }
  sample_type { type: 3 unit: 4 }

Turns into:

(1, ???)  to say this is the sample type value.
(1, count_sample_type_string_id) to give the type of the first
(2, count_sample_unit_string_id)

(1, ???)  to say this is the next sample type value.
(1, time_sample_type_string_id)
(2, time_sample_unit_string_id)

What goes into that ???. It must be the length of the next two records, to
make that easy to skip over. Yes. The slightly annoying bit will be to keep
track of that length. Especially with my varint and record helper functions.

The length in this case is the sum of:
* varint length of '1' << 3.
* varint length of sample type string id
* varint length of '2' << 3.
* varint length of sample unit string id.

Maybe easiest would be to write a helper function that I pass a list of
varints, and it tells me how much space they take up. Or tell me the length of
a varint, tell me the length of a tag, etc. And I just some it all up to
start, then output. Okay?

Sounds reasonable. Let's get going.

---

Okay! Draft of the code is done and compiling. Let's see what I got and where
I messed up.

First step, try decoding with protoc.

protoc --decode=perftools.profiles.Profile -I ~/scratch/pprof proto/profile.proto < fble-perf-profile.test.got 

Well, it got the string table entries okay. Nothing else though. I wonder why.

Maybe let's comment out code and generate little by little to see where I'm
going wrong.

Here's what it is reading for the SampleType fields:

1: 4      (sample_type, length 4)
1: 1      (type, string_id 1)
2: 2      (unit, string_id 2)
1: 4      (sample_type, length 4)
1: 3      (type, string_id 3)
2: 4      (unit, string_id 4)

Looks like what I would expect, no?

The raw bytes are:

  08 04 08 01 10 02 08 04 08 03 10 04

Which I read as:

  1, 4, 1, 1, 2, 2, 1, 4, 1, 3, 2, 4

Just like it did.

Let's compare to what protoc would generate for this sequence.

  0a 04 08 01 10 02 0a 04 08 03 10 04

It's using 0a instead of 08. That makes sense. It should for length, right?

Cool. That was it. In terms of the proto encoding it looks good now.

Next step is to open it in pprof I guess?

Looks like it reads something, but the contents are wrong. Something's wrong
with the block ids. Yeah, I forgot to use them. Also, I need the leaf at index
0, so it's a reversed sample from my point of view.

---

Something weird is going on with <root>. Is pprof interpreting the braces
somehow?

Yes. That's silly. Why would it do that? What should I use instead?

Let's go with "[root]" instead of "<root>". It matches "[unknown]" output by
perf at least.

---

Okay! We have it. Easy enough. Done.

Where does this put us?
* The code to generate pprof proto format is not bad.
* I have test and API cleanup to do.

Let's get the test cases to pass first? Do a massive round of cleanup before
worrying about how to parse and view the profile?

---

My md5 profile, which was 34MB, 30K? It comes out as 4MB uncompressed and 256K
compressed now.

---

How to test fble-perf-profile now?
* I could check in a 'golden' reference, but that feels pretty arbitrary.

I can I instead write a small program to generate the reference file with
manual C api calls? That's what we want to test, right? That parsing is
correct. We don't care about profile generation itself. That's not what I want
to test.

---

Miscellaneous thoughts:
* I should call the sample types ["samples", "count"] for time, and
  ["calls", "count"] for calls. Everywhere in the code. "samples" corresponds
  to FbleProfileSample function, and technically it is right for sampling
  every instruction.

* To test fble-perf-profile, compare the output against a test program that
  generates the profile with explicit FbleProfile* calls and outputs the
  result. Debugging test failures will be a pain, but fble-perf-profile only
  outputs pprof format now, so what can I do? Unless I want to add an option
  to output in a more human readable form and use that only for test? That
  might be worth doing actually.

* I need to test pprof generation. I vote for writing a test app to generate a
  profile with known values and compare that against a pprof file generated
  manually via protoc from a text based proto input file. Unfortunately we
  probably better not reuse this same profile for fble-perf-profile, because
  it should have file and line number info.

* Passing FILE* instead of filename might still be desirable, for things like
  fble-perf-profile where it's just way more convenient to output to stdout
  and redirect on the shell than have to parse arguments. Not sure how to deal
  with opening the FILE* in binary format in that case. Worry about it later
  when something fails, presumably on windows?

Let me get the existing test passing first. Hmm... Honestly, a custom output
format might be nice, even if for test. It could be --test as the command line
option? Just output time; name samples. To test the parsing.

---

How hard would it be to output gzip compressed? What's the API for that look
like? I'm just curious.

Looks like what I want is from zlib:
* gzopen, gzfwrite

If I used those, it looks like it wouldn't be bad at all.

Debian packages: zlib1g, zlib1g-dev. Maybe worth a try. Let's worry about that
later though, for when I get my reader going. The reader will surely be easier
to start without the decompress step.

---

Next step is to write a test case for pprof generation. After that, I think
it's worth diving into the viewer side of things. Don't worry about
miscellaneous API cleanup at this point. I need to make the profile viewer
work in practice, otherwise I'm stuck.

---

The test approach not working. There is some difference when I decode and then
re encode the proto.

Here are the first bytes of the initial value:

  0a 04 08 01 10 02 0a 04 08 03 10 04 12 06 08 01

Here are the first bytes of the re-encoded value:

  0a 04 08 01 10 02 0a 04 08 03 10 04 12 07 0a 01

I have a suspicion the difference has to do with encoding of default values.
Let's decode manually and see.

SampleType entries match:
  0a 04: sample_type, length 4
  08 01: type: 1
  10 02: unit: 2
  0a 04: sample_type, length 4
  08 03: type: 3
  10 04: unit 4

First sample entry does not match:
  12 06: sample, length 6
  08 01: location_id, 1
  10 01: value, 1
  10 00: value, 0

Versus:
  12 07: sample, length 7
  0a 01: location_id, length 1
  01:    location_id: 1

  12 02: value, length 2
  01     value: 1
  00     value: 0

  12 08: sample, length 8
  0a 02: location_id, length 2
  02     location_id 2
  01     location_id 1

  12 02  value, length 2
  01     value: 1
  00     value: 0

Yes. So the issue is with packed fields. I should fix this. Any time we have
repeated of a primitive type, I should use the packed format.

Where this is relevant for me:
* location_id in Sample
* value in Sample

That's it. Just those two. Okay? Let's implement that, see if it gets the test
to pass.

Okay, so it makes progress. Then we run into a different issue. Let me see if
I can track that down. It may be harder, because it's further into the file.

It's after all the Sample entries.

I'm outputting:

   22 0b 08 01 22 07 08 01 10 b7 02 18 00 22 0a
08 02 22 06 08 02 10 0a 18 0e 22 0b 08 03 22 07

The reference has:
   22 09 08 01 22 05 08 01 10 b7 02 22 0a 08 02
22 06 08 02 10 0a 18 0e 22 0b 08 03 22 07 08 03

This is for the encoding of location. Let's parse it out.

The reference:
  22 09  location, length 9
  08 01  id 1
  22 05  line, length 5
  08 01  function_id 1
  10 b7 02  line 311
  Note: No column

  22 ...

So, probably due to default for VarInt being zero. If we have a tagged var int
with value 0, just skip it maybe? Let's try it.

Cool. That's it. The test passes now.

---

Last bit to make all this profiling stuff feature complete: implement the
pprof viewer. I want to start in fble. How should I approach it?

An important architectural question is if I want to limit myself to fble
generated pprof, or I want to support pprof format in general. The big
difference in practice: fble pprof is, for now, always going to have 'calls'
and 'samples' SampleTypes. A general pprof will have other stuff, and who
knows what it is.

How does google/pprof deal with that? Which does it display? Based on what
I've seen, it looks like it displays the last SampleType in the list? I assume
it provides ui (but apparently not command line?) options to select the sample
type. Ah, I see. The command line option sample_index takes and index or name.
But what is the default? The last one in the report, stated explicitly in the
doc/README.md file.

Okay, let's go with that. I would still like to see call info, but not sure
how best to approach that at this point. Think about it.

Overall idea:
* We have an fble representation of the profile.
* We implement parser from pprof to the fble representation.
* For each page, we have a query function that gets the raw results from the
  profile data. We can test these separate from questions of UI. These
  include:
  - Frames by overall time
  - Frames by self time
  - Full sequences
  - A particular (sub)sequence
* For each page, we have a UI layer that renders the page in html. This is
  harder to test.

That's it. I think it's pretty straight forward. I would love for things to be
useful from the start, so maybe order it as: implement parser, then frames by
overall time: impl, test, ui. Hook that up to the server. Now I have my basic
pprof viewer. Add additional pages one at a time.
  
How do I want to represent the profile in fble?
* List of (sequence, value(s)) pairs?
* Probably want to use location_ids for the sequences.
* Then include a separate map from location_id to name, file, line, col info.
* And, depending on how I want to deal with multiple values, a list of value
  names and units.
* And maybe some high level stats as we find useful: number of samples, number
  of full stacks, number of frames.

Sounds pretty easy. Sounds pretty straight forward. Think about what I want to
do with 'calls' and/or alternate SampleTypes to start.

---

Thoughts on calls versus samples:
* It's not unreasonable to interpret calls the same as you do samples. Just
  think of calls as sampling once per function call, compared to samples which
  sample once per instruction. When you use this interpretation, everything
  you might want to do with samples you might want to do with calls in the same
  way. Personally if I had both calls and samples, I would only ever want to
  look at samples for this kind of thing. It feels way more accurate than
  calls.

* The other way to interpret calls is you only care about self time. How many
  times was 'a' called, not how many calls were done under 'a'. This can be
  seen usefully in the same view as samples, but only if we add a way to
  anchor a subsequence at the end. I could see this as useful in general.
  For example, add a '[self]' block to the end of every full stack. Now if
  your subsequence includes the [self] block, it shows self times for the
  sequence instead of cumulative times.

* In theory the list of self frames is the same as would show up for blocks
  incoming to the sequence with just [self]. I think it's still worth having a
  dedicated 'self' page in the viewer. Maybe if you click on a frame in the
  self page, it brings you to the sequence [x, [self]] instead of [x] like it
  does in the overall section.

* One way you could encode time in a profile would be to track it as counts
  into [self] from the node. Then we just need a 'calls' metric, and 'samples'
  could be derived from that. That would mess up the 'calls' metric though
  without special handling, so I don't think we should do that.

* The way I naturally want to look at calls is self mode. The way I naturally
  want to look at samples is cumulative mode. I see pretty much no value ever
  of showing both calls and samples on the same row of a table in the UI
  unless they have special handling to make calls show the self time and
  samples show the cumulative time.

* We can't fake up calls to show self time in a cumulative view without having
  the option for negative calls. But maybe that's doable? Basically we would
  subtract out the sum of the child calls from X from the count of X, and make
  the result the new count of X.

All of this leaves open a lot of the questions I was trying to answer about
how to implement the UI. I suggest the following to start:
* Show one SampleType at a time in the UI. Don't bother showing multiple in
  one page.
* Make it easy to switch between SampleTypes in the UI. This means parsing and
  storing all the SampleType values in the fble representation of the profile,
  and the names of them.
* Add an option to anchor a subsequence at [self].

Anyway, I see no reason to block on this for implementing parsing and an
initial viewer: parse all values, view the last value to start.

It would be useful to see a [self] block in outgoing of subsequence calls, so
you can easily see how much time was spent in self.

Does our interpretation of descending values in a subsequence stack make sense
when anchored at self? If we have: a -> b -> c -> d, we would show values for:
a, a->b, a->b->c, a->b->c->d. For self, would we want: a -> [tail],
a->b->[tail], a->b->c->[tail], ...? That almost suggests we want a separate
'self subsequences' page. Maybe the 'subsequences' page has an option to
switch between 'self' and cumulative modes? Maybe the 'by node' page should
have that option too?

Anyway, don't worry about all that stuff now.

Next step:
1. Define my Profile@ representation in fble.
2. Set up a test case for parsing from pprof.
3. Implement the parser so that the test case passes.

