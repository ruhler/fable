Fble Modules Revisited
----------------------
Yet another revisiting of modules. Specifically I want to work out the
appropriate language features and conventions to be used with modules to
address the following:

* Avoid introducing false dependencies.
  For example, I just added an Equals function to Map. That brings in a
  dependency on Bool that wasn't there before. And if I want to reuse EqMaybe,
  that would bring in a dependency on Maybe too. It was not obvious at the
  time how I could easily avoid that.

* Where to put different bits of code.
  For example, Foo vs. Foo_Tests vs. Foo/Tests, vs. Foo/Foo, Foo/Tests.
  I want to know how I ought to organize modules consistently. And clarify
  the conventional relationship is between a parent module and its children.
  Does the parent depend on the children? The children depend on the parent?
  Is there no conventional dependency relationship between parent and child?
  Are the children internal or public? Do they have access to internals or
  access to public stuff?

  For example, I would expect Foo/Tests not to have access to the internal
  data type for Foo, because they are testing the public API. But it's nice to
  put them in a subdirectory along with Foo just for organizational purposes.

* Abstract type support.
  I should be able to make types abstract, perhaps using the ?= statement
  suggested. But make sure this still allows me to give access to the
  definition of the type to multiple internal modules without giving access to
  the type externally.

* Add and test implementation support for private modules.


Proposal:
* How big a module is is determined by dependencies. If you use a module, you
  should only bring in dependencies you have to have to use the module. You
  shouldn't bring in other false dependencies.

* It should be possible to refine a large module into smaller submodules
  without users of the module noticing. Where the submodules can be private.
  This means a parent module has privileged access to its immediate children.

* If a module is marked as private, it produces private things.
  Ideas for how to mark a module as private:
   Foo!.fble
   Foo!/
   Foo._.fble
   Foo~.fble
   Foo~/ 
   Foo~

  Yeah, I like the last one. So if a file is Foo~.fble, the module Foo is
  marked as private. If a directory is Foo~/, it is marked as private. The
  module is referred to without the trailing tilde. There must not be both
  private and public versions of the same module.

* If a module is in a directory, it has privileged access to things in that
  directory and immediate child directories.

* Modules in the same directory are developed by the same developer.

* Use directories for organizing modules as well as privilege.
  For example, prefer Foo/Tests.fble over Foo_Tests.fble, just because it
  shows the relationship between the two better.


There are 4 kinds of modules, depending on whether they are privileged or not,
and whether they produce private stuff or not:

* private, privileged: put in subdirectory.
* private, non-privileged: put in subdirectory.
* public, privileged: put in subdirectory or at root.
* public, non-privileged: ??

My proposal, though this mixes up two different kinds of concerns, is to
group modules in the same directory that refer to the same thing, even if some
of those modules make use of privileged access while others don't.

Note: Any private stuff that needs to be shared by multiple modules must be
defined in a private module. You can only mix private and public in the same
module if only the public bits of that module are to be accessed.

Random idea: Could we use Foo/Tests.fble to say Tests does not have private
access to Foo~/FooInternal~.fble?  Uh... that looks confusing to me. It
suggests a module has the following:

  value
  privileged children - can access private children
  non-privileged children - can't access private children

In other words, we explicitly label the four kinds of children:
  privileged, private:      e.g. Foo~/FooInternal~.fble
  privileged, public:       e.g. Foo~/Data.fble
  non-privileged, private:  e.g. ???
  non-privileged, public:   e.g. Foo/Tests.fble

If that's the case, could we have two different symbols to track this in the
file system? Assume by default non-privileged, public? Then have, for example:

+ means privileged
- means private

So we have:
  Foo/Internal+-.fble
  Foo/Data+.fble
  Foo/Tests.fble

Or: '*' for read and write, '+' for read only, '-' for write only?
  Foo/Internal*.fble
  Foo/Data+.fble
  Foo/Tests.fble

Or: '-' for read and write. e.g. internal only private.
    '+' for public with private access.
        blank for public with public access.

That sounds like a nice proposal to me. Let's try it and see how it feels.
Then the only thing left is to figure out abstract data types.

---

When writing the spec for the above access controls, I realize having separate
privileged and non-privileged makes for an awkward situation.

If you just have public and private, then we assume a parent has private
access to its children and private access to its siblings. And children have
access to their parent.

If you add non-privileged, then there are two groups that a parent is involved
with: its children and its siblings. Then you need two specifications. Does it
have privileged access to its children? Does it have privileged access to its
siblings?

If all you get from privileged/non-privileged is an extra compile time check,
maybe it's not worth it? It doesn't really fit in the notion that a directory
is all owned by the same organization. It adds a strange corner case to the
spec.

Potentially you could work around it too, for example:

  Foo/
    Tests.fble
    Impl*/Impl.fble

Well, no. That doesn't really work. Because if Tests.fble doesn't have the
access it needs, neither does Foo. Unless Foo is just a re-export of an
internal module:
  
  Foo.fble        // re-exports Private.fble
  Foo/            
    Tests.fble    // uses Private.fble
    Private*.fble // re-exports API.fble
    Private*/
      Impl.fble
      API.fble
    
So yeah, worst case, using reexport, which is trivial, you could separate the
access into privileged and non-privileged just using directory structure with
public and private. My vote is to get rid of the notion of privileged and
non-privileged for the first implementation.

---

Experience Report: It feels like module organization is a hard problem.
Consider integers. I figure I want IntP, IntS, and Int. But IntP decrement
wants to use IntS and IntP subtraction wants to use Int. Then IntS division
wants to use Int subtraction, and so on. We end up with a long dependency
chain if we split everything up:

  IntP -> IntS -> Int -> IntP.Sub -> Int.Sub -> IntP.Div -> Int.Div

It is nice to have each module be a small focused thing. None of them really
have any private internal functions, so the only practical value at the moment
for using separate modules seems to be to separate them into small focused
things and for namespace support, so that we can have an Eq function for IntP,
an Eq function for IntS, and an Eq function for Int.

This is actually a deeper question I think. Is it better to define separate
IntP, IntS, and Int types that constrain the data type, or is it better to use
a generic Int type? It's certainly proving inconvenient to keep track of which
types all the functions are used for and converting between them when I have
IntP, IntS, and Int types.

Or: is it better to define these across a bunch of modules to avoid false
dependencies, or better to just define them all in one module and avoid the
headache of splitting them up?

---

Things I want to try to see how well they work:
* Lots of little modules. Because what, really, is the cost of that? I think
it's just being able to remember what module to import. It doesn't reduce the
number of import statements you would use.

* Don't require modules to all be imported at the top of the file. I'm not
convinced it adds much. For example, /Bool/Show%.Show is just as good or
better to use than locally defining ShowBool. Take a little more risk here and
see if it leads to any problems.

---

In practice, splitting things up into very fine modules hasn't caused me any
problems for things like separate Show, Eq, List/Sort, List/Length. It's nice
to have them separate. They are still organized. It doesn't really cost
anything in import statements in practice I don't think.

But I will say, picking between IntP@, IntS@, and Int@ is annoying. For
example, I want to print the number of tests ran, passed, and failed in the
test runner. A simple thing like computing passed = ran - failed suddenly runs
into problems if I'm using IntS@ as the type instead of Int@.

---

Experience after a fair bit of time has gone by:
* Little modules are fine and nice. Slightly inconvenient sometimes to
  remember where something is defined, and in the case of Abs, I ended up
  defining it twice in two different places. Otherwise, I like it.
* For the Int@ question, I'm very much trending towards always use Int@, and
  think of IntP@ and IntS@ as internal implementation detail. The potential
  runtime savings are not worth the inability to easily compose functions
  involving IntP@, IntS@, and Int@.

---

Now that we've added an initial form of abstract types to fble, it begs
revisiting modules. Some time back I removed support for expressing and
controlling access to private modules, because it seemed messy and worth
revisiting later rather than maintaining. I think now is a good time to
revisit, at least in thought, what to do about modules.

Modules are many things, which makes it confusing at times:
* A way to load things without bringing in unused dependencies.
* A way to organize code.
* A way to control who has access to what parts of code, primarily to make it
  possible to update the internal implementation of a module without breaking
  users.
* A way to package, share, and deliver code to users?
* A way to improve compilation times?
* A way to deliver code to users without sharing source code?

Conceptually an organization wants to deliver a collection of modules, call it
a package. A package ideally is hierarchical, i.e. made up of a number
of sub-packages delivered by sub-organizations. The package will include
public interface modules and internal modules used in the implementation of
the public interface.

Organizations will want to be able to update their package and make new
releases. Users will want to download new releases compatible with their code
and switch to using those. Users may want to make use of packages X and Y that
both depend on package Z, and share the package Z between X and Y instead of
duplicating it.

Packages will depend on other packages at particular versions or ranges of
versions. You probably want it to be possible to have more than one version of
a package at a time in the same executable, for example if X depends on Z
version 1 and Y depends on Z version 2, and you want both X and Y in the same
program.

Package management sounds hard. Does it make sense to make it a part of the
language, or does it make sense to let people use whatever package management
mechanism they want? Let's do a survey of some existing languages.

C/C++
  Tends to rely on OS distribution for package management. You ship a library,
  user has to download the write version of the library. pkgconfig can help
  organize things a little bit.

Python
  Has a whole bunch of stuff. It looks confusing and complicated.

Let's look at this another way. What's important for me as a developer is:
* Can I easily install packages I can compile against for development of my
  package?
* Can I easily publish my package for others to install?
* Note that distributing libraries may have different requirements than
  distributing end applications.

From the language point of view, you are writing source code. Your source code
refers to types and entities of the package you want to use. You should only
be able to access public entities of the package in your source code. You
should have a way to describe what is public or private to your package.

How do you know what is exported by the package? I have this idea that
anything private to the package can just be omitted from the public
interface. For example, you compile in internal stuff when compiling your
package, but don't let users have access to that internal stuff when using
your package. But you don't always want to compiler your dependencies into the
package. If you use a common library, you want users to provide the library.

So, what is a package?
* An interface for use in compiling source code.
* An implementation for use in running
 - depends on if you run with interpreter or native machine code for example.
* A list of dependencies your package has that the user is responsible for
  providing.

In fble, I suppose module is the link unit. You link against modules, not
functions. That means at runtime you access the module value, and if it is a
struct, any entities inside it.

So we could use something like an ELF format, where there are public and
private symbols, for public and private modules.

That suggests we may want to have multiple modules compiled to a single
library file. As it gets bigger, we would want to have multiple library files
representing a collection of packages.

How do we generate these library files? Because that's where we will want to
specify what is public and private. And we'll want some way to specify the
interface to a package.

Perhaps I can think of fble as responsible for describing a single module. And
we want some other language or mechanism for describing how to compile modules
together into reusable packages. That other mechanism is where we would
specify what modules are public or private.

---

ld has an option --exclude-libs which I think can be used to mark certain
modules as internal only. For example, we could link together a bunch of .o
files for compiled modules using ld, and indicate then which modules are
public and which modules are private. If we distribute the resulting .so file
as a package, then you can only access exported modules.

It would be strange if your program could compile but not link though. The
compiler needs to know types for all the modules you reference. How can it get
that information?

I think the original plan for module compilation was to have the compiler
generate Foo.fble.@ files describing the type of a module using restricting
fble syntax that doesn't allow for module references. If we have that, then
when we distribute a package, we could distribute a .so file with the compiled
code and a collection of Foo.fble.@ files, where we remove .@ files for
internal modules.

There is a subtle consequence to this approach: access to internal modules is
no longer based on location in the hierarchy. It's based on an explicit list
of things when compiled. In other words, you have visibility to everything you
are compiled together with. That seems entirely reasonable to me. The
assumption being the group of modules you are compiled with is the same group
of modules that a single organization has sufficient control over.

I don't necessarily want elf files to be part of the language spec. That
depends very much on the language implementation. If you are using an
interpreter, you would want .fble source files, including internal modules, as
part of the package distribution. Or maybe compile to some other format the
interpreter understands.

I guess the question for packages in fble is what I want to include as part of
the language specification in terms of package formats.

---

Modules came up as potentially useful for printf debugging. The idea is you
want to be able to compile a module with only type information about the other
modules it depends on. That's useful for distributing modules too.

So, here's what I'm thinking:

/Foo% is implemented in Foo.fble. Foo.fble.@ is an fble file containing just
the type of /Foo%. The compiler can read the .@ files to get type information
for dependencies without needing to access their implementations.

I'm thinking we may as well have Foo.fble.@ have the same syntax as a normal
fble file. It might be useful to share type definitions among these header
files, and it might be useful to make them human readable. At the same time,
we still have the option to make them compiler generated.

It will be important for the compiler to be able to confirm when compiling
Foo.fble that it's type matches what is in Foo.fble.@.

There may be duplication of code between Foo.fble and Foo.fble.@. I think
that's okay. Hopefully it means you have to think twice any time you go to
change the interface to a module. Hopefully it's not too bad to duplicate.

I think Foo.fble.@ would be a good place to document the public interface to a
module, separate from its implementation.

How would this look from an implementation standpoint?

Currently we always load and compile all modules, but we only generate aarch64
code for the main module. We'll want a different way to load modules.

Maybe we can load for each module a type and a value, either of which is
optional? I'll need some thought to work out exactly how to do it, but the
idea seems simple enough. Just need to manage the two different ways of
loading things: full source versus mostly type dependencies.

---

Let me summarize what I think I may have said above, but I'm not sure.

Modules:
* Compile an individual module to a .o file.
* Group .o files for modules from the same developer into a library file.
* When reading dependencies, search for .fble or .fble.@ files to represent
  the dependency. Type check, but don't generate code for these.

A package is distributed as a library file along with a collection of
.fble/.fble.@ files.

How this supports private modules:
* When you generate the library file, you have access to internal modules.
* When you publish the library file, you do not provide a .fble/.fble.@ for
  internal modules.

There are two broad cases to consider:
1. Compilation. fble-compile acts as described above. Generates a .o file for
a module. Loads dependencies for types from .fble/.fble.@ files. The user can
link against the distributed libraries.

2. Interpreter. It's nice if we can avoid the extra compile step, otherwise we
may as well make this a different compiler.

The interpreter mode is kind of funny. You need the source for everything, or
the ability to find and load .so files dynamically because you can't link them
into the interpreter itself.

---

How I would implement modular compilation, assuming I didn't have to worry
about the interpreter case:
* FbleLoadedModule contains FbleExpr* type and FbleExpr* value.
  FbleLoad loads value and optional type for main module.
  FbleLoad loads type only for dependant modules, wrapping in typeof as needed
  in case of .fble vs. .fble.@ file loaded.
* FbleTypeCheck knows to get types for dependent modules.
* FbleCompile returns code only for the main module.

And the interpreter case?
* Do things as we do today: load full, type check all, compile all, interpret
  all, link together all.

Perhaps what I want, then, is Module and Program variations on everything:
* Add an FbleExpr* type field to FbleLoadedModule.
  This is the value of the loaded .fble.@ file, if any.
  States:
   1. type && value: main module, loaded both. Typecheck confirms they match.
   2. type only: loaded .fble.@. Use that for the type.
   3. value only: loaded .fble. Use that for the type.
* FbleLoadModule
  Loads type && value for main, prefers type over value for non-main.
* FbleLoadProgram
  Loads type && value for main, loads type and value for non-main.

Okay, so this brings up a tricky point I think I mentioned before. The graph
for .fble could be different from the graph for .fble.@. Is that an issue? Can
we assume they can be sorted the same way?

For modular compilation, we do want to check that the type of main.fble.@
matches the type of the value main.fble. We need to load both main.fble.@ and
main.fble. Then main.fble.@ loads its own graph and main.fble loads its own
graph. Either we process them as two separate graphs, or we load them all at
once. To load them all at once requires the merged graph not be cyclic. I
think that's fine. It maybe means we have to load all the dependencies for
main.fble.@ and main.fble before we can add the FbleLoadedModule. That's fine.
Should be straight forward if we do them as a group.

* FbleTypeCheck
  Type checks all loaded type && value for all modules.
  Verifies type matches value for all modules if both are present.

* FbleCompileModule
  Compiles only the main module (type checks all).
  In particular, does not compile .fble files used just for type information.

* FbleCompileProgram
  Compiles all modules.

* FbleCompile
  To simplify things getting started, maybe just have the compiler compile any
  module with a value loaded. We can ignore the results of any we don't care
  about. The performance optimization is to skip compilation for modules we
  don't care about.

Yeah, so incrementally we want:
* Add 'type' field to FbleLoadedModule.
* FbleLoad implements FbleLoadProgram behavior: load whatever .fble.@ and
  .fble we find.
* FbleTypeCheck checks all types and values and types against values.
* FbleCompile compiles everything with a value.

And we later optimize to not load .fble if we can load just .fble.@, and to
not compile any values we aren't going to use.

This raises a question: should we have a compilation mode that generates code
for an entire program in one go? It would be faster, because we don't have to
reload all the dependencies for everything in separate calls to the compiler.
But it would complicate the interface because we need to generate multiple
files instead of a single file, and need to come up with names for those. No.
Let's not worry about compilation time for now. Maybe some day I can have an
option to compile an entire package specified by directory root or something.

Good. I think the next steps are clear. And I think this is worth implementing
so we can start to split up fble programs more realistically.
 
---

What does it mean if a .fble.@ depends on a module that the .fble doesn't?
There's nothing to stop it. Should we say that means both the .fble and
.fble.@ modules depend on that module? Or should we distinguish between them?

I expect in the normal case that the .fble.@ file depends on a subset of
modules depended on by .fble. How about I don't make things complicated, just
say you depend on all modules referenced from .fble.@ and .fble file if you
are trying to load them both.

I worry this is introduces some confusing, questioning cases. Having two
definitions of a module. But we know they have to match, enforced by the
compiler, and we know one is a type, so it's not like two different
implementations really.

Hopefully I can find a reasonable place to document the difference and
interplay between .fble and .fble.@ files.

---

Trying out to see how some .fble.@ files would look written by hand... it
feels rather tedious to me. It feels almost like I want a different language
for the .fble.@ file, which is like the normal .fble file, except with the
definitions of things removed, in some cases.

For example, what if I used Undef to define all values?

Or, should I prefer auto-generated .fble.@ files? But then, what's a good way
to read documentation for the library? Is it bad to try and overload the
purpose of .fble.@ for compiler and human consumption? If it's not for human
consumption, why not just use the .fble file for the implementation file?

Remember the goal: only read what we need from the modules to know their type.
If we use the full .fble file, we end up reading dependencies that exist
solely for the implementation.

Also, I'm not sure how abstract data types can work with separate .fble.@ and
.fble files, because they will be considered as having two different types.
How do we map the type in the .fble.@ file to the type in the .fble file?

---

Revisiting the goal of modules. There are a few different ones:
1. Providing documentation for users on how to use a module without the
distraction of implementation details.

2. Efficiently providing the type of a module to a compiler.

3. Making it possible to compile a module without the full implementation of
the modules it depends on, so it's possible to restrict access to internal
parts of that implementation.

4. Making it possible to implement a module outside of the language.
For example, for printf, or a hypothetical optimized integer library.

Thoughts on documentation:

In practice when programming I use man pages, reference guides, wikis for
documentation. It's only my own code where I tend to refer to comments on the
prototypes of functions. I've never found things like Deoxygen output useful.
Occasionally it's useful to read comments in code, and that tends to be in the
implementation file anyway.

In other words, I'm not convinced documenting code via comments on a header
file like thing separate from the implementation is anything fundamental.
There are lots of ways to document code. We could have doc comments in the
implementation and write tools to auto generate online reference guides or
documentation.

Items (2), (3) and (4) are, to me, really about specifying just the type of a
module, separate from its implementation. This could be a machine
representation, it need not be human produced or consumed.

The dream is that you specify the type of a module in a human readable format
where you can also document things.

In practice, most modules I write have a struct type.

But fble can already be used to describe types. What's so tedious about it?
Really, honestly?

* Annoying to do @<Foo@> Foo@ for a type. I rather do @(Foo@, ...).
* Annoying to have indent of *( T@ t, ... ). I rather directly do 
   T@ t;
* Annoying to have to duplicate a complex type like Char@ from implementation
  to type.
* Annoying to have to give the type of something like Chars, which we
  otherwise never have to write explicitly.

I almost wish we had a special language construct designed to help here.
For example, what if we had a special kind of statement:

type name ';' stmt

The meaning of this is declare a variable of given type and name. The type is
the type of the body of the statement. At runtime this gives a runtime error
'unimplemented'.

It's okay to use for describing types, because you don't ever run the code.

This gets around the tedious concerns. And there's nothing I can do to get
around having to give the type of something like Char@ or Chars. And that's a
fairly special case. We don't often have some complicated types. And users
would need to know those types regardless.

@ Chars@ = *(Char@ nl, Char@ tab, Char@ ' ', ...);
Chars@ Chars;

This is the best of all worlds, right? You get full fble syntax for describing
types in a human readable format. You can put documentation with those types.
You have a pleasant syntax for putting the types. We don't have to have two
separate syntaxes for header and implementations.

A couple of questions to follow up on:
1. Should we require a .fble.@ file for every module?
2. How do we support abstract types this way?
3. Should the .fble.@ file give the value or the type? Like, end in @(...) or 
  @<@(...)>?
4. Should there be a way to reference the type from a .fble.@ in the .fble
file, or do we really need to duplicate things like the definition of Char@?

---

Here's an idea, just for the fun of it. What if we allowed you to write
abstract values and abstract types, such as:

@ Bool@;
Bool@ True;
...

And we said the way you provide an interface for a module is by providing a
file with the same type as the module.

And we said this replaces abstract types. That way, instead of having two
different mechanisms for access control, we have a single one that works for
both.

For example, say you have two versions of Map.fble, the public and the private
one. The private one has the full type def. Anyone allowed access to the
internals sees the private one. Anyone without access to the internals sees
the public one. For the purpose of typecheck, the public Map@ type is an
abstract type you can't create or access.

This brings up a question of how hard it should be to break abstraction. For
example, let's say you use the Map@ library and want to work around the access
issue. What do you have to do?

Without any restriction, you have to try hard not to accidentally modify the
Map. With abstract data types, you have to not import the token type. It's
much harder to accidentally access internals. If we say you can implement fble
modules outside the language, then there is nothing stopping you from
accessing internals of the type that way.

I think, in practice, it's good enough if we just provide a .so file and
public Map.fble to prevent people from accessing internals. They would have to
go out of their way, and then it's on them.

But what if you have access to both the internal and external Map.fble? For
example, because you want to run the interpreter, or because you are the
implementer of Map.fble and some modules that should treat it as an abstract
type?

I'm basically saying access control is done at build time based on how you
compile. You pick what can access something by pointing it to the public files
or the private files. Again, if you don't have the private files, that's easy.
But if you have both, it's on you to make sure you don't access what you
shouldn't be accessing.

Could we have two different types? The internal Map@ type, and the public Map@
type, so that an extra step is required to explicitly access internals?

How would the build setup look like, if I were to follow this? Conceptually I
don't really want to manage on a per file basis what files things can access.
I want packages. So each package has the source code and the public API, let's
say they belong in different folders.

To compile a package P, with public code in P_api and source code in P_src, I
say P_src code has access to *_api for all packages it depends on, and P_src
for implementation.

If only there was some lighter weight way to group modules together. For
example, Int@ and Map@ seem like good candidates for a core package, but it
would be nice if Int@ couldn't see inside Map@. Or, for pinball, Num@ should
be part of the package, but still not visible. It's like, it would be nice to
have sub-packages. A fine grained hierarchy of packages.

How might I describe that for the current collection of modules I have
written? In general it's a graph. Not a total order. It's an abstraction of
the module graph.

Let's think about packages then. A package is a named set of modules. A
package can depend on other packages. A module within a package may only
reference modules from the package or packages the package depends on.

That's not too hard. We could have a package format, like, name, list of
modules, list of packages depended on.

Next we introduce the notion of a subpackage. Or, conversely, a way to group
together a collection of packages into a bigger package.

A composite package is a named package that depends on other packages and
provides a list of packages implementing it. Packages inside the composite
package can only depend on each other or packages depended on by the composite
package. A module acts as an atomic package.

So, essentially, I just want a package to be able to denote the boundary
between where the internal version of a module is used instead of the
external version of the module. Well, that and restricting the set of
dependencies a module is allowed to have. That's important too.

---

I'm sort of leaning towards my old package idea.

A module is a module and a package. For example, /Foo% is the module /Foo% and
the package /Foo%. These are organized hierarchically. For example:

Core/
  Map/...
Pinball/...

The package /Foo% includes the module /Foo% and all modules / packages in the
/Foo/ directory.

A package has a name, a list of direct modules/packages in the implementation,
and a subset of those that are 'public', meaning they can be accessed by
modules that are not in a sibling package. And a package has a list of other
packages it can depend on.

By default a package can depend on any other package that any of it's modules
depends on, and a package exports all entities as public.

If you want, you can specify a .fble.@ file for a module to give its public
interface. Modules in sibling packages have access to the .fble file, all
others see it using the .fble.@ interface.

If you want, you can specify a .fble.# (or whatever syntax I want) file, which
gives non-default package info: a list of packages it can depend on, and the
list of children that it exports publicly. Not sure what the syntax should be.
Maybe:

  a; b; c;    (Imported)
  @(x, y, z, ...);  (Exported)

This gives us fined grain packaging. And we can distribute a package as a set
of just .fble.@ files and .o files, or .fble.@ files and a .so file.

Let me play around with this idea and see what it might look like in practice.

---

Things that came up when playing around:

1. Let's say I want Unit, Bool, etc. to all be together in a single Core
package, so that we can say other packages can depend on Core, but not other
things. That means I would have to move all those things to the Core/
directory, which would mean I have to update all the modules paths to include
Core/. But I don't really want to do that. And how do you know what goes into
Core or not in that case?

If the goal is to restrict the modules some app can depend on, what's the
criteria for what that set should be? 

If you have two package developers, how do you prevent them from unknowningly
defining modules with the same path? The only way is to require they know
their package name and that their package names don't conflict. That suggests
that if you are providing a module Bool as part of a package Core, then the
module should really be called /Core/Bool, not /Bool.

As a corollary, every module at the top level of the hierarchy must be in its
own package. Because if it was in another package, that other package would
own the entire namespace, meaning it would be the only package that could
exist.

There is a need, at the global level, to prevent conflicts. You need a single
package database that owns a namespace and can allocate parts of that
namespace to separate entities. Ideally, like anything else, it's
hierarchical. You could have a sub namespace that is manage by some
organization that allocates parts of it to other organizations. The point is,
the unit of allocation is directory: if you can put things in a directory, you
are the sole entity that can put things in that directory.

2. If you reference some module Bar from Foo.fble.@, does that bring in the
internal version of Bar or the public version of Bar?

For values, we should only need the public version of Bar. I think it only
matters for types, because Foo.fble.@ is only used for types. So, imagine Bar
is Map, and Foo.fble wants to export something that works with Map@. It should
be fine for Foo.fble.@ to only refer to the abstract Map@ type, because it
doesn't need to access internals.

Is it possible we have a third module that imports Foo, but also has access to
the private version of Map@? Perhaps it's part of the Map implementation that
wants to reuse Foo, and Foo is not part of the Map implementation. How do we
reconcile the two map types, the one read from Map.fble for internal use and
the one read via Foo from Map.fble.@?

This suggests to me, that for consistency sake, what matters for determining
use of .fble versus .fble.@ is the main module being compiled. If you want to
compile module X, and X depends on Y depends on Z, then when Y refers to
module Z, it should see the version of Z visible to X.

That sounds pretty confusing to me. If only values are abstract, then it
should always be safe to read the .fble.@ file. But if types may or may not be
abstract, then ... ? Can we say something like: if Y depends on Z, and Y
compiles under Z.@, and Z is consistent with Z.@, that guarantees that Y will
compile under Z, so when X references Y, it's okay to do so with Y having
access to Z?

3. I find the .fble.@ files pretty appealing in terms of providing
documentation and including just what you need to know to understand how to
use a module, without the distraction of an implementation.

For example, lots of the Tests.fble.@ files are just three lines of code,
compared to hundreds for the corresponding Tests.fble file.

4. It's kind of nice having to write down explicitly dependencies for
packages. For example, given I know the Core package has no external
dependencies, that makes it very clear what all has to be grouped together as
part of the core package.

5. Subpackages must be able to depend on each other recursively. So it would
seem that packages should be able to depend on each other recursively as well?
Isn't that scary?

For example, Unit/Show% is part of Unit package, depends on String package.
String depends on Unit. So that's a package cycle, even if not a module cycle.
I argue this is fine because really all of 'Core' is compiled together. So, it
would seem, 'Core' is a real package, and somehow 'Unit' and 'String' are not?

That begs the question: is there really any such thing as a subpackage? Or,
what is the meaning if explicit package dependencies? If a module in a package
can freely refer to any other modules in the package, it makes sense that a
package in a package can freely refer to any other package in the package.

It does mean that you can't necessarily compile a sub-package in isolation.
You can't compile the Unit package without compiling the String package, and
you can't compile the String package without the Unit package.

We could, in theory, compile Unit and String sub packages separately, if we
had the interfaces to the corresponding package. But then, how would we know
if the implementations depend on each other or not? How could you check that
at compile time?

6. How to handle top level module of a package?

The Core package has no top level module. It's easy. Everything is under Core,
and Core just has to export those immediate packages under it.

For Stdio, we have a /Stdio% module. Maybe we say /Stdio% module is always
included in Stdio.fble.# package, and additionally we list some other modules
that are exported.

What about something like /Map%, where it seems like we should have Map and
Set both in the same package, but if the package is Map, and we have /Map%,
that means we have to have /Map/Set%? Wouldn't you rather have /Map/Map% and
/Map/Set%? That almost suggests that you should not be allowed to have a
module and package with the same name? As if a package is a directory and a
module is a file. You don't have something that is simultaneously a directory
and a file.

Then we would have /Stdio/Stdio%. Package and module separate. And where
should Cat% and Test% go? In /Stdio/Test and /Stdio/Cat? Or /Stdio/Stdio/Test
and /Stdio/Stdio/Cat? Clearly the former.

But then, how can we have /Core/Int% module with /Core/Int/IntP inside? Are
you suggesting instead we have /Core/Int/Int%? /Core/Int is the package name
and Int% is the module within that? Or, in this case, should we not bother
making /Core/Int an official package, so we can have a module named /Core/Int%
and Core/Int/Test%?

Let's say we have a package Core, and under Core is a package Int, and we have
a module /Core/Int%. Does that belong to the Core package or the Int package?
Where it would make a difference is if the Int package hides some internal
modules. The question is, should /Core/Int% have access to those internal
modules or not? I lean towards saying /Core/Int% is part of the Int package,
but it kind of only makes sense to make that a public part of the /Core/Int%
package?

---

Let's work out some details for abstract types.

The question: how can we define a type that is abstract for everyone outside
of a package, but visible to everyone inside a package?

If it's visible to everyone inside a package, the type must be exported from
the module that defines it. It could be exported privately or publicly.

Privately means anyone in the same package can access it? Then say we define
it in /Map%, with Map.fble.@ not defining it, but Map.fble defining it. And we
have a Map package, so any module under /Map/... should have access to it, and
anyone else not.

/Foo% imports /Map% directly, gets the abstract type.
/Map/Foo% imports /Map% directly, gets the defined type.
/Foo% imports /Map/Foo%, gets the /Map/Foo.fble.@ file. Then say
/Map/Foo.fble.@ gets the abstract type.

Which would suggest: when referencing a module from a .fble file in the same
(or privately accessible) package, you read the .fble file of the imported
module. When referencing a module from a .fble file of a non accessible
package, you read the .fble.@ file. When referencing a module from a .fble.@
file, you always read the .fble.@ file.

It still feels weird to me that abstract values can be guaranteed to make no
difference reading .fble versus .fble.@, but abstract types can.

Let's say we export the definition of a type publicly in an internal module.
Eventually, for a user to be able to refer to the type, we have to export it
from someone who can see the original type definition.

Going back to the private approach. /Map/Foo% imports /Foo% and /Map%, and
/Foo% imports /Map%. Now we have two different Map@ types, the one defined and
visible from /Map%, and the abstract one from /Foo% --> /Map%.

Okay, now lets explore an approach based on token types. Say there is no such
thing as an abstract type in .fble.@. .fble.@ and .fble have exactly matching
types. How would this work?

* Define the implementation type in a private internal module. Everyone in the
package has access, nobody outside the package has access.
* Define a token type in a private internal module. Everyone in the package has
access, nobody outside the package has access.
* Define the abstract type in a public module that references the internal
  module for the token type and the internal type. Only export the abstract
  type.

Now /Foo% imports /Map%, which imports /Map/Map% and /Map/Tok%. /Foo% cannot
access /Map/Map% or /Map/Tok%, but /Map% can. This means access control is
based on where the reference comes from, not the main module being compiled.
That's different from my other approach, where type consistency requires
everything see the same definitions.

But now we have a situation where we have to ship /Map/Map% and /Map/Tok%
with the package, because they are needed to be able to compile /Foo%. In an
ideal world, though, they shouldn't be needed to compile /Foo%. /Foo% doesn't
actually need to know anything about the Map@ type.

Going back to the private approach again. /Map/Foo% imports /Foo% and /Map%,
and /Foo% imports /Map%. This implies that the Map package depends on the Foo
package, and the Foo package depends on the Map package. Which means they
cannot be compiled in isolation. Which brings us back to the question of sub
packages.

Let's imagine there we are not allowed to have cyclic package dependencies.
Does that solve the abstract type issue? A-cyclic package dependencies means
there is a total order in which you can compile packages. When compiling a
package, all modules inside the package have access to the internal type
definitions inside the package. Modules have access only to the public type
definitions for modules outside the package.

It's almost like you want three different files: .fble that does the
implementation, .fble.pkg.@ which does the package internal interface, and
.fble.@, which does that public interface. Or, to be able to say there is some
internal package defining this type, but that package is not visible to
external users.

Something like:

@ Map@ = /Map/Map%.Map@;

Where /Map/Map% is private to the module. In other words, we allow you to
refer to modules that don't exist (in the publicly visible set of modules)
from a publicly visible .fble.@ file. Which brings us back to the 2 interfaces
idea: a package internal interface and a package public interface, both
separate from the implementation.

Let's be honest. We only ever want to access a module via its interface,
regardless of whether we are in the same package or not. Abstract types
implies multiple separate interfaces for a module.

---

Here's an idea for supporting abstract types with modules. It's based on the
idea of having public/private key pairs. For example, normal cryptographic,
or, say, module name, where the compiler enforces that only a module can
create it's private key.

We have a public token type that takes a public key as part of the name.

  @ _@ = ?(/Map/Private%);

Anyone can create abstract types from the public token type:

  @ Map@ = _@<Map@>;

If you want to cast, you have to have access to the private token.

  @? key;   # key has type ?(/Map/Private%), assuming this is declared in
            # the module /Map/Private%.

And you can use the key to cast.

    key.<target>(source)

key has the ability to cast anything abstract via type @<key>.

Hmm... be a little careful. We need the key to be real. We don't want people
doing things like: Maybe@<_@>.just.<target>(source). So either have a runtime
check that the key is valid, or use a slightly different type for key that you
can't write down.

---

A few more thoughts:
* I think the compiler needs to know the internals of an abstract type when
  compiling the user code. Because, depending on the language implementation,
  the compiler will need to know how much space or what the runtime layout of
  the type is. This suggests it's not sufficient to withhold the internal type
  from external packages.

* It would really be great if we could use the same mechanism to prevent users
  from accessing internal modules as we do to prevent users from accessing
  internals of an abstract type. I would prefer not to have two separate
  mechanisms.

* We can generalize the idea of keys and tokens and casting conceptually.
  Add a type to the language which describes the set of modules with access.
  That's our token type. Only modules in the list can do the cast to and from
  token type wrapped values. Whether the set of modules is the current module,
  any module with access to the type, an explicit list of modules, or
  implicitly any modules in "this" package is a detail from that point of
  view.

* Random idea that I'm not convinced makes sense: if an internal module has
  some module type, maybe we wrap that module type in an abstract type. That
  way only people with access to the internals of the abstract type can
  actually use the module. In other words, instead of using module access to
  restrict access to abstract types, I feel like we could just as easily, or
  perhaps even more naturally, restrict module access using abstract types.
  And this could give us finer granularity of restriction. For example, maybe
  some fields of a module are public and some are private. It's not all or
  nothing, it's value by value based on their corresponding types.

* It's still useful to have modules as separate entities for the purposes of
  limiting dependencies, even if you have to ship something as a package. For
  example, you could imagine statically compiling a binary. It would be nice
  to only have to include implementation code for the modules used, not the
  entire package.

---

If we want to support private modules easily, it would be good to have syntax
in addition to cast that knows how to directly wrap and unwrap values. Then,
for example, we could do something like:

_@(@(...));

To mark a module as private. And

% Foo = /Foo%.@;

To unwrap, assuming we unwrap in a module that has access. 

---

Taking a step back, here's where I'm at.

1. It's nice to have separate interface files for each module, for human
documentation purposes. I think that can be done as an orthogonal language
feature. Header files can have undefined values, but not undefined types.

2. It's nice to organize modules together by package. I can do this today by
putting separate modules in separate subdirectories, allowing multiple paths
to be passed to the compiler, and building packages with access to just the
packages they depend on.

3. From (1) we get the ability to distribute packages with header files but
without source files. And we get the ability to implement a module in a
different language.

4. What's left is
a. A question of trust: can you disallow untrusted users from creating values
   passed to your APIs. Solve by abstract type token, which essentially gives
   access to an explicit list of trusted modules.
b. A question of upgradability: can you restrict the APIs that a user uses so
   that you can make updates without breaking them. This could be entire
   modules to mark as private, or fields of abstract types. It feels to me
   like this is tied up with the question of version numbers and the ability
   to make backwards/forwards compatible changes to a module or package.

In both these cases users may be within a distributed package boundary. For
example, Core is a distributed package boundary, Int and String are within.
That suggests to me that compile time hackery is not the right answer. We want
a language feature for access restriction.

I think it's worth thinking more about backwards/forwards compatible changes,
and what language features would enable them. For example, perhaps a read only
unordered struct type would help, because you could add members without
breaking things, allowing for a wide range of compatibility.

---

Another line of thoughts.

1. If two people can both add modules to the same directory, there must be
some mechanism of trust between those two people. Otherwise they could add the
same module and create a conflict. So let's assume that people with access to
the same directory trust each other. This suggests it's reasonable to use
directory hierarchy for describing sets of trusted modules.

2. There are three things we may want to restrict access to: modules, fields
of a module, or types. Is it okay for a user to know about a restricted
module of field? If yes, then we can use the same mechanism of abstract types
to restrict access to modules, fields, and types.

Module: Make the type of a module an abstract type.
Field: Make a field of a module an abstract type.
Type: Make the type an abstract type.

In practice this would mean a user could import a restricted module. They just
couldn't do anything interesting with it. I prefer the user wouldn't be
allowed to reference the module at all though, so that you could remove an
internal module without breaking anyone.

Same thing for a field: In practice a user could import a restricted field,
they just couldn't do anything interesting with it. Except perhaps to pass it
back to a module that could do something with it.

An abstract type is useless if a user can't refer to the type, so we don't
have that question there.

3. It doesn't make sense to have separate public and private namespaces for a
module, because we want a way to make a sub sub module public to a sub package
but private to a higher level package. Namespaces cannot easily convey that
distinction.

4. Could I come up with a way to make modules and fields unreferenceable? Or does
it only matter for the case where you want to remove an internal module/field
without breaking a user, in which case, just leave it as an empty
module/field?

Which brings us to the question of compatibility. What changes can you make to
a package/module to guarantee you don't break the user?

There are different kinds of compatibility:
* binary compatibility: the user doesn't have to recompile. You would not be
  allowed to change any interface type, assuming the compiled code depends on
  the number of bits needed to represent a type.
* source compatibility: if the program compiled successfully with the other
  package, it should also compile successfully with this package.

Are we always dealing with 2 packages? Let's consider four packages, A1, A2,
B1, B2. Say B depends on A.

Say A1B1 works, does that mean A2B1 works? In general, only if:
* No interface types changed between A1 and A2.
  For both binary and source compatibility.
* It is possible to add new modules.

Say A2B1 works, does that mean A1B1 works? Only if:
* No interface types changed.
* It is possible to remove modules going from A1 to A2.

Say A1B1 works, does that mean A1B2 works?
* Presumably B2 is compiled against A1, so yeah. No issue.

So, kinds of actions involved:
* Change a module's type.
* Add a module.
* Remove a module.

What I wonder is if there are ways we could support changing a module's type
while still being compatible. For example, add a field to a module.

For binary compatibility, I claim no.

For source compatibility, you could imagine it's fine to add a field to a
struct type as long as the user only uses values of the type to access fields.
For example, don't construct values of the type, and don't compare the type to
an explicit other type. You could think about making @(...) form produce an
anonymous struct type that is unique from all other struct types and that does
not support explicit type struct value construction. We could use that for
modules. It means you couldn't use a  short hand for, e.g.
 
  Coord@ c = @(x: 3, y: 4);

You would have to do instead:

  Coord@ c = Coord@(3, 4);

But I presume that's a relatively minor loss of ability in the language.

If you had this, you could add fields and maintain backwards compatibility.

How about for union types? You could only add fields if there were default
cases, and only remove fields if users avoided constructing those fields in
the first place.

How about changing abstract types? It's source compatible, but not binary
compatible, to change abstract types.

So, I think the relevant questions are:
* Do I want to enforce binary compatibility, or is it okay and desirable to
  enforce source compatibility without also binary compatibility?
* How do I feel about having anonymous struct types as a separate language
  type?

I like the idea of binary compatibility. If we can force people to have binary
compatibility, isn't that for the better? And we can enforce that if we keep
the same type system we have now. Because currently, source compatibility
implies binary compatibility.

That suggests to me that we would want to allow different versions of a
module. For example, instead of /Int%, we would want /Int-1% and /Int-2%, or
some such. To update the library, you add a module /Int-2%. Anyone on the old
version stays with /Int-1%. New people can pick up /Int-2%. If you want to
make a change, just add a new module. And, I suppose, update all references
from the old module to the new module.

So, maybe we can have a convention. You have a notion of an unstable module
and a stable module. Users can refer to the unstable module if they are
willing to take updates. Otherwise they refer to the stable module that they
want. If there is a stable version for each unstable version, they could
always recompile to use the stable version of the previous working unstable
version and migrate to the new unstable version incrementally.

You could do this on the package level too, where you need a new package
version any time you add or remove a module.

I feel like that's a convention we could enforce outside of the language, in a
package management system? Or maybe, every module has a version number. If you
reference module /Int%, it automatically rewrites that to the latest version
of the /Int% module. Then the developer could add a newer version of the
module without fear of breaking anyone at all, and users can slowly migrate.
The key being that you have to maintain all previous module versions when you
ship a package.

And we could do the same approach when managing package dependencies. Then
package Core version 2 could remove a bunch of deprecated stuff from Core
version 1, and allow users time to upgrade at their leisure, because Core
version 1 is still available. The difference, perhaps being that it makes
sense to reuse the same underlying implementation  for /Int-1% and /Int-2%,
but maybe we want to have two separate copies of Core-1% and Core-2%.

All of which is leading me to the following conclusion:
* All we need is language support for abstract types. We can do all access
  control that way. No need for explicit packages. And let's say we deal with
  package level access control at build time.

In other words, all I need to do is update the syntax for abstract types. I
want:
1. package_type <module_path>
Defines a token type to restrict access to modules of the given path or
submodules of that path.

2. abstract_type <package_type> <type>
Creates an abstract type based on the restricted package type.

3. abstract_value <package_type> <expr>
Creates a value of abstract type by directly wrapping a value.

4. abstract_access <abstract_value>
Accesses the value of an abstract value. Only allowed for modules belonging to
the 'package' of the abstract value.

5. abstract_cast <package_type> <target_type> <expr>
Cast an expression to target type. Only legal if the types are the same up to
abstract types matching the package_type and if the module is part of the
package type.

Let me spend a little time brainstorming the concrete syntax. And then we can
implement it and see how it looks.

Summary of next steps:
* Update language feature for abstract types as above.
* Reorganize source code into packages.
* Add search path for compile time and restructure code and build system to
  only give access to explicitly listed packages for compilation.
* If desired, write down a proposal for versioning modules and packages in
  fble.
* Implement support for .fble.@ header files. I think required of all modules.

---

The language update for abstract types is going well. It feels like a good,
solid improvement to the language.

Next step, I think, is to organize my code into packages, like how we would
imagine them to be developed separately if they were developed separately.

The vision: it will be something like C code. You pass flags to the compiler
with search paths for include and library files. We move source and build for
each package into its own subdirectory. Eventually they could be different git
repos, but for now I see no need to suffer the costs of that.

The tricky part will be deciding how to do the builds for each package.
Specifically:
* Can/should we reuse bits of tcl script for wrapping ninja files?
* What mechanism should I use where ./configure traditionally goes?

The ./configure step is necessary if we want to allow packages to be built in
isolation. We have to have a way to figure out where dependencies are,
including packages we depend on and binaries, like 'fble' that we depend on.

If I want I can make a full build wrapper that calls configure for each sub
project knowing that the dependencies are in the out folder of whatever other
project we just compiled.

What do you think? Should we use ./configure? pkg-config? Autoconf? Is there
any other standard way of finding things you depend on in a system you are
unaware of?

Options:
* autoconf
* CMake?

It looks like ninja is built with CMake as an option, not autoconf. Let's
learn more about CMake. Maybe it's more suitable

Thoughts from reading the cmake documentation:
* Looks like the user will have to install cmake to do the build.
* cmake supports ninja, makefile, windows, mac, etc.
* cmake has command line and gui options, which is nice.
* Um... looks a little complicated, and like cmake wants to be the build
  system as well as the configuration system. And the test system too?

I don't know. I don't like users having to install cmake. It's not clear to me
how easily I can add custom rules. I don't like having all of configure,
build, and test in one. But on the other hand, if I ever want fble to compile
for windows, seems like I'll need to use some more structured system like
CMake?

Or perhaps the real concern should be: can I use CMake for handling fble
dependencies, as opposed to native library/executables?

Sounds like autoconf isn't a great solution for windows. And I know it's
clunky and doing a lot of stuff that I don't really care about. Sounds like
it's hard to make it work correctly in general too.

I could try writing a custom configure script. That will be simple, minimal
dependencies. Unlikely to work for all general use cases.

I really like having the programmability of tcl for my build system.

I think the right approach is to give CMake a try. Who knows, maybe it works
great, and then I don't have to worry about configure and maybe even windows
just works. Maybe I get a test runner for free, and it fixes my issues moving
around files. Or maybe it's just a complete pain and I can't find a nice way
to describe spec tests and code coverage at all. Either way, I learn, right?

Looks like cmake can generate debian packages. That would be cool.

I feel like, ideally, we have the following entities as separate packages:
* The language specification, vim utils, and spec tests.
  Builds and installs documentation, and maybe spec tests distributed in some
  shared data directory?
* The fble compiler, interpreter, and tools.
  Depends on language specification tests for test suite.
  Builds and installs libraries and tools.
  This is the most straight forward thing I would expect we could build by
  cmake. Cmake should definitely work for this.
* Fble packages, separate package for each of:
  Core, Stdio, App, Md5, Sat, Invaders, Map, GameOfLife, Graphics, Pinball,
  BenchmarkGame, Snake, Sudoku, and maybe some other miscellaneous.
 
Uh, that's a lot of little packages. Is that a bit excessive? Maybe Map and
LFSR and Rat could go into Core? I'd rather not though... It would be nice if
little packages were okay. Maybe Snake, Sudoku, TicTacToe, GameOfLife, and
BenchmarkGame could all go into a DemoApps package?

Let's start by trying to use CMake to build fble as is. Don't worry about
separating packages yet. Learn CMake first. See if we can get to parity with
what I have using build.ninja.tcl now.

---

I wonder how we can do code coverage with cmake. Maybe we need to have
conditional code coverage based on availability of gcc, because I doubt that
bit of the build is portable.

Let's move cmake related discussion over to thoughts/fble.cmake.txt.

---

So, I'm running a little out of steam on cmake. Let's consider more
alternatives.

How about I just use pkg-config? Easy way to find compile and link flags. It
supports sdl2 and gl. I can add pkg config entries for packages I compile and
reference them.

And then we have, say, a custom configure script I can use based on
pkg-config. No need for autoconf or cmake or anything fancy like that.
Probably it won't be so portable at first, but I suspect neither is anything
else until I try it on other platforms for real.

pkg-config looks good for use of libraries, but maybe not for finding
executables like gcc or fble. Maybe I can deal with that using a custom
configure script myself. Search for the executable in PATH, and if it's not
there, allow users to specify alternate values.

How would this look if I wanted to go the pkg-config route?

The vision:
* You use pkg-config in your own build commands specifying the other fble
  packages you want to build against. You can build/configure your own package
  however you like, but you're encouraged to install pkg-config files.
* I add a custom ./configure script to fble implementation that uses
  pkg-config to find SDL and GL. And clean up whatever else I feel like I
  need, for the pleasantness of others trying to use fble. I provide a
  pkg-config the fble libraries.
* For each fble package I want, I can create a custom configure script.
* Maybe I ship some helper scripts for building and distributing packages as
  part of the fble package that you can refer to if you want.

And the incremental progress to get there?

Each subpackage will want its own build system and build directory. Or, users
can choose their own build directory, right? So we can set up a top level
build directory for sub packages and install those locally. That installs the
package config files. When we run configure for sub packages, we specify where
to search for the pkg-config paths.

The key part in practice will be reorganizing the git repo to have separate
directories for each separate package, and set up separate build systems for
each separate package.

The fble compiler is going to want to take multiple -I options instead of a
single package path. Same with the interpreter I guess.

---

Next issue: assuming we support multiple -I options, fble-deps needs to
change. It could get the dependencies from the loaded module. The question is:
if we have .@ files, which dependency should it include? That depends on the
tool in question, right? Probably whatever is actually loaded.

It could be we load both .fble and .fble.@ file. I don't know. I haven't work
out the details of .@ files yet.

The other question is, should fble-deps be a separate tool? I feel like no,
because the particular files you read may depend on the settings for how you
load them. Coming back to the question of .fble and .fble.@ files.

So, the proposal is, any binary that reads .fble files should provide a -MMD
-MF equivalent option that records the files it read. That way, for example,
you could use that for when you run fble-test and test cases.

I imagine when you run the compiler on module Foo, it would list all the
include files that were loaded, dependently or independently. For the
interpreter, it would load more, because you need the implementation details.

Two approach to how to get this info:
* Store it with the loaded module somehow reasonable.
* Add an option to FbleLoad to record all the files read.

Here's an interesting question. Say you have a search path with two entries, A
and B, and nothing is in A. So you first check for something in 'A', then
check for something in 'B'. This means, if I add something to A that shadows
B, then it would effect the compilation results.

In other words, we need to add the non-existent path to 'A' to the implicit
dependencies, even though there is no such file there.

Does that work? Is that meaningful? If you don't add it as a dependency, then
if you added a file, you wouldn't see it as a change, and your build would be
out of date.

What does ninja do if I always include a non-existent file in the .d? Does it
always rebuild every time? Ninja gets unhappy and goes into an infinite loop.

I suppose the proper thing would be to include the path to the (existent)
directory searched. That's a little more work to identify, but not unduly so.
Regardless, the point is, we don't store that info in the loaded program. We
need the loader to record it for us.

It doesn't look to me like gcc handles this subtlety.

Could we use strace for this? Does it need to be built in? strace knows what
files we try to read. But again, it doesn't know which ones exist?

strace -e file -o foo.txt <cmd>

Lists all the system calls with file names. Note the file names are in their
original form, not canonicalized. Presumably I could write a script to clean
them up. It includes a lot of shared library search.

I'm tempted to try, as a proof of concept, whether we can mess up dependencies
by adding a duplicate header file name.

Let me try.

fble/src/expr.o depends on fble/src/expr.c and fble/src/expr.h. I claim it
should also depend on the absence of fble/include/expr.h. So, idea is, if I
add fble/include/expr.h, ninja doesn't try to rebuild. If I rebuild from
scratch, the whole project fails to rebuild.

If that doesn't work because I got the order mixed up, try adding
fble/src/fble-alloc.h instead.

Yup. fble/src/fble-alloc.h shows the bug. Ninja does not attempt to rebuild
when fble/src/fble-alloc.h is added, but if you then do rebuild, it fails to
build.

I think ninja can get away without being blamed for this, because it's just
doing what it is told by the .d files. Maybe gcc can get away for the blame
because it doesn't claim to support this use case.

So, what do we do about this case? Just ignore it? Don't worry about it?
Handle it correctly by adding directories to the generated dependencies? Use
strace to generate .d files so we know it's correct and handles other
potential issues, such as new library files?

I wonder if ninja has a discussion list I could ask about this on.

---

Let me do the following:
* For now, stick with what gcc does: just add to dependencies the files we
  read.
* I can improve that a little by adding directories we scan later down the
  road.
* Let's make dependency tracking part of FbleLoad. That's a natural place,
  because FbleLoad knows what files it is searching for and opening.
* We can reuse the fble-deps approach now, and later on, if so desired, change
  it to something all the relevant fble tools support directly.

---

All the places where I want to change a single search path entry to use -I:
* fble-profiles-test
* fble-deps
* fble-compile
* fble-disassemble
* fble-stdio, fble-md5, fble-test

Which really means:
* FbleMain, fble-deps, fble-compile, fble-disassemble.

Let's switch them all and see how goes.

Is there anyway I can share the code for all of them? Like, have some way to
parse argv/argc for -I and module path values, but allow other stuff to be
added? Specifically the other options are 'target' for fble-deps and
'--export' for fble-compile.

I don't see a clean way of composing them. For either to skip over the other
args requires knowing the semantics of the other args. Unless we require the
user to put all app-specific args at the front or the back and all generic
args at the other side. Sounds messy.

I'll just brute force it for now, see how bad that is, and worry about it
later.

Yeah, it's a pain to have so much duplicated options parsing code. That would
be nice to standardize and reuse somehow.

How about using getopt? Looks like it's part of POSIX standard. Maybe we can
have FbleMain supply some part of the options specification and ... something?
I'll have to figure out how getopt works first to see what makes sense in
terms of separating code for standard fble options and executable specific
options.

I think the first step is to switch everyone over to getopt, and worry about
sharing logic later. But for sharing, it looks like maybe getopt parses
options in the order passed in the spec string, reordering argv as needed. In
that case, perhaps we could write a function that parses standard options out
first? Or callers could parse their own options out first? It's really a
question of whether I want things to be passed as options with flags or direct
values. If everything is an option, it's easy. If some things are direct
values, whatever thing wants to parse direct values probably needs to parse
the command line last. In other words, the common parsing logic wants to go
last to parse the module as a direct value instead of a flag value.

---

Okay, so this is a bit of a digression, but how should we do option parsing?

There are three levels:
1. What's an easy way to write the option parsing code.
2. Can we have a single place to describe standard options?
3. Can we reuse the same code everywhere for describing standard options?

Survey of current command lines:
* Standard options: [-I DIR ...] MODULE_PATH
* fble-md5: FILE + standard options
* fble-stdio: [--profile FILE] + standard options + ARGS
   And I just realized I broke this with my recent change.
* fble-app: [--profile FILE] + standard options.
* fble-disassemble: standard options.
* fble-mem-test: [--growth] + standard options.
* fble-deps: target + standard options
* fble-profiles-test: standard options
* fble-test: [--compile-error | --runtime-error] [--profile] + standard options.
* fble-compile: [--export NAME] + standard options
  But ideally with a way to say export only, no compile.

Thoughts:
* Can --profile be made a standard option?
* Notice that both standard options and a few other cases want to specify the
  non-option argument.
* There are some very command custom options in some cases.

With regards to using getopt, I don't think we need to support things like
bunching options together and supporting -Idir in addition to -I dir. I don't
think it will work out well in the long run to stick to single character
options. Better to support the better documenting long options. Maybe short
options too, but ... yeah.

With regards to non-argument options... it feels nice to me to specify module
path as the non-argument option, like when passing a file. But maybe it's
worth turning into an option, just so standard options are all normal options?
It's not like the module is a file. We could use something like --main <>, or
--module <...> as the option.

I think fble-deps 'target' could easily be made an option: --target TARGET.

Passing --module as an option fits well with compiled binaries, where you
wouldn't list the module anyway. Things like fble-md5, fble-stdio, fble-app.
It's a little weirder for things like fble-compile or fble-disassemble, where
you think of them as taking the module as the main input.

I think --profile should be a standardized option to be used whenever you run.
Hmm... but it doesn't make sense for when you compile. So maybe we should have
standard run options and standard compile options, possibly with some overlap?

Ideally you can mix orders of options. They can come in any order, regardless
of whether they are standard options or not.

Ideally users can reuse standard option parsing with whatever mechanism of
parsing options they want. I'm fine if the specific tools I write reuse a
custom framework for options parsing, but I don't want to require everyone to
reuse that custom framework for options parsing.

I don't know. Sounds hard.

Anyway, proposed changes to the user command line interface, regardless of
parsing implementation:
* --module MODULE instead of MODULE_PATH directly.
* --profile FILE instead of --profile for fble-test.
* --compile option to fble-compile to indicate if compilation is requested or
  just exporting.
* Ideally you can reorder flags and mix flags with non-option arguments.
  Though I think it's fine to require options before non-option arguments.

---

Okay, here's what I'm thinking for reusing command line parsing. The top level
executable is responsible for the main argument parsing loop. It looks
something like:

while args remain
  if is arg X
    parse arg X, advance args
  else if is arg Y
    parse arg Y, advance args
  ...

We can factor out the if/parse/advance for a particular arg into a function.

In general the parse function for a particular option or set of options has:

Inputs:
  argv, argc pointers to read and update.
  output pointers to parsed options.
    For example, FbleSearchPath*, FbleModulePath**.

Results: 
  NO MATCH - if the argument isn't recognized by the parser.
  FAILURE - if there is an error in the option recognized by the parser.
  SUCCESS - on consumption of an argument, in which case argv, argc, and
            output values are updated.

Maybe, for example:

bool ParseStandardOptions(bool* error, int* argc, const char*** argv,
                          FbleSearchPath* search_path, FbleModulePath** module);

Where returning 'false' means no match, returning true means match, which may
or may not result in 'error' being set to true. argc and argv and everything
else gets updated. So you would write something like:

  bool error = false;
  while (!error && argc > 0) {
    if (ParseStandardOption(&error, &argc, &argv, &search_path, &module)) {
      continue;
    } else if (argc > 1 && argv[0] == "--profile") {
      profile = argv[1];
      argc -= 2;
      argv += 2;
      continue;
    }
    ...
  }

  if (error) {
    PrintUsage();
    return EXIT_USAGE;
  }

Do we need to use if/else or check the result? If it matches, what harm in
trying to advance? Unless, I guess, it fails to parse? Well, because we want
to be able to detect 'option that didn't match anything else' as an error.

We could write some macros to hide the if/else/continue syntax. Though to what
end? It's not that expensive, and why obfuscate things?

---

I tried updating fble-deps to have nice options. My take aways:
* It's nice having short and long options.
* It's nice having general option parsing support that allows things in
  different orders and gives nice usage error message.
* It's 80 lines of straight forward but repetitive and slightly tedious code
  to implement the options parsing.
* It's nice having cleaned up options and options parsing.

The next question is: do I want to try and abstract away some of that options
parsing logic? The trouble is it is awkward, because it interacts with control
flow (exit 0, exit usage error, or continue), it interacts with variables we
want to set, it interacts with things we need to clean up in case of exit.

I think it's fine to read the options. Just a little annoying to write them.
Could I easy factor out option parsing into a separate function? Then again we
have the question of whether we want to exit 0, exit non-zero, or succeed. And
whose job is it to initialize and clean up things like FbleSearchPath as we
build it up?

Ugh, there are a lot of details of various error conditions to remember and
get right. I really ought to factor out the code into an options parsing
library so I don't have to keep duplicating this logic.

---

Thoughts on the library approach:
* For --help, return a boolean help option that the main function can read
  after all arguments are parsed. No need to have special return immediately
  handling for it.
* For allocation/cleanup of args, let's say that's the responsibility of the
  caller.

So I'm imagining the following. We have an Option type that takes maybe a
couple of strings for the flag names, a function to do the parsing, and a
pointer to a void* for where to set the results. We define a function for each
kind of option parser that produces a value of type Option. We specify the
list of options using a vector of Option.

The user is responsible for initializing data structures and passing pointers
to them to the Option functions. The Option implementation functions need not
now about the flags names. We can have a function that does a parse all given
a list of options. Maybe we can have a function to do documentation too, if we
want to include documentation along with the Option data structure. Or that
can be done separately.

---

For my first attempt, just defining some parser functions for parsing boolean
flags and string flags factors out a lot of the messy logic. Maybe that's
enough. The signature is something like:

bool Parse(const char* name, T* dest, int* argc, char*** argv, bool* error);

Where T is whatever relevant type you are trying to parse. Then, the top
level is as easy as:

while (!error && argc > 0) {
  if (Parse(...)) continue;
  if (Parse(...)) continue;
  if (Parse(...)) continue;
  if (Parse(...)) continue;
}

if (error) { 
  ...
}

All the yucky logic is factored out, but I think we retain all of the
flexibility and compatibility that I want. Let's start with this route and see
if that meets my needs.

In theory, we could define a ParseStandardOption function that knows how to
parse a collection of things that we can then pass to FbleMain.

You don't get automatic documentation, but I think that's fine. It's nice to
separate the specification from the implementation in some sense.

Some clean up:
* Rename to fble-arg-parse.h
* Implement FbleSearchPath arg parser.
* Maybe document sample use at the top level, and general invariants.
* Use FbleSearchPath parser in fble-deps
* Switch fble-compile to use library routines.

---

Next step is to update uses of FbleMain. But it's not clear how that should be
done. I'm kind of thinking FbleMain shouldn't exist at all.

For example, take fble-app.c. This wants to be the main program. We compile
different apps by defining different values of FbleCompiledMain and
recompiling. That requires the compiler have access to both fble-app.c and
FbleCompiledMain. But if we split things into packages, we don't want to ship
fble-app.c from the App package. We want to provide it as a library that
someone else can call.

So, either we let fble-app do all the option parsing, and make it very easy to
write the wrapper function. Or we have the wrapper responsible for option
parsing, passing preloaded values to fble-app.

I'm thinking maybe it's better to have fble-invaders.c, fble-graphics.c,
fble-pinball.c, fble-tests.c, fble-bench.c, fble-debug-test.c. It's just,
that's a lot of wrapper programs to have to write, which is kind of tedious.

I think the next step for modules is going to be tedious.

I want to split up prgms into different directories for each package. This
means updating the build system. In particular, updating the directory
structure of the out directory.

What if I took a more generic approach to the out directory, and build
everything in a subdirectory parallel to the source directory? That way we can
avoid name conflicts, and everything feels like it's in a good place. The
downside is we don't have a nicely organized bin and lib directories.

I'm not really making use of bin and lib directories yet though, except to
make it easier to run executables by hand. And eventually I would expect that
to be part of a separate install step.

Okay, can I try that? Somehow make it work incrementally?

---

Another random idea to explore with regards to organization of separate
packages together: could we use git submodule somehow? What's the expected
flow there? For example, maybe the 'core' package depends on the 'fble'
package via git submodule? I don't think git submodules has a solution here,
but it's worth taking a look again to understand that approach.

The motivation at https://git-scm.com/book/en/v2/Git-Tools-Submodules looks
like exactly the problem I'm having: "you want to be able to treat the two
projects as separate yet still be able to use one from within the other."

Git submodules looks complicated and error prone to me. They don't talk at all
about how to do build across submodules.

I know that conceptually I want things to be completely separate packages,
that can be checked out and built and installed separately. What causes me to
hesitate is:
* How to manage benchmarks, which I want to use for libfble development, but
  depends on random things I wouldn't want libfble development to necessarily
  depend on, like the pinball game.
* Manually having to do configure, make, make install for each subpackage any
  time I change something.
* Abstracting build dependencies, would could cause excessive rebuilding and
  false dependencies.
  - Think recursive makefiles.
* Version management becomes complex if we don't have every package perfectly
  aligned based on git revision.
* Duplicating build system logic.

The thing is, I kind of feel like these are all pretty fundamental real world
issues. These are the reasons I want to organize things into packages, to
understand and address these real world issues that will come up in the real
world if anyone wanted to use fble as a language for real.

The benchmarks issue seems like a special case. In reality, we should probably
have a separate benchmark suite with a copy of code, rather than depend on
libraries that could change their implementation whenever. That way the
benchmark suite is a single separate package that freezes benchmarks in time
when they are ready. libfble development may want to use benchmarks to aid in
development, but that would be outside of the development of the package
itself. As in, separate integration testing, benchmarking stuff that doesn't
show up in the repository. Which is a little bit awkward. If it's not in the
libfble repository, where is it? It's almost like we want a 'release' view of
a package and a 'development' view, and the 'development' view can have
dependencies on other packages that have dependencies (e.g. benchmark package)
on the 'release' view of libfble.

Manually having to do configure, make, make install, maybe make test for each
subpackage any time I want to change something? I can define some wrapper
scripts to do that for me if I want. Manually configure, or write a script to
configure, each of the subpackages. Add a make command that calls make in
turn. Sounds like recursive make. Also similar issue as previous: it's like I,
as a developer, have different requirements than we want for the release. But
if there are other developers, they should have access to those scripts and
things that I have, but not require access to those for doing releases.

Abstracting build dependencies. Sounds like recursive make. Not great if I'm
working on a bunch of different packages at once. The other approach would be
to say: don't work on a bunch of different packages at once. Work on one at a
time. If I ever want to update the version I depend on from one package to
another, that's something explicitly I do from that particular package. That
suggests we may want to install for real on my system and work on each package
independently. Doing updates that span across multiple packages, like API
changes, is going to be a pain. Especially if I want the feedback: if I change
this API in core, will it satisfy the needs of pinball game? Well, create a
new version of core, release it, update pinball game to pull that new version,
if it doesn't quite turn out right, then oops. Bad release of core. Go back.
That's not a very pleasant situation to be in. This suggests we really want
some stability with these packages before splitting them up, otherwise the
overheads of dealing with updates is too high.

Version management: again, we want enough stability that versions aren't
changing that much, and that packages can be developed independently in
practice.

Duplicating build system logic is maybe okay, because the logic isn't that
complicated. Or I can make yet another project for storing just the build
system logic.

It feels to me like all of these issues are different views of some common
underlying thing about package stability and independence. If the packages
never changed, sure we could make separate packages. But if we are changing
them often in non-compatible ways, it's just a big massive pain to have them
as separate packages. In that case, conceptually, they really can't be
considered separate packages.

So maybe the way to think about it is a package can be experimental or stable.
It only makes sense to pull out a package as a separate package for real when
it becomes stable. Otherwise all the experimental packages developed by the
same organization, in particular depending in any way on each other, should be
developed as part of a single monolithic package.

---

Interest read on versioning in Java here:
https://docs.oracle.com/javase/7/docs/technotes/guides/versioning/spec/versioning2.html

Some interesting ideas from the doc:
* specifications can be versioned by number with clear order: an update to a
  specification can be backwards compatible with a previous spec.
* Implementations cannot be versioned by number with clear order: if you fix a
  bug, that's a backwards incompatible thing to do.

---

I think the real challenge of what I'm trying to do is this: As soon as you
release a package for someone else to use, you suddenly have much stricter
requirements for what can change, how often, and when. That's the hard part
here. If I want to make fble or core a separate package, it has downstream
dependencies, so it is much hard to make any changes.

In terms of compatibility for fble, I still think the only way you can be
compatible is:
* change the implementation without changing the type of a module.
* add a new module.

And maybe we should have a set of conventions based on that. Namely, you want
to include the major version number of a module in its name, and the major
version number of a package in its name. You keep all old module versions
(source/binary/type compatible) when updating a module to a new version, which
doesn't require a package version update. Only package version updates allow
you to remove deprecated modules. Different packages in the same executable
can refer to different versions of a package and different versions of a
module. The thing that is shared is that, for example, a package foo could
depend on a package bar major version 1, and work with package bar major
version 1 with any minor version.

But I don't want to worry about all of that right now?

---

How to know what should be in the same package or a different package?
* I would argue fble-stdio should be part of Core, because you need it to run
  tests. There's no point for tests to record error messages if you can't
  display them somehow.
* Presumably fble-app shouldn't be part of core, because it adds an additional
  dependency on OpenGL and SDL. So maybe packages can be like modules in the
  sense of doing things based on package dependencies.
* fble-pinball should clearly not be a part of core, because it is an end
  application, and we don't want changes to it to impact users of core who
  care nothing about it. Same for TicTacToe, Sudoku, Snake, Invaders, Sat.

How about Map, LFSR, Rat? Those are not end user applications. They are
intended to be reused. I could make a strong case for Map to be in Core just
like Maybe is. Although Map is a bit more complicated because there are so
many different ways you might want to implement a Map. Most of the other
things in Core aren't like that? So what, you don't want to put something in
Core that would have a name conflict with something you want to use a
different implementation for? But we have 'Core' in the module path, so that's
not a legitimate reason.

* Namespace conflict prevention is not a good reason to leave something out of
  a package, because the conflict would exist whether or not they are in the
  same package. Unless you want, say, two different map implementations, and
  they should go into two different directories to avoid name conflict.
* How about size? Is there a downside to having a large number of unused
  modules in a package? Size of .so, size of install. But in my case these
  things are small. Maybe compilers can be smart enough to only link in those
  modules that you depend on? That would be ideal.

How about Md5? It is an end user app, but also something you could imagine
people would want to reuse.

* Presumably there is some notion of developer and release cadence. A single
  package has to be updated all or nothing. If you want to be able to release
  updates to a specific part of a package at a different rate than other parts
  of a package, they would be better as separate packages.

Here's a summary of points to consider for defining package boundaries:
* Different packages can be updated at different times and rates.
  If that's something you desire, then split into separate packages.
* Avoid false package dependencies.
  Ideally most modules in a package have the same external package
  dependencies. It would be bad if a single module introduced a false package
  dependency for all the other modules in a package.
* The core package should be standalone testable.
  This is a special case for the 'core' package.

Is that enough? That would suggest to me we have the following:
* Core: includes Stdio, Map, LFSR, Rat
* App: Separate from Core because it adds dependence on OpenGL, SDL.
* Pinball, Snake, Sudoku, etc.. separate from App because we want to make
  updates frequently without requiring users of App to recompile.
  These can be each separate packages, or grouped together into an 'Apps'
  package. I might suggest: {GameOfLife, Snake} together as silly graphics
  apps, {Sudoku, TicTacToe, BenchmarkGame} together as silly stdio, Invaders,
  Pinball, Graphics, Sat, Hwdg all as separate packages. Or just everything as
  separate packages, because you never know if I want to make a graphics
  version of sudoku or tictactoe for example.

Or maybe don't include Map, LFSR, Rat in Core, under the idea that 'Core'
should be more stable than those? Like, level of stability is important for
Core? Maybe distinguish between Core-Stable, Core-Development,
Core-Experimental?

---

Trying to clean up fble-stdio compilation, in the sense of making it a library
to share instead of .c file to share. First question is, how to distinguish
between args to the fble-stdio runner and args to the Stdio@ function?

For example, -I, -m, --profile would be for runner. Anything could be for
Stdio@ function.

How does tcl or python deal with this?
* tclsh has exactly 1 special argument, then file, all others go to args.
* python looks like anything after a particular argument is treated as args.

That seems to be fairly standard. Options to the runner, followed by a
particular option or non-option argument, then everything after is args to
pass down.

The fble-stdio things I build today:
* fble-stdio interpreter
* fble-stdio-test
* fble-tests
* fble-bench
* fble-debug-test

Usage is: [--profile FILE] [-I ... -m MODULE_PATH] ARGS

One approach I could use is: the first unrecognized argument is considered
ARGS and passed there. But we should also allow '--' to be specified as a
separator, otherwise you can't necessarily pass things like '--profile' in
ARGS.

All the compiled fble-stdio things have the same command line. The interpreter
is slightly different, because it has -I and -m options that need to be
provided. Hypothetically we could compile things to shared libraries and pass
-I a .so file and dynamically load compiled code. So the real difference is
whether the module to run is hard coded in the executable or not.

I'm tempted to provide an fble-stdio-main library that provides an
implementation of main and assumes you make an FbleStdioMain symbol available
at link time. It can use standard parsing options. The fble-stdio executable
will have special command line parsing for -I and -m and module loading. Both
can share common FbleStdio library function for setting up and running a
Stdio@ value.

Can we do linker tricks to make anything nicer? Maybe not require a separate
fble-stdio-main library? Seems silly. I vote for fble-stdio libs as part of
whatever package I use for fble-stdio, and fble-stdio-main library that
includes just the main program for fble-stdio.

Proposal:
* stdio.fble.h, stdio.fble.c - library to go in libfble-core.a. FbleStdio.
  Defines fble level interface to run a Stdio@.
* fble-stdio.c - interpreter executable. Defines main.
* fble-stdio-main.c - generic compiled executable library. Defines main.
  Requires FbleStdioMain to be defined. We turn this into a separate
  fble-stdio-main library that people can link with (assuming they provide
  FbleStdioMain as the exported function).

---

I'm not convinced about fble-stdio-main yet. The sticky thing is, what should
the usage message be? Doesn't that depend on the actual compiled application?

So, maybe instead I define a FbleStdioMain program that can be used to
trivially implement a main function in these cases. They have to use the
standard options for fble-stdio, namely --profile. Otherwise they pass as an
argument the description of the program (and exit status?), and the module
function to load from (or already loaded module function?).

Or, should usage be printed by the Stdio@ program? Then how do we deal with
the --profile option?

I like the idea of --help being handled by the Stdio@ program. Or at least the
option to. In that case, we could use a standard usage description. Does that
mean fble-stdio-main library still makes sense? Or should we require people to
define their own wrapper functions for main? They would be trivial:

int main(int argc, char** argv) {
  return FbleStdioMain(FbleTestsModule, argc, argv);
}

With a couple of lines of imports. It's just, why make people write this? We
could do it with command line. All we need is to specify the name of the test
module?

---

Here's my proposal:
* stdio.fble.c defines an FbleStdioMain function that takes the compiled
  module to load, argc, and argv. It does all command line option parsing,
  using a generic usage message.
* Add a --main option to fble-compile to generate a 'main' function
  automatically via wrapper function. You specify the name of the wrapper
  function. If you want a Stdio@ main, you do --main FbleStdioMain. You could
  do FbleTestMain, FbleAppMain, etc. Any user defined function, as long as it
  has the right signature.
* The FbleStdioMain function accepts a NULL module to load, in which case it
  adds -I and --module options to read that from the command line and be
  interpreter.
* fble-stdio.c is a simple wrapper around FbleStdioMain with NULL module to
  load.

No need for sharing .c code, doing weird defines, having a separate library
with main, or doing wrapper functions for every thing you might want to make a
main executable. This solves the fble-test case. It also would make it
possible to write main fble executables in fble from the compiler without
having to write any C code at all.

Let's give it a try.

---

I'm trying to make an 'app' package, which I thought should only depend on
'core', but it uses /Map%. Is that enough justification to add /Map% to the
'core' package? Do you see any harm in that? It seems like a pretty standard
thing to have a Map implementation, no? Maybe core should be used for common
useful reusable utilities?

As long as it doesn't bring in extra dependencies like SDL and doesn't contain
stand alone not reusable things like Sat, Pinball, etc? I think that's
reasonable. So LFSR, Maybe, Map, Rat all make sense in there. Md5? I'm not
sure.

I suppose the downside of adding something to 'core' is:
* We may start depending on that thing in other parts of core.
* We encourage people to use that as a standard interface thing instead of
  other, perhaps better designed and supported types.
* Extra external dependency requirements (easy: just say no).

Or, thought of as another way:
* Packages produced by different organizations should be separate. Easy.
* Packages that are small enough need not be split into subpackages.
  So, map, sat, md5, lfsr, etc. have no motivation to split finer grain.
* There is overhead of splitting into separate packages - you have to keep
  things in sync version wise between packages.

I like the idea of 'core' being standalone testable. Let's review the maximum
set of packages I expect again:

* core, maybe, map, app, gameoflife, tictactoe, sudoku, pinball, graphics,
  hwdg, benchmarkgame, rat, invaders, md5, lfsr, fble, sat

And things I'm happy to make separate packages:
* core, app, pinball, graphics, hwdg, invaders, sat

Questions are:
* Should maybe, map, lfsr, rat, md5 be part of core?
* Should gameoflife, tictactoe, sudoku, benchmark game be grouped together?
* What to do about 'fble' package?

So, looking at each questionable package:
* maybe: put in core and be done with it. If list is in core, maybe should be
  too.
* map: put in core and be done with it. I see no significant value to having
  it as a separate package, but it would be convenient to have as part of
  core.
* lfrs: put in core and be done with it. I see no significant value of having
  it as a separate package, but it would be convenient to have as part of core.
* rat: This is unused, right? Let's get rid of it.
* md5: It uses a non-core Bit8@ type and isn't reused anywhere currently.
       Let's make this a separate package.
* gameoflife, tictactoe, sudoku, benchmark game - let's group these together
  into a 'games' package, for demo/small/benchmark games. They can be stdio
  and/or app games.
* fble: let's keep this as a separate package targeted at libfble development.

---

Observation: Looks like pkg-config cflags may not work for fble compiler
flags, because it includes extra things like -D_REENTRANT and uses compiler
flag formats like -Ifoo instead of -I foo. That's too bad.

Wait, how about using cflags-only-I? Isn't that exactly what we want?

I need to understand this a bit better. What are the dependencies when
building packages?

Example: package C depends on B, B depends on A.

* -I flags need to compile C fble modules and .c source: -I B.
  We do not need -I A.
* -L,-l flags need to link C library: ??
  Do you need -L flags to link a library, or only the user of the library
  needs it? Only the user needs it. So, answer is none.

* -I flags needed to compile executable depending on C: -I C.
  We do not need -I B or -I A.
* -L,-l flags needed to link executable depending on C:
  -L and -l for all of A, B, C, and -lfble, and any other libraries A, B, or C
  may depend on.

Okay, so question is, what does pkg-config give us for -I and -L flags?
Ideally -I is shallow and -L is deep?

pkg-config has a --static option to include private libraries. So I guess we
want that C privately depends on B, B privately depends on A, and use --static
in pkg-config. That way -I flags don't recurse but -L flags do?

Yeah, looks like I should be using Requires.private, not Requires.

If I do things this way, we should be fine to use pkg-config for fble compile
flags as well as C compile flags. Maybe we only need to support -Ifoo syntax
if that's what's given in the .pc file?

Let me try to clean this up. Summary is:
* Use Requires.private instead of Requires for dependencies.
* Use -I foo syntax in .pc files.
* Use --cflags of packages we depend on when compiling a .c file for a
  library.
* Use --cflags of package when compiling a .c file for an executable.
* Use --static and --libs for package we depend on when linking an
  executable.

Let's see if I can specify all the cflags and libs this way in the build.ninja
file.

Um, I think all fble packages depend privately on fble, to link with the
runtime for compiled code.

Looks like we have another case I missed.
* When compiling a .fble module for package foo, we want to include -I for
  dependencies and for -I foo. That's something I don't think pkg-config will
  give to us in one invocation.

Experimenting with pkg-config
* Looks like -Ifoo form is needed, otherwise pkg-config rearranges them.
* -I flags are output recursively, even for Requires.private modules.
  Which is not what I expect or want. Or is it? Actually, maybe it is.
* libs works as expected: --static to get recursive, otherwise non-recursive.

Do we want -I flags recursive? If I reference /Foo.fble, and that references
/Bar.fble, then I need /Foo and /Bar, even if I'm only using /Foo. Okay, so
that works as expected then.

Updates:
* I'll want to support -Ifoo form in the compiler. Maybe make a short form
  flags parser thing and use that for -I.
* I'll need to go back to -Ifoo from in the .pc files.
* For compiling self modules, we can use our own cflags from pkg-config, which
  includes self and dependencies.
* That means, because of things like SDL, we may need to filter just -I flags.
  But SDL is a private include, right, so it shouldn't be included? But we
  need to link. It's a funny thing, we need to link, but don't want to bring
  in the -I flags. Oh well.

---

We've made good progress. Source code is reorganized into package now. We have
a compile time search path and restructured code in the build system to only
give access to packages you depend on.

Next steps:
* Clean up build.ninja.tcl to put package related code together.
* Consider splitting build.ninja.tcl into per-package .tcl files.
* Implement support for .fble.@ header files.

I don't feel the need to really do separate packages yet. Also, there have
been a lot of changes recently, so I might hold off on .fble.@ header files
for a while, just to give things space to breath.

