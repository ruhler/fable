Fble Generate C
===============
It's been a while since we tried generating c code for fble compilation. Now
that we're considering cross platform support, seems like it's worth trying
again.

The problem we had with generating C before was that it took too much memory
(>1GB) to compile the generated C code. A lot of things have changed in fble
since then. Perhaps most importantly, we pass list and literals all the way
through the compiler instead of desugaring them first. Also, we are single
threaded now and have a standard way of supporting tail recursion at the C
level.

The benefits of generating C code are:
* Support all platforms we can build fble for without any extra dependencies.

The potential disadvantages of C code, compared to custom assembly:
* Compile time memory and performance costs.
* Runtime performance costs.

My vote is to give it a try, again. See if we can get past the compile time
memory costs and how close we can get to runtime performance. This will help a
lot for cross platform support.

How about I do it from scratch, ish. Based of generate_aarch64.

Initial thoughts looking over generate_aarch64:
* I think there's a lot of code we can reuse from aarch64. My vote is reuse by
  copy-paste, under the assumption we keep either one or the other in the end,
  but likely not both.
* Function args: heap, thread, locals, statics, profile
* Don't worry about debug info to start.

There will be some tricky parts, but for the most part, I think it should be
straight forward.

First step: wire up some stub code and make it so I can run it.

---

How can we do debug info? Like, what does bison do in its generated code?

We have pragmas like:

#line 3 "fble/lib/parse.y"

Let's start with that then.

---

I'm getting close. Some observations:
* The #line pragmas are making debug the code generation logic annoying. Can I
  get rid of them for now? I don't think they work the way I think either. I
  think we need a separate way to get dwarf.
* I'm getting real mixed up on what GenerateCMain and GenerateCExport should
  do.

GenerateCMain:
int main(int argc, const char** argv) {
  return <main>(argc, argv, <compiled module>);
}

Type of <compiled module> is FbleCompiledModuleFunction.

That looks fine.

Next, GenerateCExport: it should take a heap as input. What does it do? I
guess it should call the module function, passing the heap argument?

Maybe my comment in the documentation is wrong? The assembly code is literally
branching to the module function and returning the result. It must be passing
all the arguments directly.

Uh... I'm lost.

FbleCompiledModuleFunction is a function that, given a program, adds the
module to that program. That's what we define. What's what we call to ensure
dependencies are loaded, right? That's what we pass to a main function to be
able to instantiate a program. Export 'main' invokes main function on that,
and that's what we generate code for. That all makes sense.

What I don't understand is the CExport function. What type is that supposed to
have? I assume it's supposed to be a wrapper something or other?

According to docs, the exported function should return a zero argument fble
function that can be executed to compute the value of the given module. How
does it work? I don't understand what generate_aarch64 is doing. That can't be
correct, can it?

Step back. How would I generate the FbleValue in the first place?

I think the documentation is wrong. That must be out of date. It should just
give a nice name to the FbleCompiledModuleFunction with the internal name,
that is: void X(FbleExecutableProgram* program), right? Yeah. That makes
sense.

Next issue: Need to generate prototypes for all Run and Abort functions up
front.

Notes:
* Double check if spec tests are using the c target or aarch64. I think it may
  be different.
* Will want to clean up includes of execute.h, value.h at some point.
* Memory use for compiling the generated c code is looking not too bad so far?
* Runtime for compiling the generated code isn't great.

Actually, we're starting to hit issues with gcc memory use to compile the
generated code. Let's see if I can find out which one it is.

  out/pkgs/md5/Md5/Rounds.fble.c

It's 100K lines of C code. Ugh.

Well, something to dig into next:
* Why is Rounds.fble.c so big. Anything we can do to cut that down by an order
  of magnitude?
* Try running just the spec tests with -c target, see if everything passes.
* Try running some program manually that's compiled, just to convince myself
  it actually works.

One thing that stands out scanning through Rounds.fble.c is a whole lot of
struct accesses, each of which has an error case. And a function with a lot of
instructions, which leads to a pretty big case statement and per-instruction
overhead.

---

Seeing if I can add c code generation to spec tests at least. Looks like they
expose some more bugs. A couple of bugs fixed. All the spec tests pass with
the compiler. Let me figure out how to do both aarch64 and c tests in the spec
tests in a hopefully clean way.

Yeah. This works well. We can abstract away all the different targets and make
sure we test them all in spec tests. Or, at least, all of the available ones.

It might be fun to try compiling everything with generate_c on my windows
computer, which is x86_64 and has lots of memory. Just to see if it works and
maybe try running some benchmarks to compare interpreted, aarch64, and c.

---

The compiler appears to work fine on my windows computer. Max memory use I see
for compilation is around 500MB, which suggests perhaps we're close to being
able to do it on my Raspberry pi. Things that need to change to be able to
check this in:
* Distinguish better between hand written c code and generated c. In
  particular, we need different names for object files in both cases, e.g.
  Stdio/stdio.fble.o could be from Stdio/stdio.fble.c or Stdio/Stdio.fble.
  Don't assume case is sufficient to distinguish them.
* Use fbleobj instead of manual invocation to fble-compile where possible.
* Figure out how to decide which approach to build with: one, the other, or
  both.

It would be great to set up some benchmarks to understand how performance is
looking on c versus aarch64 target.

The path to getting rid of aarch64 target:
* Check in cleaned up changes to avoid name conflict in stdio.fble.o file and
  friends.
* Get it to compile on low memory.
* Setup benchmarks, check if they are within, say, 20% of aarch64.
* Figure out how to emit debugging info via C approach.

---

Goal: See how close we are to being able to compile everything with 1GB RAM,
and what we could do to get us closer.

Let's see how much memory it actually takes on what files.

Part of the challenge is, even the ones that don't take a ton of memory can
take multiple minutes to compile. Any way I can profile that? Maybe ninja -d
stats?

There are 4 culprits:

/Md5/Rounds%
/Sat/Aim%
/Invaders/Graphics%
/Graphics/Camera/SinCos%

Let me let them run for longer and see if any come close to finishing.

I clearly can't run all four at the same time. Let's try one at a time:

/Md5/Rounds%: Goes up to 60% memory and then completes okay.
/Sat/Aim%: Goes up to 72% memory and then completes okay.
/Invaders/Graphics%: Goes up to 45% memory then completes okay.
/Graphics/Camera/SinCos%: Goes up to 40% memory then completes okay.

So we're close. If I could get an order of magnitude better. Or even just
compile all of these one at a time, I maybe could swing it.

Let's see if number of lines of C code correlates here. It would be nice if we
could use that as a metric to try and improve.

Yes. That's shows it pretty clearly:

   108797 out/pkgs/md5/Md5/Rounds.fble.c
   105934 out/pkgs/sat/Sat/Aim.fble.c
    76723 out/pkgs/invaders/Invaders/Graphics.fble.c
    60029 out/pkgs/graphics/Graphics/Camera/SinCos.fble.c
    34514 out/pkgs/graphics/Graphics/Triangle/Tests.fble.c
    22824 out/pkgs/core/Core/Int/Tests.fble.c
    22319 out/pkgs/pinball/Pinball/Tests.fble.c

The top four. Looks like 40K lines maybe a good target to shoot for.

Where are all these lines coming from? Let's review the modules and see what
they do.

Rounds:
* Lots of hex literals.
* Lots of big structs.
* Lots of deep chains of struct accesses.
* C code shows lots of struct access instructions, including check for
  FbleStrictValue and corresponding error message.
* Some functions with 5K or more instructions.

Aim:
* Lots of int literals.
* Long nested list literals. There are no struct accesses at all.
* Some functions with 5K or more instructions.
* Lots of aborts and error messages.

Invaders Graphics:
* Lots of int literals and list literals.
* Some functions with 5K or more instructions.
* Lots of error messages.

Graphics SinCos:
* Lots of int literals.
* Long list literal.
* Some functions with 5K or more instructions.
* Lots of error messages.

Note: none of the fble files are particularly large. I expect all of these
should compile just fine.

Brainstorm of things to try:
* abort on error instead of have error handling.
* Remove 'abort' function implementations.
* Remove profiling code at every instruction.
* Use different options to gcc to optimize for memory somehow?
* Use initializer list instead of assignments for literal instructions.
 - Also for "statics".

What sticks out to me is:
* From fble side: lots of literals. See if I can improve code generation for
  that.
* They all have 5K+ instruction functions with lots of abort cases.

Let me start with something easy and noncontroversial: Generate as many arrays
using initializers as I can rather than separate assignment statements. It
won't reduce lines of code, but hopefully it helps with something inside. I'm
not sure if I expect much, but it's worth a try.

I don't see any memory improvement from this change. I like it anyway though,
so I'll keep it.

Next thing to try, just because it's easy to try: put all these array
initializers on a single line instead of spreading across lines. Clearly the
structure of the code is identical. But I'm wondering if the memory is
proportional to number of lines in terms of, perhaps, debug info. This
approach should drastically cut down on number of lines. I can see by how
much. And then we can see if it helps with memory.

If it helps with memory, I'll take it. If not... I'll look at some of the code
and see how hard it is to digest.

Looks like it actually doesn't make a difference in overall line counts, aside
from a few thousand lines saved. The code looks good. Longest line I'm seeing
is 255 lines or so, it doesn't look too bad. Most of these all fit easily in
80 characters. I'll keep this change.

   106841 out/pkgs/md5/Md5/Rounds.fble.c
    99785 out/pkgs/sat/Sat/Aim.fble.c
    72427 out/pkgs/invaders/Invaders/Graphics.fble.c
    55915 out/pkgs/graphics/Graphics/Camera/SinCos.fble.c
    32884 out/pkgs/graphics/Graphics/Triangle/Tests.fble.c
    21252 out/pkgs/core/Core/Int/Tests.fble.c

Next thing to try? I'm thinking either abort or profiling code. Both of which
are sort of not needed for typical use case and potentially take up a lot of
extra code.

A couple different experiments we can do for abort:
* Don't generate an abort function. Just use NULL.
* Don't output error messages on aborts.
* Don't generate tests for aborting.

I want to get a sense of what makes more or less difference. Let's try one at
a time and see if any are particularly noticeable.

1. Don't generate an abort function.

Interestingly enough, only about 25% of the generated C code is Abort
functions:

    79447 out/pkgs/md5/Md5/Rounds.fble.c
    73327 out/pkgs/sat/Sat/Aim.fble.c
    55834 out/pkgs/invaders/Invaders/Graphics.fble.c
    43343 out/pkgs/graphics/Graphics/Camera/SinCos.fble.c
    25419 out/pkgs/graphics/Graphics/Triangle/Tests.fble.c

More importantly, removing them doesn't have a significant effect on memory
use. Maybe it drops from 63% down to 57%. 72% down to 69%.

Next thing to try:
* Remove all checks for abort behavior. Add this on top of the no abort
  functions change. I'm looking for something to make a substantial
  difference.

They still take a long time to compile. Rounds is down from 57% to maybe 45%
with this change. Sat/Aim is down from 69% to 63% from this change.

We haven't reached an order of magnitude yet. And
interestingly enough, it's only shaved off maybe 15% of lines of code:

    65292 out/pkgs/sat/Sat/Aim.fble.c
    57523 out/pkgs/md5/Md5/Rounds.fble.c
    48176 out/pkgs/invaders/Invaders/Graphics.fble.c
    36794 out/pkgs/graphics/Graphics/Camera/SinCos.fble.c

Next thing to try, again on top of all this: remove profiling code.

That's it. Removing all the profiling code makes a big difference. Cut down
memory use by half. We can now compiling everything in parallel no problem.

    38828 out/pkgs/sat/Sat/Aim.fble.c
    31953 out/pkgs/md5/Md5/Rounds.fble.c
    31433 out/pkgs/invaders/Invaders/Graphics.fble.c
    24234 out/pkgs/graphics/Graphics/Camera/SinCos.fble.c
    15611 out/pkgs/core/Core/Char/Eq.fble.c

It also put us under the 40K line mark.

Interesting. Let's see if I can walk back some of the abort changes. Can
everything be explained by profiling, or is it some mix?

I'm starting to suspect the issue is total number of conditions in a single
function.

---

Focusing on Aim now. Let's add back abort checks while leaving out profiling.

Remember where we are:
* 72% baseline memory use.
* 69% -ABORT_FUNC
* 63% -ABORT_FUNC, -ABORT_CHECK
* 30% (?) -ABORT_FUNC, -ABORT_CHECK, -PROFILE

Add back abort check now:
* 40% -ABORT_FUNC, -PROFILE, but still it's fast to run (40s, not 2 minutes).

Add back abort function now:
* 44% -PROFILE, still fast, but not quite as fast.

Something to try: enable dumping of info from gcc. Things to try (from
https://gcc.gnu.org/onlinedocs/gcc/Developer-Options.html):
  -ftime-report
  -fmem-report
  -time
  -fdump-statistics (doesn't seem to do anything?)
  -fdump-statistics-option [-detail | -stats]

Looks like -ftime-report and -fmem-report are good starts.

Most of the time is spent in phase opt and generate. That's a little broad.
Can we get more detail?

I can't figure out a combination of options to give a summary of time spent in
each pass with more detail than -ftime-report.

Could we reproduce the issue, you think, by writing a function that's pure
profiling code? Just a bunch of:
  if (profile) {
    if (rand() % 1024 == 0) FbleProfileSample(profile, 1);
  }

Back to back to back?

---

I'm not sure what to do. Misc ideas:
* We could coalesce profile blocks into one. Do a single sample for a combined
  straight line of instructions. Not sure if the math is straight forward for
  the random of the sample, but that would cut down on calls to profiling.
* We could statically detect potential undefined values and avoid inserting
  abort checks if we know they can't trigger statically.
* We could wrap the condition around a function. For example, 
   ProfileSample(profile) = if (profile && rand() % 1024 == 0) FbleProfile(sample
  That way we get straight line code in the generated functions, and even
  though there is overhead to doing the function call, at least, hopefully,
  the compiler can handle compiling better.

I think, at this point, it would be good to get some benchmarks. Some sense of
how much performance we get interpreted versus aarch64 versus c generation
with various gcc compile options. For example, if C code is faster than
aarch64 even with putting conditions behind functions, then that would be
great, right? Or at least make the decision to add that indirection in the
generated code.

What benchmarks did we have before? What's the state of them today?

I see the following:
pkgs/graphics/Graphics/Bench.fble
pkgs/md5/Md5/Bench.fble
pkgs/pinball/Pinball/Bench.fble
pkgs/invaders/Invaders/Bench.fble
pkgs/sat/Sat/Bench.fble
pkgs/games/Games/GameOfLife/Bench.fble
pkgs/games/Games/Snake/Bench.fble
pkgs/games/Games/BenchmarkGame/Bench.fble

Here's what I think: each package should define a compiled program that is a
benchmark for the package. Assuming there is something interesting enough to
benchmark. I think initially Md5 and Sat would be great to start to get an
initial idea of performance.

Looks like the benchmarks are expressed as tests. So we could run them using
whatever we do to run tests.

Tests are wrapped in /Core/Stdio/IO%.Run(/Core/Test/Stdio%.Run(Tests)). Let's
try the same for md5.

md5-bench interpreted:  1m22s
md5-bench aarch64:        50s
md5-bench c:              57s (no special gcc options.)

---

Note: it doesn't look like there's any way to specify dwarf debugging info as
input to a c compiler. We would need to target the gcc or llvm IR level.
That's rather annoying. I don't see any options for llvm text based bytecode
dwarf either.

The tradeoffs for custom code generator versus targeting something generic
like C or llvm:
* Custom code generator is faster, less memory to compile.
* Custom code generator has full flexibility on dwarf debug info.
* Custom code generator has lots of flexibility on compiled code.
* Targeting c or llvm means lots more architectures supported.
* Targeting c or llvm probably gives better runtime performance with
  sufficient optimization options than I'll ever get in my own compiler.

Some possible compromises:
* Be able to generate c/llvm, but still do architecture specific dwarf info?
  Is that feasible? Or would we have to generate the assembly ourself to be
  able to do that?
* Specify a set of supported architectures.

Really, if we are going to support multiple architectures, I see no reason not
to generate C code as another option. That doesn't add any extra effort, but
gives much better tail coverage of weird platforms. Presumably without debug
support.

In terms of which architectures to support, I vote for: arm, aarch64, x86_64
to start. Or, in other words, just the ones that I have computers to test on.
Let other people worry about other architectures if they care.

Ok. I like this approach. Multiple specific target architectures, as I desire,
and a generic C architecture as a catch all. Debug capabilities are only
available on the specific target architectures.

What's my strategy around build and test?
* We can only test targets supported by the architecture: the specific one and
  C. Unless we want to do some qemu thing, which sounds complicated. We would
  need to cross compile the fble library, which means we would need a cross
  compiler.
* I think prefer to compile a specific architecture. But, for example, maybe
  we build benchmarks, and possible compiled tests in specific and generic
  targets, to improve test coverage.

That sounds reasonable. Everyone is responsible for testing the 'C' target.
Otherwise, only make changes to your own specific target and make sure that
keeps working. Worst case, if we break some other target, at least they can
use 'C' as a target until they have a chance to fix it.

Strawman:
* For each benchmark and test, provide foo-tests-c, foo-tests-aarch64,
  foo-bench-c, foo-bench-aarch64 variants. Run both tests.
* That means, for each file, we really want libfoo-c and libfoo-aarch64.
  That's a lot of compiling to be doing, which means we need to get past the
  high C compilation overhead issue before we can do this for real.

TODO:
* Set up sat-bench, get some initial numbers.
* Set up aarch64 and c versions of the profiles-test
  - Debug and fix failing c version.
* Factor conditions out of profile code, see if that helps compilation while
  continuing to enable profiling.
* Factor common strings out of abort code. Who knows, maybe that will help a
  little bit with memory requirements.
* See if I can add static analysis to compiler to track what values may be
  undefined or not. I think that could help performance and code generation
  for both aarch64 and C code.

---

sat-bench interpreted:  2m00s
sat-bench aarch64:      1m16s
sat-bench c:                s (no special gcc options.)
