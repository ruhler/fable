Fble Spec Tests
===============
Question: can we change how we specify fble language tests?

Currently they are specified using tcl with a funny mechanism for specifying
module hierarchy. Question is, could we write .fble files directly, like as a
separate fble library package with no other fble library dependencies? I think
that could be nicer.

We just need a way to indicate the metadata for each test:
 fble-test
 fble-test-compile-error <loc>
 fble-test-runtime-error <loc>
 fble-test-memory-constant
 fble-test-memory-growth

Advantages of what I'm envisioning:
* No need for special tcl syntax for modules.
* We know the name of the top level test module.
* We write .fble files directly, so we get syntax highlighting and direct line
  numbers for errors.
* We avoid an extra extraction step to get .fble files, which should simplify
  the build system notably.
* Hopefully more easily reused by hypothetical other fble implementations.
* We can compile Nat.fble just once and easily reuse it.

All I have to do is figure out a reasonable way to do the metadata.

Seems something like checktest from llvm, but I don't really like that, and I
certainly don't want a dependency on llvm.

Most natural thing to do is add a comment to the code. Some syntax tests will
be out of reach, like the ones involving comments, but that's such a basic
thing I don't think we need to test that using a separate test.

Note that the top level for the test case may be different from where the
error occurs. I suggest putting a metadata line only at the top level of the
test case?

So it's pretty straight forward, right? First line of the file for a test case
should be:

# fble-test-* <loc>

Or, maybe, because we aren't using tcl, we can use different names. Like:

# TEST
# TEST_COMPILE_ERROR <..>
# TEST_RUNTIME_ERROR <..>
# TEST_MEMORY_CONSTANT
# TEST_MEMORY_GROWTH

Or, like, have assertions? I dunno. I don't think it's hard, it's just
arbitrary.

We could do double comment:

## NO_ERROR
## COMPILE_ERROR 8:3
## RUNTIME_ERROR 8:3
## MEMORY_CONSTANT
## MEMORY_GROWTH

If it isn't annotated, it isn't as separate test case.

How will the test runner work? How will it integrate with the build system
where we want to compile?

We could run a single grep command over all modules in the directory. That
gives us a mapping from filename (hence module name) to test case. We can then
do a loop over those lines. Maybe we can generate that using a build rule and
have build.ninja depend on it and build.ninja.tcl read it? With a slight
bootstrapping process to generate it initially I suppose? That's unnecessary
though.

Then we can parse the entry easy enough in tcl.

So, I just need some metadata easy to locate with grep, and easy to parse from
tcl. Let's explore options.

For prefix, we want something that is a comment and is unlikely to occur by
accident. It may be nice if it indicates to readers of the code what it is,
instead of being some magic. If we have text, we don't need magic symbols
aside from the comment character. I like 'test' and 'fble' in the name.

# FBLE-TEST no-error 
# FBLE-TEST compile-error 8:3
# FBLE-TEST runtime-error 8:3
# FBLE-TEST memory-constant
# FBLE-TEST memory-growth

Say it has to be the first line in the file, because why not? Or is that too
picky and too easy to accidentally not be applied? No. I think anywhere should
be okay. Allow any amount of whitespace between tokens. How to prevent
accidental uses of 'FBLE-TEST' in a document? I think we want some special
symbols for it.

Okay, how about this, a line of the form:

# @@fble-test@@ <type> <msg>

<type> is one of:
  no-error
  compile-error
  runtime-error
  memory-constant
  memory-growth

<msg> is an arbitrary string expected to be included in the error message,
primarily used to identify the location of the error.

There should be at most one @@fble-test@@ line per file. I'm not sure who
enforces that though.

Clearly it has some special meaning. Simple to detect with grep. Clearly
related to fble and testing. Simple to parse. I say go with it. No need for a
name, the current module path should be fine?

Good. What do you think? Shall I try doing this cleanup?

---

What are the next steps here? I need to come up with a plan for how to
implement this.

So we have the .fble code already extracted.
We get a list of tests from build.ninja.tcl using glob and/or grep.
We parse the test type from within build.ninja.tcl and call the appropriate
spec-test-compile, fble-test, fble-test-compile-error, etc. functions.

I'll want to migrate slowly to the new scheme. So duplicate and modify in
build.ninja.tcl to start. Start with just a handful of tests, get that
working, and slowly migrate.

What directory structure do I want for the spec tests? Put them in spec
directory maybe? spec/SpecTests/... I would say the tests are part of the
spec. 'lang' versus 'spec'? I think 'spec' is more specific. Do we want an
'fble' prefix? Or is everything okay to be fble based now?

Let's try 'spec/SpecTests', where spec is the -I value and tests go under
/SpecTests/...%

Immediate next steps:
* Port the langs/fble/0... tests to new format.
* Set up build.ninja.tcl infra for running tests in the new format.

---

The easy way to build the new test format is:
* For every file under SpecTests directory, create a spec test rule that calls
  a script to run the file. The script reads the metadata, verifies it is a
  test (trivially passing otherwise), and executes the commands necessary to
  run the test.

That way we don't have to read the contents of any of the spec tests to
generate the build.ninja. The downside is we don't know at build time whether
the test depends on fble-test, fble-compile, fble-mem-test, etc, so we'll end
up adding a dependency on all of those, which could trigger unnecessary test
reruns in some (rare?) cases.

The slightly harder way is to parse the test file in build.tcl and generate
rules depending on the test. Let's start with the easy way. Once we have the
easy way, we can decide if we want to migrate to the hard way, which needs all
the same code, just a bit more and in different places.

First steps for implementing script to execute:
* Pass an include directory and a module path? Or include directory and .fble
  file name. Either way, want the info necessary for the .fble file name, the
  include directory, and the module path.
* Read the file, search for and parse the @@fble-test@@ metadata.

---

Next step, let's consider the case of no-error.
* No need to extract anything.
* We want to compile .fble -> .o all modules the test depends on.
 - How do we know which modules the test depends on?
* We want to compile the main .fble -> .o with --main.
* We want to generate a binary.
 - Can we call ld directly, or do we need gcc flags?
 - How does that interact with code coverage?
* We want to run fble-disassemble, for the fun of it.
* We want to run interpreter with --profile (can ignore result)
* We want to run compiled executable.
* We want to catch any errors and report the test file.

Questions:
* How to know which modules to compile?
  - Maybe run fble-deps to get that info?
  - Can we avoid recompiling, e.g. Nat.o multiple times?
  - Or should we separate compilable from non-compilable, and just compile all
    the compilable modules up front in one big go?
* Where do we put output files?
  - tempdir? out dir?
* How to link compiled executable / how does that work with coverage?

I should be able to answer the question about linking on the existing spectest
infrastructure. Maybe let's start there.

Looks like directly invoking 'ld' doesn't work great, because it's missing
some standard options and libraries that gcc adds when it calls 'ld'. For
example, _start as entry point and whatever library implements 'rand'.

Does that suggest we want to reuse the build options in build.ninja for
building these things? In which case, maybe I really want to parse the spec
test .fble files and generate build rules from those rather than doing a
one-off test run directly from the .fble?

---

Here's the issue:
* Unlike the old approach to spec tests, the new approach does not explicitly
  list the modules that a test depends on.
* Using fble-dep to get that information is feasible, but suggests we can't do
  it as part of generating the build.ninja file, because we need build.ninja
  to build fble-dep first.
* The fact that ld isn't trivial to call directly, and that we ideally would
  like to reuse compilation of Nat.fble, suggests we would prefer to have
  build.ninja in charge of building for tests.

So we are stuck in a knot. It's a question of how I want to break the knot.
Some possibilities:
* Come up with some way to make dependencies explicit in test cases. For
  example, a test /SpecTest/.../Foo.fble depends on exactly Foo.fble and
  everything in the directory Foo/, plus Nat.o for memory tests.
* Don't worry about recompiling Nat.fble or factoring out common code for
  linking fble-based executables.

As far as building with coverage goes, I think the current approach is the
best we can do:
* Run fble-test.cov, fble-compile.cov tools.
* Link tests against non-coverage libfble.a.

Because I assume gcc needs to generate special code to support coverage, and
when we are compiling fble code, we bypass gcc and whatever special code you
need. In other words, compiled fble code does not support code coverage.

I kind of wish there was an alternative to gcov we could use for code
coverage. Something perf based possibly? I feel like perf would require
sampling at every instruction, which is way overkill.

---

Solution for build.ninja dependency on fble-deps: see if I can set up a
subninja file that depends on fble-dep. I feel like ninja should be able to
read the initial build.ninja file, use it to build fble-deps, use that to
generate the subninja file, then reload the subninja file, start over again,
see fble-deps and subninja are up to date, and execute those rules.

Solution for knowing whether we should be able to compile a test .fble or not:
any .fble file that can't compile should be marked with @@fble-test@@
compile-error. Even if it's not the main test entry, for example, if we want
to test reference to a module that doesn't compile. That's a rare case, so
it's fine to have an extra @@fble-test@@ compile error on the referenced
module saying that compilation would fail from the top level module too.

So, I guess my vote is the following:
* Use fble-deps to figure out dependencies we need to compile and link
  together for a test. We want that regardless of whether we build via ninja
  or not.
* Start by using separate test script that duplicates compilations and gcc
  command lines. Generate intermediate files to out/<path>/ directory. Boot
  strap that way.
* If desired, switch to hack for generating subninja dynamically.

Next steps:
1. Add support for interpreted test cases via script.
2. Set up spec/build.ninja to run test cases.
3. Add support for compiled test cases.
  - Figure out how to know what to compile and link using fble-deps.
4. Migrate rest of test cases.
5. Optionally figure out hack for generating subninja dynamically.

---

Note: One thing nice about having the .fble be the source for the test is we
don't have to output a separate error message to the user to direct them to
the .tcl file.

Err, actually, that's not true. We still need to output an error message
location in case of unexpected success, or mem-test. Ideally if the test
fails, we output an error message of the @@fble-test@@ line.

---

Okay, we have new spec tests set up for non-compilation cases. I'm actually
fine not writing out artifacts of the test, just be silent in case of passed.
Probably saves some IO cost?

Next step is the tricky part: how to support compiled tests. Components:
* Generate a .d file with fble-deps. We'll actually want to use this as part
  of the test output in spec/build.ninja because we need these extra build
  dependencies in ninja.
* Extract the list of .fble files from fble deps. Compile each of those with
  fble-compile.cov to a .s file somewhere (temp file?)
* Compile all the .s files to .o files somewhere, handling the main .fble file
  slightly specially.
* Link all the .o files to an executable somewhere.
* Then run the test.
* Then delete the temporary files?

Support for temporary files/directories via tcl?
* We can stream the .s directly to .o. No need for intermediate .s to be
  saved.
* file tempfile has a way to create a temporary file, but it opens the file
  for us, which doesn't work great with the assembler? What if we don't pass
  -o to as, does it stream it out? That would be okay then. No. 'as' without
  -o creates an a.out file. Maybe that's okay.

Idea: use tcl's file tempfile, create named files for .o and the compiled
executable. Pass the filenames to as and gcc. Run the stuff, then as cleanup
regardless of error condition, delete all the temp files.

Better yet, there is a linux command 'mktemp', that can create a temporary
directory. Make a temporary directory, generate whatever I want into there,
then delete the directory recursively when the test is done running. Easier to
clean up 

---

One idea: we could make fble-deps work regardless of whether the .fble can be
compiled. Just ignore any errors and write down as many dependencies as we
see. That way we could do fble-deps as part of build by running it on all
.fble files in spec directory. We need to add these missing dependencies
regardless, even before adding support for running compiled tests.

It shouldn't be hard to parse an fble-deps output file. For example:

out/pkgs/core/Core/Stdio.fble.d: pkgs/core/Core/Stdio.fble \
 pkgs/core/Core/Bool.fble pkgs/core/Core/Unit.fble pkgs/core/Core/List.fble \
 pkgs/core/Core/Maybe.fble pkgs/core/Core/Process.fble \
 pkgs/core/Core/String.fble pkgs/core/Core/Char.fble

Split by whitespace, remove '\' entries, remove first entry, then for each
.fble file, trim the prefix directory.

For it to work properly, in theory, we do need fble-deps to work on .fble
files with compilation errors. Otherwise we might miss important dependencies
for compile-error test cases.

My vote is that fble-deps can handle anything, it ignores errors while
loading, just puts out whatever it has seen as a dependency so far. The only
slightly annoying thing is how to handle stderr from parser error messages.
From the point of view of fble-deps as a tool, it makes sense to propagate
those to the user via stderr. But we want the option to ignore those, which is
easy enough with 2>/dev/null. What about exit code? I vote, exit success as
long as we were able to write a meaningful fble-deps file to output.

That sounds like a reasonable next step to me. Add support for this slightly
unusual fble-deps use case of deps for invalid .fble files.

Next question is: should we run fble-deps as part of build or as part of spec
test? We need it for build certainly. I assume eventually I'll want to
move most of spec test logic into build. So it doesn't really matter to get
started. Let's do it at build time blindly, and for now pass the .d file to
fble-spec-test.tcl for it to parse.

---

I think it will work well to create a temp directory that we can put
intermediate files in, then delete non-conditionally at the end of the
fble-spec-test script. Parsing the .d file is not hard. So, all we need to do
next is:
* create a tempdir at the start of the test. Clean it up at the end of the
  test.
* invoke the compiler on each of the .fble files.
  Pass in the FbleFooMain function to the compile procedure.
* invoke gcc to link them together.
* Run the compiled binary where and as appropriate.

And then we're all set!

---

On further thought, let me use a directory in the 'out' directory instead of a
temporary directory. That way it will be easier to reproduce and run gdb on
failing compile tests. To make sense of build versus fble-spec-test.tcl, move
spec/build.tcl to fble/test/spec.build.tcl or some such, because it fble/
should be downstream of spec/ and running the spec tests are really about
testing fble. If the script and build file are in the same directory, we can
think of them as one module together.

For directory name, let's use out/spec/$FBLE as a directory.
For module names, substitute '/' in the file path for '-'. We want to compile
relative to the test, not the reference module, because multiple different
tests may reference the same module. In the future, if we move everything to
build, then we can share compiled modules across tests. For now, let's just
recompile shared things on a per-test basis.

---

Cool! It's all wired up with separate build and run scripts. Next step is to
port all the rest of the test cases over to the new framework and remove the
old test cases. I don't see a compelling reason to push the run script into
the build script at the moment. We can do that later if a compelling reason
comes up.
