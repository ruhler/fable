Stack Allocate
==============
Exploring whether we can do stack allocations to improve performance of fble.

The idea stems from the problem of function allocation.

The following allocates an intermediate function object:

  foo(a)(b)

The following does not:

  foo(a, b)

One way to think about the difference is in the second case we effectively
allocate an intermediate function object, but we do so on the stack as a pair
(foo, a). Thus we never need to allocate a heap object, so we don't incur the
costs of malloc or GC.

Can we somehow enable stack allocations for foo(a)(b) so we don't have to pay
the extra cost there too? And in particular in more complicated cases such as
monadic bind where the application of argument a is separate from the
application of argument b in the code.

Review of memory management. There are a few different categories of kinds of
objects:
* Packed values.
 No need to manage, we copy everywhere.
* Explicitly owned reference
 There is one clear owner of an object. The memory for the object can be
 reclaimed when the owner is done with it.
* Reference counting
 There is no single owner of an object. The memory for the object can be
 reclaimed when all the owners are done with it.
* Cycles
 There are reference cycles in the object without a clear owner. Typically we
 need more advanced GC to handle this case.

Currently in fble we treat everything as either packed values or potentially
having reference cycles. We don't take advantage of explicit ownership or
reference counting opportunities.

Memory management is made hard by references. Without references, everything
would be passed by value, you make copies.

References work as follows:
* A packed value 'ref' copied around everywhere.
* An allocated object 'obj' that stays put.
* The lifetime of 'obj' must be at least as long as the lifetime of 'ref'.
* The lifetime of 'obj' should not be significantly longer than all of the
  references to it.

The reference requirements are ensured by the current gc algorithm. If you
take a new reference to an object, that causes the obj to stay alive at least
as long as the reference. Once all the references to an object are gone, the
object will (eventually) be freed.

Consider a function. It has:
* Input arguments, guaranteed to stay alive longer than the function.
* Output, required to stay alive longer than the function.
* Local variables
 - If not captured by the output, don't need to stay alive longer than the
   function.

A key question about an object is: is it (potentially) captured by the output.
If the answer is no, I claim it could be stack allocated and automatically
freed when the function exits.

Let's pretend we have some memory heap associated with every function call,
and we can dynamically allocate as much memory to that heap as we want. We can
label these heaps based on position in the stack. Say the label is the number
of stack frames below this stack frame, so that inner calls have a higher
number id than outer calls.

If you have two heaps with ids x and y, if x > y, then the lifetime of all
objects in y is greater than the lifetime of all objects in x.

An object in heap x can freely refer to an object in frame y without
additional tracking. An object in heap x can refer to an object in frame x,
assuming we free everything in frame x all at once. An object in frame y
cannot safely refer to an object in frame x, because the reference in y would
outlive the object in x.

Let's say, when a function is called, it is provided the heap where the result
of the function should be allocated. Maybe it also needs to know the heap
where the arguments are allocated.

If there aren't reference cycles, and some object a references some other
object b, we can say the heap id of a >= the heap id of b.

In a function, when an object is allocated, we ask if there is a possibility
of the object being captured by the output. If yes, allocate it to the heap of
the output object. If no, allocate it to the current function's heap.

When calling a function, if the result of the callee is possibly captured by
the output of the caller, then pass the caller output heap as the callee
output heap. Otherwise pass the caller heap as the callee output heap.

We have also the case where the output of a function captures the inputs to
the function. If we already knew this, then the inputs to the function would
be allocated in heap that lives as long as the output to the function already.

Now the question is: is this enough to avoid memory leaks? In particular, I
worry about the 'possibility of capture' case. What if there is a small
possibility of capture, so we end up allocating a lot of garbage to an upper
frame?

Or, could we do accurate dynamic tracking?

For example, have a notion of virtual heap, still associated with a stack
frame. Virtual to make it so we can easy move an object from one heap to
another.

An object is allocated in the current heap. If there is a reference taken from
an object in y to an object in x, and x >= y, then the object in x is moved to
y, along with any other x references that are in heaps >= y. When we return
from a function, we move the return value to the caller's heap.

I claim this will be too expensive. Think about the case when allocating a
long linked list. Every time we return, we have to iterate over the entire
list to move all the objects. We need it to be constant time to return a value
to the caller.

That suggests we need to live with "possibility of capture" to allocate
objects to a safe heap to start with.

We haven't talked about tail recursion yet.

Say we are in heap 5. The result will go to heap 4. We have some local
variables in heap 5 that are being passed to the tail call. In theory, we need
to keep the local variables captured by the tail call and drop all the other
variables. Otherwise we'll have a memory leak. Perhaps functions that do tail
calls should have two heaps: the heap of objects potentially captured by the
tail call, and the heap of objects not captured by the tail call.

Can I come up with an example where potentially captured leads to a memory
leak? Almost surely. For example:

(Int@) { Int@; } F = (Int@ x) {
  Int@ y = decr(x);
  IsZero(y).?(true: x);
  F(y);
};

Contrived, yes. The variable x should only be captured at the end, but it has
the possibility of being captured every time. We end up allocating every value
on the outer caller's heap, thus using O(N) memory instead of O(1).

Can we flip the condition around and at least get some benefit? For example,
if I'm certain that an object isn't captured by the output, then we can
allocate it on the function heap. If that means we reduce the number of
objects participating in gc heap, that's great, right?

So, say for each object we can say: I guarantee this object will not live
longer than 'x'. If an object x references an object y, where x >= y, then we
don't need to track the reference?

Or maybe we keep track of a range of lifetimes. The lifetime for an object is
somewhere in the range [a, b]. It will not live longer than a. It will live
for at least as long as b.

Say an object [a, b] takes a reference to an object [m, n]. What happens? The
lifetime of the object [a, b] does not change (assuming no cycles for the
moment). How does the object [m, n] change?

If a >= n, no change. This means the [m, n] object will always live long
enough.

I guess it would be: [min(a, m), max(b, n)], right? It will not live longer
than both the objects. It may live as long as either of the objects. But,
again, if you change the value on the object [m, n], you have to change the
value of every other object it references recursively, which is too costly.

If an object has a single owner, we could say the lifetime of the object is
the lifetime of its owner. An object can only get a second owner through its
original owner.

If an object has multiple owners, and one owner is guaranteed to live longer
than all the other owners, then the object essentially has a single owner
still: the owner to live the longest.

Perhaps owners of an object could coordinate. Say I have object A with owner
X. Then, via X, we take a second reference to A from object Y. If X knows
about Y, and Y knows about X, then they could coordinate. Whoever is first to
be freed tells the other they have been freed and they are transferring
ownership. That's basically reference counting...

Makes you wonder if it's worth trying to go back to a reference counting based
GC. The language is simpler now. No links anymore. The only cycles will be
through RefValues.

---

Let me assume the high cost of memory management today is because objects are
allocated using malloc and an associated call into the gc. That suggests
switching to a reference count based approach will not help: we would still be
using malloc for objects.

Idea for allocating things on the stack:

Assume we reorganize values so that if you know the type of a value, you know
its shallow size. And assume we worry about polymorphic functions later or
have a way to know concrete types when they are invoked.

Have a function return its value as a copy. So the caller allocates space for
the return value on the stack, the callee fills in that space. That space
could contain references to other objects. Those other objects could be
allocated on the heap by the callee, allocated on the stack by the caller, or
allocated elsewhere with a lifetime that will outlast this function call.

Assume callee allocated objects transfer a reference to the caller as part of
this.

For local variables that would have been released when the function exits, the
function should go through and release any callee allocated parts of it. No
need to do anything else. Then we have successfully managed to allocate the
value on the stack and avoid malloc/free/gc for it.

Otherwise the object is returned to the caller of the function. For each local
and child allocated object referenced, heap allocate those as new objects and
copy that all over to the caller. We've successfully managed to allocate the
value on the stack and avoid malloc/free/gc for it, but we may have had to do
heap allocation for some things it referenced.

I claim this is good enough to allow stack allocation of the function value in
the monadic example stdout(byte).

To make this work, we need an easy way for the function to tell if an object
is:
a. heap allocated by a child.
b. locally allocated.
c. owned by the caller (either via the stack or heap).

How can we take a reference to an object owned by the caller from a child heap
allocated object if the object owned by the caller was stack allocated?

---

Important observation: Some things need to be allocated on the heap, even if
their lifetime is tied to the stack. For example, say we allocate a long list.
Because it's a long list, we can't allocate it on the stack. Then we use that
list as a local variable from some function f. The lifetime of the list is
clearly bounded by the call to f.

This means there are cases where heap allocated objects can safely refer to
stack allocated objects.

I think it's pretty clear conceptually how things should work. It's just
figuring out the details. Local variables should be allocated on the stack if
they aren't captured.

The details in question are:
* Can we allocate returned variables on the stack?
* Do we detect what's captured statically? Dynamically?
* Do we have a way to heap allocate objects that aren't gc'd?
* Will some combination of the above solve the fble-cat allocation case?

Next step, I think, should be to walk through the code for the simplified
fble-cat example. Understand what allocations we could stack allocate with
what knowledge and whether all the functions could be stack allocated.

---

Deep dive into monadic fble-cat code.

Function allocations are:

11. l1 = s2(l3);     stdout_(char)
13. l3 = l0(l1);     Do(stdout_)
15. l0 = func;       (Unit@ _) -> MCat
17. return l3(l0)

Let's start with the first one.

   0.  l0 = func /Core/Stdio/FastCat%.Main!.stdout_!![0017] [s0, a0];
   1.  return l0;

This is an anonymous function. We capture l3 as a static variable. So,
conceptually this is the pair (stdout, char). This is definitely captured by
the result of stdout_(char). To allocate this on the stack, we want to support
allocation of return results on the stack. If we did that, we end up with
(stdout, char) on the stack. No need for a heap allocation.

Note: we'll need to figure out how to allocate space for a function on the
stack when it could require an arbitrary number of static variables.

Next for do. Again, we allocate space for the result on the stack. The do
function at this point literally just captures the argument and returns a
function. So, in this case, we return (do!, (stdout, char)). No reason that
couldn't be allocated on the caller stack directly.

Except: are these functions potentially captured by the return result? I know
nothing about l3 statically without type information or some kind of inlining.
f(l0) could potentially capture l0 for arbitrary f.

Let's try working backwards. What is l3?

It's the result of calling do. A function that captures stdout(out). The
result of that will be another function that captures l0. So l3(l0) captures
l0 for sure. It captures stdout(out) too. So here is a case where the function
we want to return references other local variables. We would need to allocate
all of these local variables on the callers stack. Or copy them over to the
callers stack.

The resulting function is then passed to m.do(stdin).

---

Lots of thoughts on this. No major breakthroughs yet. Some highlights:
* We could allocate everything on the stack maybe, if we are willing to deep
  copy the result from the callee to the caller.
* Current GC is almost like copying the entire heap every time we finish a
  round of GC.
* How will we manage the API for external references to FbleValue* if we
  allocate everything on the stack?

It seems like the incremental improvement approach would be to try to
conservatively allocate values on the stack. But that won't eliminate the
function allocations in the cat example.

The aggressive improvement approach would be to try to do away with GC almost
entirely. Allocate everything on the stack. I don't have a good answer for the
pathological case of constructing a list though.


---

Did another round of thought on GC in performance.txt. It suggests stack based
allocation is worth trying, if only to get the following:
* Avoid having short lived objects put pressure on the stack after they are
  done being used.
* Avoid smashing the cache to free short lived objects.
* Avoid repeatedly smashing the cache when traversing long lived objects.

There are many different ways I could go about this. I fear getting bogged
down in details. How about we start like this:

* What is the easiest way we could adjust the heap allocator to have the
  properties above? Implement that and see what impact it has on performance.

Easiest way to start would be if we could restrict all the changes to heap.c.
Keep the same API for allocating objects and interacting with the heap. How
would that look?

Say we keep allocated objects in a linked list, maintaining the order they are
allocated in. To allocate a new object, you put it at the end of the list.

Say we magically make it so that new objects can refer to old objects, but old
objects can't refer to new objects.

When we release a heap object, if it's the last object in the list, we can
free it.

Could we traverse the heap from the back always?

---

Let's try generational gc. See gen_gc.txt for discussion.

In theory we now have all the following:
* Avoid having short lived objects put pressure on the stack after they are
  done being used.
* Avoid smashing the cache to free short lived objects.
* Avoid repeatedly smashing the cache when traversing long lived objects.

I'm not sure how to verify. It only helped performance a tiny bit though.
Like, 3% performance improvement.

Actually, turns out it was a bug in generational GC. With the fix, we now see
18% improvement and better memory behavior. I think we have achieved the goals
above.

Let's give memory management a break for a little while since the switch to
generational GC. Focus on partial application or other approaches to avoid or
reduce the cost of function allocations rather than a rework of overall memory
allocation for now.

---

We are again back to the high cost of allocations. Specifically a struct,
union, and function allocation required as part of the Result@ monad. Every
statement of Result@ monad requires an allocation.

I really wish we could allocate these directly on the stack. I think that
would make it much cheaper.

<@ A@>(Result@<A@>)<@ B@>((A@) { Result@<B@>; }) { Result@<B@>; }
Do = <@ A@>(Result@<A@> ra)<@ B@>((A@) { Result@<B@>; } f) {
  ra.value.?(nothing: Raise<B@>(ra));

  Result@<B@> rb = f(ra.value.just);
  Result@(Append(ra.errors, rb.errors), Or(ra.failed, rb.failed), rb.value);
};

Look at rb here. We know rb is a struct. We know that we only access the
individual fields of rb, after that we are done. From the Do function's point
of view, rb is short lived. Though we don't know if anyone else is using it
for other reasons.

In practice, that function f is likely to be Do itself, allocating a brand new
result.

Let's say we could return values in three different ways:
1. packed.
2. stack allocated.
3. heap allocated.

We always prefer packed over stack over heap.

To stack allocate, we are specifically talking about results of functions. The
caller should allocate space on the stack for the presumably shallow
allocation of the object. The callee knows about that and fills it in on the
stack if possible.

We just need to work out when to do which kind of allocation and how to manage
references.

---

Three approaches for returning objects on the stack:
1. Return the entire object on the stack.
2. Return the shallow object on the stack, anything else it references on the
heap.
3. Return up to a fixed amount of memory for the object on the stack, anything
else on the heap.

I want to try (1) to start, because it's so far in a different direction. With
(1), everything is allocated on the stack. There is no heap.

The big concern with (1) is the pathological case of building a large object,
like a linked list. Because we copy back every value return, and we are
copying N values back O(N) times, that's O(N^2) runtime to allocate an O(N)
list.

Heap allocations are expensive to allocate and cheap to move around. Stack
allocations are hopefully cheap to allocate but expensive to move around.

For (1), we need a managed stack, because standard calling conventions have no
way for a callee to return an unbounded amount of data on the stack to the
caller.

One idea that may make it easier is to have two stacks that we alternate
between for every call. If A calls B calls C calls D calls E calls F:

First stack: A C E
Second stack: B D F

This way, for example, when F is returning, it can return directly to the end
of E's stack frame without worrying about clobbering things. Otherwise it may
have to worry about clobbering itself. That is, compacting is easier if you
are compacting to a different memory region.

The API for the new proposed value heap:

FbleNewValueHeap, FbleFreeValueHeap
  As before, except Free cleans up any remaining objects.
FbleNewStructValue, FbleNewFuncValue, FbleNewUnionValue - as before.

void FbleValueHeapCall();
  Sets up a new stack frame on the heap, recording the current position.

FbleValue* FbleValueHeapReturn(FbleValue* return);
  Returns to the previous stack frame on the heap. Frees every object
  allocated on the current stack frame that is not reachable from the return
  value. Moves everything reachable from the return value allocated on the
  current stack frame back to the previous stack frame. Returns the new
  address to use for the returned value.

To manage objects in the C API, it will have to manage the stack as if it was
a bunch of stack frames. The easiest would be have everything it uses in the
first stack frame, which stays alive until FbleFreeValueHeap is called.
FbleEval can allocate a new stack frame to start so its only returning what it
uses.

We can still have packed values if we want. RefValues are not allowed to span
across stack frames, but that should come naturally. No need for
retain/free/addref.

Open question: how to properly free function values? Any way we can avoid the
need to free function values? Seems like that would make things easier in
general. Perhaps during compacting we iterate through all the allocated
objects (they should be next to each other on the heap) and free the ones not
moved. We could keep functions separately so its easier to iterate through
just those.

This approach sounds doable to me. I'm not convinced it's viable though,
because of these two things:
1. The pathological O(N^2) runtime to construct a N element list.
2. The need to iterate through all functions to clean up their executables.

Is it worth trying? Or should we find a solution to (1) and (2) first?

To address (1), we would want to limit to a constant amount of data returned
to the caller. Put the rest on the heap. If we do that, then we are basically
going to original approach (3). No need for a managed stack anymore. We
allocate a fixed amount of space for the callee to return values to. Anything
that doesn't fit in that fixed space we allocate on the heap.

The hope is this efficiently handles the case of small, short lived
allocations that are slowing us down with monadic code today. I fear it makes
inefficient use of stack space. All that potential space that won't be reused.

Honestly, I think we should try (1). Give it a chance to see its full
potential before worrying about how to address the bad cases. See how close it
gets to the current approach and if it solves any problems we are having
currently.

---

Here's an idea for how to avoid iterating through all the objects to clean
them up. Change FuncValue to directly store num_args, num_statics,
tail_call_buffer_size, profile_block_id, and run functions. Then we no longer
need to do anything on free of functions.

To support destructors, define a new internal value type NativeValue which is
a wrapper around a native object. NativeValue participates in GC, it's
basically raw data and an on_free function. Allocate things like file
descriptors and FbleExecutable (for interpreter) in NativeValue objects that
belong to statics of a function value. The run function can assume those
objects exist and have the desired native type.

The hope is that in normal execution, we very rarely have to deal with
NativeValue objects. Add some extra data structure for NativeValue objects,
such as an embedded doubly linked list, that makes it easy to traverse just
the native objects for on free.

I'm feeling good about this approach. We could implement it with the current
heap garbage collector if we wanted to. I'm not sure it gets us much. I
suppose the main downside is we can't share num args, num statics, tail call
buffer size, profile block id and run functions. So, if lots of functions are
instantiated with the same parameters, we end up with duplicate data. Perhaps
memory for that is expensive. On the other hand, when do you expect a bunch of
different functions to have the same executable? It would be functions
allocated by other functions. Maybe it's not that common.

---

First question: how to implement the stack data structure?

A. Use the existing native stack.

I'm not sure this is legit, but, for example, maybe we use alloca to allocate
on the callee stack. Then return the resulting pointer to the caller. The
caller used alloca to keep track of its local allocations. To access the
returned data from the callee, call alloca with the needed size to get access
to whatever was on the callee stack when it returned.

Downsides:
* It's pretty hacky. Probably not portable.

Assuming the hacks work, we could compact the stack as follows:
* The callee returns the value from its stack without any compaction.
* Immediately on return, the caller uses alloca to capture the callee's stack.
* We call a Compact function giving it a pointer to the original caller stack
  and the new caller stack. It can copy over everything.
* Somehow we need a way to realloc to free up the unused space from the
  callee. Hmm.. Sounds tricky.

B. Implement a single continuous address space.

This sounds nice, because:
* it's cheap to allocate and free: increment and decrement a pointer. 
* We can easily check if a pointer is in the compacting region of the stack or
  not, so traversal is easy. Just do pointer comparison.

How can we get a single continuous address space?
* Pass a fixed size heap when we create FbleValueHeap. 
  Sounds reasonable in theory. We could pick a default large heap.
* Use something like MAP_GROWSDOWN?
  Is that portable? How are you supposed to use that?
* mmap on demand and hope you can get continuous regions?

Some experiments to try:
1. What does the virtual address space look like for a mostly empty c program?
2. What does the virtual address space look like for, say, fbld while it's running?
3. What's the largest single region I can successfully mmap in one attempt?

(1):

55800b0000-55800c2000 r-xp  /home/richard/scratch/stack/main
55bab88000-55baba9000 rw-p  [heap]
7fa96b1000-7faa0b1000 rw-p  My 10MB mmap.
7faa0b1000-7faa21e000 ***p  /usr/lib/aarch64-linux-gnu/libc-2.28.so
7fde589000-7fde5aa000 rw-p  [stack]

(2):
  0bb34000-  10e9b000 rw-p  [heap]
7fd93ca000-7fda670000 rw-p  [stack]

(3):

Somewhere between 512MB and 1GB. I wonder if it's trying to reserve the space.

Let's try with MAP_NORESERVE now.

That gives us somewhere between 256GB and 512GB.

Cool. So with MAP_NORESERVE, we can get plenty of virtual address space for
the stack. Looks like in my case it allocates memory at the end of the
available region.

Any way to pick a decent default value for the heap size? I guess I could do
what I'm doing now: keeping trying 2x until we fail to allocate that much
virtual memory.

I suppose a more portable way to do this would be to malloc 2x until we fail,
use the last successful size as the stack.

Using malloc, the biggest allocation I get is 512MB.

If I search more precisely, the biggest allocation I get is around 1009MB.

I don't see any need to use mmap myself. Malloc a big region sounds fine to
me. We can search for what's available. Or take a fixed size.

---

Random different idea: what if we implement a copying collector instead of
mark sweep?

The stack allocation approach has already abandoned the idea of incremental
garbage collection. We know we care about maximum memory. So how about
something simple like this:

Start with some reasonable smallish heap size. Say 1MB. When we fill up that
1MB, then we do a big copy traversal of the entire heap.

A couple problems with this:
* We need to update all external references to use the new pointers. We can't
  do that as easily when there are external references beyond the one we are
  returning.
* How/when do we grow the heap? It's only after we copy that we know how much
  space we need for it.

Maybe we say at 1MB, copy over. If the result is less than 512KB, fine. If the
result is more than 512KB, then the next time we allocate a new region,
allocate it as a 2MB region?

In other words, we have heap size (after compaction), heap max (allocated
space). We set the next heap max based on heap size after compaction.

The nice thing about this approach is:
* It should be really fast when we aren't compacting.
* It's based specifically around the idea of minimizing the max memory usage.

Downsides:
* It doesn't work well with caching. Short lived objects will go out of cache.
* Big stop the world GC events.

---

Anyway, back to previously proposed stack allocation approach. We can use
malloc to allocate a single large enough region of memory up front. Allocation
is bumping a pointer. On 'enter', save the current stack pointer so we can
jump back to it. On traverse, we can use the pointer values to check what is
included in the traversal and what is down the stack.

---

Time to try implementing my stack allocation approach for real. The hope is we
remove all the time for IncrGc, malloc, free, ReleaseValue, and AddRef. We
replace it with time to traverse objects when returning. There's no doubt the
traversal will be expensive. But how will it compare to what we save? That's
what we want to find out.

Use md5sum as a best case example. I don't expect any large objects returned
there.

Where to start? Implement the heap API. Which will be:

FbleNewValueHeap, FbleFreeValueHeap,
FbleValueHeapCall, FbleValueHeapReturn
FbleNewStructValue, etc.

No need for FbleRetain/FbleRelease. No need for FbleValueAddRef. Start by
assuming we can make one big allocation for the heap space, if that's easier.
No obligation.

There are three major parts to this:
1. Implementing the new heap API
2. Switching to the new heap API
3. Cleaning up now unnecessary retain/release.

(3) we can do last. Just implement them as no-ops to start. That will be fun
cleanup/performance optimization.

High level sketch of implementation:
* Value is vtag + data
  - struct value: vtag + fieldc + fields
  - union value: vtag + tag + arg
  - func value is vtag + function + statics
  - ref value is vtag + value
  - native value is vtag + data + on_free
* Two stack regions, alternating back and forth for frames
* Each region has:
  - Pointer to the top, for new allocations.
  - Pointer to the base -> which should form a linked list of stack frames.
* We keep track of the current 'to' and 'from' regions for allocation.
* New heap: allocates and initializes the regions.
* Free heap: frees the regions.
* New value: increments the top of the current region and returns the previous
  top. All values are now properly aligned, so no worry about alignment here.
* Call: swap to/from regions, push a new frame onto the new to region.
* Return: Traverse/Copy everything on the top frame of the from region to the
  too region. Pop the top frame. Call native destructors on everything in the
  top frame being popped.

Let's say the 'base' of a stack frame points to a single pointer which is the
base of the previous frame on that stack. That's how we store the linked list
of frames.

That leaves two tricky parts:
* Keeping track of native values whose destructors need to be called.
  Ideally make this really fast assuming no native values, and reasonably fast
  assuming some native values.
* Implementing the traverse/copy logic.

For the traversal, implement a recursive function that, given a value, copies
it over to the new space and returns the new pointer. Case:
* The value is in an old space. Return it.
* The value hasn't been seen yet.
  Allocate space for it on the new stack. Set its type on the 'from' space to
  COPIED or some such and store a pointer to where it was copied to.
  Recursively copy over all the fields.
* The value has been seen. It's marked COPIED. Return the saved pointer for
  that value.

Easy.

For native values, keep them in a singly linked list. Use a field of the
native which isn't one that will be overwritten by the 'COPIED' value. After
the traversal, walk through the linked list. Skip over any values marked
'COPIED'. Call on_free on the rest.

That's it. Easy.

---

I drafted the code for implementing the new value heap. It's all very straight
forward and nice. Should be low overhead, if only it weren't for the bad
complexity of traversing the entire result every time we return from a call.

---

What to call my Call/Return, Push/Pop functions? Let's brainstorm.

FbleValueHeapCall, FbleValueHeapReturn
FblePushValueFrame, FblePopValueFrame
FblePushFrame, FblePopFrame
FbleValueHeapPush, FbleValueHeapPop
FbleNewFrame, FbleFreeFrame
FbleFramePush, FbleFrameReturn
FblePushValueFrame, FbleReturnValueFrame

I like Push/Pop. I like Frame. How to link that to the heap and values?

FblePushHeapFrame, FblePopHeapFrame.
FblePushStackFrame, FblePopStackFrame.
FbleNewFrame, FbleReleaseFrame

FbleNewStackFrame
FbleNewDataStackFrame

I like New too. Maybe better than 'Push'.

FbleNewHeapFrame, FbleReleaseHeapFrame
FbleNewValueFrame, FblePopValueFrame
FblePushValueFrame, FblePopValueFrame

FbleCallFrame, FbleReturnFromFrame

I like Return.

FbleNewHeapFrame, FbleExitHeapFrame
FbleNewHeapFrame, FbleReleaseHeapFrame

FbleReturnValue - This is good.

FblePushFrame, FbleReturnValue

Let's go with: FblePushFrame, FblePopFrame.

---

How to implement tail call?

I can bundle up the function and arguments into a struct value so it gets
copied back to the caller frame appropriately. But then I'll want to add more
args to the back of it.

Cases when we add num_unused to the back:
* If it's a tail call. Always that case.

How about we make num_unused and unused available for FbleTailCall, it can
bundle in those automatically? I kind of like that idea.

I think it's worth defining a special internal ThunkValue for this, so it's
easier to access the fields. We don't have to worry about packing. We can have
separate accessors for the function and args.

---

Trouble with tail call. We can't return the func and args to the caller frame,
because that memory will not end up getting reclaimed if we are in a loop.
Every subsequent tail call we do will add more and more to the caller frame
leaking memory.

We need the func and args for the tail call to end up on the callee's frame.
How do we manage that?

The other thing to figure out: what convention should I use for who calls
FbleFramePush/FbleFramePop when, particularly when dealing with native
functions? I think if I call some function FbleFoo(...) that returns an
FbleValue*, it should be the job of FbleFoo to do FbleFramePush and
FbleFramePop internally. Yeah. That's consistent and reasonable.

How to handle tail call then?

======= caller
======= callee

Seems like we want a way to traverse/compact in place. If executable->run
returns tail call, it returns the function and args to tail call still on the
callee frame (we could pass them via gTailCallData still if we want to).

What I want, then, is a function like:

void FbleCompactFrame(FbleValueHeap* heap, size_t count, FbleValue** save);

It frees everything in the frame except things reachable from those values
listed in save. It updates the pointers listed in save. If we had this, then
the implementation is straight forward I think. Do just like we have now,
except run FbleCompactFrame just before doing the tail call.

How could I implement this? We can't assume the frame is going to be small.
Some of the args could have been allocated on the frame and be really big.

Brute force way would be to double compact. Compact from A to B. Compact back
from B to A. That's two traversals instead of one. It's probably doable to
start, but not much fun.

For example, push a new frame, 'return' from the caller frame to the new
frame, then push a new caller frame, return from the previous new frame to the
new new frame. We don't have to write any new code for this, aside from the
wrapper to update save values.

Could we instead flip the polarity of the two stacks?

Say our stacks are A and B.

Say X calls Y and Y tail calls Z.

A: X
B: Y

When Y tail calls into Z, we 'pop' from B to a new frame in A.

A: X, Z
B: 

Now two things can happen.
1. We do another tail call. Z to W say.

A: X,
B: W

We end up exactly where we want to be.

2. We do a return from Z. Now we are in trouble again, because we can't return
in place.

What if we had 3 stacks, A, B, C?

X calls Y

A: X
B: Y
C:

Y tail calls Z

A: X
B: 
C: Z

Z tail calls W

A: X
B: W
C: 

Z returns to X

A: X
B:
C:

I think that works. We just need to keep track of which secondary stack is
active. Tail call swaps the active secondary stack.

We'll need to record where to return to. I'm sure it's doable. Just need to
work out the details.

Okay, easy.

Heap is a Frame* current.

Frame stores a Frame* caller and a Frame* alternative.

PushFrame:
  Pick either current->caller or current->alternative for the new current.
  The old current becomes caller, the unchosen becomes alternate.
  (In the future we could maybe pick based on free space for better memory use?)

PopFrame:
  current becomes current->caller

CompactFrame:
  current becomes current->alternate. It's caller is old current->caller and
  alternate is old current.
  
Sounds like a plan. How will we make use of this for tail calls now?

FbleTailCall does exactly what it does today. Copy func and args to
gTailCallData and return TailCallSentinel.

When we see TailCallSentinel, we add unused to gTailCallData like today. Then
we call CompactHeap, passing a pointer to gTailCallData.

The only thing is, I think we should store func and args next to each other on
gTailCallData and avoid storing the FbleFunction* pointer so that we can call
CompactFrame and have it work.

Okay? Let's try it.

---

More guidance on when to use FbleFramePush: Any native function that allocates
new FbleValues that are not returned directly to the caller should probably
call FbleFramePush/FbleFramePop to avoid leaking those allocations.

---

Code is drafted. Let's see if it's in a working state.
