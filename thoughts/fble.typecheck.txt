Fble Typechecking
=================
There is a bug in the core type checking algorithm that tests whether two
types are equal.

Specifically, List@<List@<Unit@>> is considered equal to List@<List@<Bool@>>
when it shouldn't be.

The goal of this discussion is to rewrite the core type equality checking so
that it works properly, and ideally understand the algorithm better as a
result.

Debugging suggests the reason the type checker considers those two types
equal:

1. List@<...> versus List@<...>
Assumes they are equal for recursion, using the type id of each to denote
what's equal.

2. Comparing the arguments: List@<...>, List@<...>
Thinks they are equal given the recursion from above, because substitution
preserves type id.

That's the surface explanation. The details are more tedious, because we are
walking the types after normal substitution.

The high level idea is pretty straight forward. Each type is described using a
finite representation. You could think of it as a possibly cyclic graph. Each
node in the graph should have an id. To avoid recursion, any time we compare
node A.x from type A with node B.y from type B, we enter it in a list to
assume equal for purposes of preventing recursion.

I think that algorithm is okay. The challenge is in identifying the nodes of a
graph. In particular, handling the combination of recursion and poly apply,
where without any special handling we would end up unrolling the graph into an
infinite graph.

Let's start with List@<Bool@>:

<@>@ List@ = <@ T@> { +(*(T@ head, List@<T@> tail) cons, Unit@ nil); };

List@<Bool@> does substitution:
  +(*(Bool@ head, List@<Bool@> tail) cons, Unit@ nil);

If we walk this, eventually we'll get back to List@<Bool@> and want to expand
that. The important thing is to make sure we give back the original expansion,
right?

Let me see if I can dig up more notes on this from before. Nope. Looks like I
didn't record it anywhere.

The fix I am envisioning is:
* Remove the id field from FbleType.
* Use the FbleType* pointer as the id instead.
* Keep a table somewhere mapping poly apply X<Y> --> Z, if we see us doing the
  same poly apply again, make sure to return the original type we returned.

The tricky part being how to record this table and pass it through the
different layers of abstraction for type equality check and normalizing a
type.

--

Two options I see:
A. Don't do any substitution or normalization during type checking.
B. Do substitution, but keep track of all substitutions and refer to previous
substitution of same.

I'm pretty confident (B) is doable. Minor tedium to pass around the table
through Normalize.

The real concern with (B) is it might not be enough. I'm worried about a type
such as:

<@>@ L@ = <@ T@> { *(T@ t, L@<L@<T@>> l); };

Because there is no way to write this type as a cycle without poly apply.

Compare to list, which can be written as:
  <@ T@> {
    @ L@ = *(T@ t, L@ l);
    L@;
  };

It's like it has embedded within it an infinite number of different types.
That means there is no way to limit the recursion.

Two options again:
1. Figure out how to do an implementation based on (A) that works.
2. Don't worry about this funny case. Who would write something like that
anyway? Add some heuristic to detect it and give an error.

The key idea behind (A) is to take advantage of the logic:
  F A = G B  if F = G and A = B

Graph isomorphism is NP, not undecidable, which is a good sign. I'm not sure
how adding poly apply to the works could mess things up there. Functional
equivalence is undecidable, but I don't have a conditional operator at the
type level, some hopefully that result doesn't apply here.

I think it's worth exploring whether I could do type checking by defining a
set of equality constraints and solving them. I am again worried about the
poly apply case.

---

Thinking more about approach (A) from above, what I had in mind is actually
more of a type inference algorithm, not type equality. I'm starting to doubt
that's a meaningful approach to take.

Let me try out (B). See if I can make some progress with it. It's going to be
tricky.

Steps:
* Remove 'id' field from FbleType.
* Switch to using FbleType* instead of id for recursion tracking.
* Add an additional argument to Normalize which is a map from pairs of types
  to result, used to store poly apply results. We only need to pass this for
  the type equality check, not for other uses of Normalize.

I'm thinking we should set up a separate branch for this work, because it will
be finicky.

Let's dive in.

Concerns:
* Normal needs a way to prevent infinite recursion. Do we handle that properly
  without using 'id'?
  I'm not convinced it won't work...

Let's test ourselves. What do we expect to break if we don't put the extra
table for caching poly apply results?

Easy: Type equality comparing any recursive polymorphic data types. They will
spin for ever allocating new types.

I think we'll want the table to be a vector instead of a linked list done on
the stack.

---

Draft of option (B) is done. Let's try and see what happens. My hope is that
it works fine as long as we don't have that funny case of
L<T> = *(L<L<T>> foo) mentioned above. And I don't think we have that, unless
I ran into it before and wrote a very specific test for it, or we have some
funny vacuous value test case like that.

Good news: this fixes type inference of concat and the List<List<Unit>> versus
List<List<Bool>> bug. And all other current tests pass!

I'll check it in for now, but definitely need to write up that failing test
case I know about.

---

Idea: if the only place we allocate types in Normal is poly apply, then maybe
we can use the table we have now to check if we are doing a poly apply P@<A@>,
where the table already has P@<X@> => A@? Or where A@ is a result in the
table (in case of mutual recursion)?

Anyway, let's write up the test case that I expect to fail with infinite
recursion / out of memory.

It behaves as expected. I put a check for poly_applies blowing up with detects
the case for the time being and gives an assertion failure, but that's by no
means a perfect solution.

Unfortunately, I can't turn this into an expected fail test if it fails due to
an assertion.

---

In terms of a complete solution, I really feel like, for a given type, there
should be some sufficient amount to unroll the type. If everything is equal up
to that amount on both types, we can say the types are equal. Because if there
was a type difference, we would have encountered it by then. Like, we want a
proof that says if the two types are different, then you'll notice the
difference within depth K of unrolling. And we just need to figure out what K
is.

Here's a question:

<@>@ L@ = <@ T@> { +(Unit@ x, L@<L@<T@>> y); };

Is L@<Bool@> equal to L@<Int@>? I think yes. Feels like we don't say clearly
enough in the spec. Maybe I want to add a spec test for this case.

I think the type equality algorithm would be much simpler if we could compute
this 'K' value, because then I don't need caches, memo tables, cycle
detection. I just unroll the type sufficiently and then compare.

---

Let's be honest. Equivalence of types is almost certainly undecidable in my
language, which is lambda calculus with recursion. So there are two
approaches:

1. Restrict the language of types so that equivalence is decidable.
2. Have type equivalence be undecidable and hope for the best.

(1) sounds reasonable, given I don't expect we really ever want to make use of
those crazy kinds of types that lead to undecidable equivalence. (2) is rather
unsatisfying.

So go the route of (1). I already have the idea of the restriction in my mind:
you have to write list as a recursive type with the poly factored out:

<@>@ List@ = <@ T@> {
  @ L@ = +(*(T@ head, L@ tail) cons, Unit@ nil);
  L@;
};

How about this: when a type variable is in its 'undefined' state because you
are in the body of the definition, you are not allowed to do any poly apply of
it. Is that enough? That sounds simple enough.

So there are a couple of questions with this:
A. How onerous is it to rewrite the code this way?
B. Does this restriction make the type system decidable?
C. Is there any way we can relax the syntactic restriction and support the
existing definitions I have of recursive types?

(A) we can figure out pretty easily. Let's just try it.
(B) is the most important question.
(C) doesn't matter, and I suspect we won't want to go that route, because it
just adds confusing complexity.

Let's try implementing (A). See what happens. We need some way to mark a type
as 'undefined'?

After we know the value, we call FbleAssignVarType. What happens before then?
We have an FbleVarType with null value.

The question is, can we distinguish between an 'in the process of being
defined variable type' and a 'poly argument variable type'? I think not
currently, but we could make it possible to. Add an option to FbleNewVarType.

Perhaps better yet, use two different kinds of types?

Or, set the type kind differently? Maybe let me try that as an initial hack.

First hack is to set the kind to basic kind during definition. Places that
need fixing:
* Definition of List@, as expected.
* Definition of IterH@. Awkward, because it's mutually recursively defined.
* Definition of Map@. Awkward too, due to mutually recursive definition.
* Definition of state monad type.
   - Looks like this didn't need to be recursively written this way.

Issues with my hack. Note that we have to do it by kind, not by variable,
because you could assign to another variable and we need to track that. In
other words, we must capture in the kind whether we are allowed to apply it or
not. Let's see if I can hack my hack to make it passable.

Looks like I had a real motiviation example for this in the past:

  <@>@ Map@ = <@ T@>{ +(Unit@ empty, MapP@<T@> map); },
  <@>@ MapP@ = <@ T@>{ *(Unit@ unit, Map@<Map@<T@>> product); };

And we had a regression test for this.

---

* We need to apply this kind restriction for values too, in order to avoid
  using typeof to convert a poly value to a poly type. That's better. We can
  say in general, when using kind let, in the definition the value has basic
  kind.
* I'm convinced this change makes type equality decidable. Think of a type as
  a finite graph, and poly apply operates at the graph level. If you do a poly
  apply for some poly P, the result will not have any poly apply of P. Because
  there are a finite number of polys, there are a finite number of poly
  applies you can do before you've fully unrolled to a (possibly cyclic) graph
  without poly applies.
* My hacky approach to implement this is, I think, the right approach.
* It's not worth trying to come up with some special way to allow L<T> in the
  body of L. The minor syntactic inconvenience for mutually recursive types is
  not sufficient to justify such additional complexity and confusion.

Let's clean things up then.

1. Write a test case to capture converting poly value to type value to bypass
my current implementation.
