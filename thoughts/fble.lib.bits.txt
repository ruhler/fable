Bytes
-----
We use bytes for md5. We want it for crc, compress, files, png, etc.

So I want to do some cleanup of the interface for bits.

Thoughts:
* Support arbitrary bit lengths in a standard way.
* Have the type be different from comparison operators. So don't distinguish
  between signed and unsigned. The type is the same regardless.

So, generically, a BitN has the following interface:
  NOT, AND, OR, XOR, ADD, INC, ZERO,
  ISZERO, EQUALS, MSB, 
  FULLADD, FULLINC, SHIFTL.

Because that's what we have today. But also the type: T@ and T@ with overflow.

Some bits we'll want to also support literals, like binary and hex. Some maybe
worth supporting that for all kinds of bit literals.

How literals should work:
* If too few digits, assume most significant bits are 0.
* If too many digits, truncate.
  So, for example, if you specify 0x123 on a byte, that turns into 0x23?
  Or do we have to worry about signed vs unsigned in that case?

  Or, consider it an error. I guess we could do either way.

I want an abstract interface BitN@<T@>, where T@ is the bit type.
I'll want a type name too. So, like, Bit8@. That doesn't make sense to include
in the abstract interface.

How do we want the implementation structured? I think
  2 = 1+1
  4 = 2+2
  8 = 4+4

As opposed, to, for example: 8 = 1+1+1+1+1+1+1+1.

More generally, Z = X+Y.

So we'll want a module for Bit1 that provides Bit1@ and an instance of
BitN@<Bit1@>.

And a module for Bit8 that provides Bit8@ and an instance of BitN@<Bit8@>.
And a module for Bit_X_plus_Y that provides T@ and an instance of BitN@<T@>,
given two instances for BitN@.

Should Bit8 be a separate module, or should we have one module, like today,
that defines Bit8, Bit16, Bit32, Bit64, etc.?

What names would we use?

Bit8.Bit8@, Bit8.BitN

I guess I'm suggesting we do that. Each module defines a pair of entities: the
type and the instance of BitN@ for that type.

Then we could have a single top level module that defines Bit8, Bit16, etc.

Proposed module hierarchy:

/Core/BitN/BitN%
  Definition of BitN@ interface.
/Core/BitN%

Hmm... Maybe use Bits@ instead of BitN@?

/Core/Bits/Bits% - Definition of Bits@ interface.
/Core/Bits/Bit1% - Implementation of Bits@ for Bit1@.
/Core/Bits/BitXY% - Implementation of Bits@ for BitX@ + BitY@.
/Core/Bits/Bit8% - Implementation of Bits@ for Bit8@.
/Core/Bits/Bit16% - Implementation of Bits@ for Bit16@.
/Core/Bits/Bit32% - Implementation of Bits@ for Bit32@.

etc..

If people want to use, for example, Bit4%, they should just redefine it
themselves, even though it's defined and used in the implementation of Bit8@?

The option is to have
/Core/Bits/BitN% - That defines Bit1, Bit2, Bit4, Bit8, Bit16, Bit32, and Bit64?

No. I would prefer to import /Core/Bits/Bit8%. Or, perhaps /Core/Bits%.Bit8?

How about this:
  /Core/Bits/Bits% - @(Bits@)
  /Core/Bits/Bit1% - @(Bit1@, Bits)
  /Core/Bits/BitXY% - <@ X@, @ Y@>(Bits@<X@>, Bits@<Y@>) { @(BitXY@, Bits); }
  /Core/Bits%% - @(Bit1, Bit2, Bit4, Bit8, Bit16, Bit32, Bit64)

We can reuse /Core/Digits%.Bit@ type for Bit1@.

Summary of changes from what's currently implemented:
* Rename BitN@ to Bits@. (done)
* Change Bit2X@ to BitXY@. (done)
* Rename Result@ to Overflow@. (done)
* Clean up coding style with regards to braces. (done)
* Rename 'zero' to '0'? (done)
* Move from Md5 package to Core package.

Let's see if I can make some of these improvements incrementally.

Can we come up with a nicer convention for 'with overflow' naming? Like,
instead of 'add', add with overflow is 'add_o', or 'add_'? Then we can change
name of Result@ to T_@? Brainstorming: 'add_overflow', 'inc_overflow',
Overflow@. 'add_ov', 'inc_ov', 'Ov@'.

Let's just be explicit: add_overflow, inc_overflow, lshift_overflow.

Ov@ is nice too? Then Ov@ = <@ T@> { *(T@ x, Bit@ ov); };

Or, 'o', where 'o' could stand for overflow or out? Or...

Overflow = <@ T@> { *(T@ x, Bit@ out); }?

---

The real concern I have with the bits API is the distinction between
'primitive' functions that are part of the Bits interface and other functions
implemented on top of those that either could be included in the interface or
not.

With normal functions it's nice because anyone can add a function. Not anyone
can add a function to the Bits interface. So, as we scale, I would expect most
of utilities for working with bits to be outside of the Bits interface,
right? That suggests keeping Bits as small as possible?

Note: we can always copy functions from bits to top level for a particular
type, and we can always create more parameterized variations on Bits. From
that point of view, seems like this isn't really so important an issue to
worry about?

---

A flash of insight tells me the Bits interface should be an internal
implementation detail. I should start by making the BitXY@ type abstract, move
circular left shift to the library, then export all the top level named
functions you would want to use with bits rather than export the Bits
interface. After that, I can move /Bits% to core and add test cases.

Double checking the places where hi/lo fields are accessed:
* HexFromBits, HexFromBit4, HexFromByte, HexFromABCD
  - General conversion of bits to list of hex digits.
* cls
  - Circular shift left by N.
  - I guess this would normally be something like:
      (x << N) | (x >> (32 - N))
  - I should have shift left and shift right functions that take an Int@ as
    the amount to shift by.
* Pad, which outputs one byte of a 64 bit word at a time
  - I guess this would normally be something like:
      Truncate(x >> 8)

It would be nice if we could do a bit slicing operation. Treat bits as an
array of bits and get a substring. I feel like that would be a much simpler
implementation too, because we can use the same underlying type (list of bits)
for all bit types, instead of having to define a bunch of different types
using polymorphism.

The benefit of packing bits into a struct is in theory you can put 32 bits
into 32 bits, instead of need, say, 64 bits to represent 32 bits.

In C code, to convert to hex, I would just do shift, truncate, and then
arithmetic or index into an array. I guess the closest thing in fble would be
to use a map instead of an array, support shift and truncate. But how do we
support truncate? There are some many combinations of source and destination
types. Unless we assume that, for example, bit 64 is implemented is a
combination of bit 8 somewhere inside, so we can truncate. Maybe truncate in
half? But that's just 'hi' and 'lo' field access?

Another option could be a function to get the 'ith' bit.

Easiest is just to convert to a list of bits, then convert that list of bits
to a list of hex.

---

Extracting sub bits is expensive. That's natural. I can't do arbitrary array
access. Imagine you have a trillion bit string and want 7 bits starting at
5223413. You aren't going to be able to do that in constant time.

How we organize bits matters. If you want the low 4 bits from a Bit8, and we
organize Bit8 as 4+4, it's fundamentally more efficient than organizing Bit8
as 5+3.

The most natural representation of Bit8 as the sum of two sizes is 4+4. I
think it's fair to let users know and take advantage of Bit8 as 4+4.

That's extra knowledge you can't easily encode in BitXY. But you could in
Bit2X like we had before.

I still think Bits should be more of an internal helper thing or used in rare
cases. Each BitN you define should have a top level interface of function.
That allows us to do functions specific to a type. For example, L4, L8, etc.

I almost wonder, for the whole polymorphism thing, if it isn't better to just
go piecemeal. Polymorphic add takes the sub add function. Polymorphic shift
takes the sub shift function, etc. That way you avoid bundling things
together into a closed set. People can add new polymorphic functions tailored
to exactly what they need.

In summary:
* Use top level functions when interacting with BitN types.
* Explicitly construct bit types using power of two.
* Define "least_significant_8" function to extract bytes from larger types.
  Etc. 
* We really ought to support general shift left and shift right.
* I do think BitN types are suitable for use as abstract types.
  - Then we could easily try out switching to list or other representations.
  - Just allow users to assume the functions we provide, e.g. ls8, are
    reasonably efficient to implement.

How to do shift in general? I mean, shift by N?

Let's take a Bit32. The most it makes sense to shift left or right by is 32
bits. So, what type do you use to represent that number <= 32?

Brainstorm:
* Bit32. It's a type we have. We ignore all the high bits. That's like what
  happens in C and other languages. I guess we could ignore the high bits, or
  see if they are non-zero or not.
* Bit5. Accurately captures what the range is, though you couldn't shift by 32
  in this case. But how do users create the Bit5? If it's always just by
  truncation, maybe that's not great?
* Bit8. As the closest power of 2 to Bit5. Let's people shift by 32,
  presumably more general support. But also a bit of a step function.
* Int. As a general integral type to shift by. Though this adds a dependency
  on Int for shifting, which maybe it would be nice to avoid.

The kind of shifting I would want to do so far:
* shift by 8, or 4, to extract segments of a larger number.
* shift by up to 24 in md5 circular shift code.

Is there anything that would be convenient for implementing shift by
composition? I like thinking of the shift as a sum of powers of 2. Then we can
implement it with a barrel shifter approach: shift by 4 yes/no, shift by 2
yes/no, shift by 1 yes/no.

I suppose we could implement it as a series of fixed shifts:
   shl1, shl2, shl4, shl8, ...

We can optimize each of those, and we could write wrappers on top for things
like Int or larger Bit.

One benefit of having constant shift values is we can easily represent the
bits shifted in or out, to chain things together composition wise.

For example, take a byte.

shl8: x = in, output x
shl4: hi = lo, lo = in, output hi
shl2: ...

In general, shlN:
  BitN tmp = shlN(lo, in);
  shlN(hi, tmp)

Which is really simple to implement. We could implement it polymorphically. I
like this idea. I think it's at least worth trying out.

Okay. Plan? Steps?

Steps are:
* Implement shlN and shrN for the bit types.
* Reimplement circular shift in Md5 using shlN and shrN.
* Move BitN type definitions to separate modules. Done.
* Export top level functions for the bits types.
* Switch to using top level functions for the bits types.
* Implement LowN functions, at least Low8 and Low4 to extract bytes and Bit4.
* Implement conversion from Bit4 to hex digit.
* Implement general conversion from BitN to hex string using shift and Bit4
  extraction.
* Get rid of Bits type, replace with individual polymorphic functions.
* Turn bits types into abstract types.

Let's do one step at a time, in whatever order is easiest, and see where we
end up.

What name to use for least significant bits and most significant bits
functions? Include the number of bits at the end, that's fine. Otherwise, MSB,
LSB? Msb, Lsb? Maybe Msb4, Lsb4? I like that. Most significant bit4, Least
significant bit4 is a way to read it. It leaves the common 'msb' and 'lsb'
acronyms in the name. Most and least significant bit is more precise than high
and low I think. Let's go with that.

---

How to distinguish between shift where I want to be able to shift in and out
bits, versus shift where we always shift in 0 and ignore top bits? What should
the names be? We'll want a number suffix for number of bits to shift in and
out for fixed sized shift.

It's sort of similar to the question of Add versus AddOverflow. I kind of want
something like: Add and AddEx, or Add and Add_. ShiftLeft and ShiftLeft*. Shl
versus ShlG, where G is for general?

shl
lsh

shr
rsh

I think I prefer shl and shr over lsh and rsh.

Shl4 and Shl4x? Where 'x' is for 'extra'?
Shr4 and Shr4x?

Shl4 and Shlx4?
Add and Addx?

That's not too bad. Short. Concise. Not too crazy. x is unlikely to blend in a
strange way with the previous characters. What if we try that convention?

So: Shl and Shlx.

Partly where this came about was in thinking about minimal requirements for
implementing literals. The idea is:
  start with 0, shift in a bit at a time.

Some possible collections:
* 0, Shlx1
* 0, 1, Shl1, Add
* 0, Shl1, Inc
* 0, 1, Shl1, And

I think the first is most direct and to the point. Most efficient. It's
exactly what we need: start with zero, shift a bit in at a time. And who
knows, maybe we want to verify it doesn't overflow, so it's useful to check
the output bit.

What to use for the name of the type of the shlx function? It's something
like:

(BitN@, BitX@) -> (BitN@, BitX@).

Similar in a sense to overflow, except overflow is a single bit, right?

And what to use for field names?

For shift, let's assume we have MsbX and LsbX functions. Then you don't
actually need to shift out the bits. You could use MsbX and LsbX to get
whatever you would have shifted out.

And you don't really need to shift in bits. You could instead say AND the bits
you want to shift in.

If we go that route, then the shift function interface is much simpler: 
BitN@ -> BitN@. And because we don't have BitX@ in the type, it's much more
natural to support a generic shift function: (BitN@, Int@) -> BitN@. Assuming
we have a generic Msb and Lsb functions, it's not even that hard to implement,
right?

Shift the lo. Shift the hi. And the msb of the original lo with the shifted hi.

What interface do we use for generic Msb/Lsb? Msb you could get by right
shifting (uh... make sure we aren't circularly defining things here). Lsb you
could get by masking or left shift followed by right shift. In either case, a
natural format for outputting Msb and Lsb for, say, BitN@ is a BitN@ where the
Msb or Lsb are in the least significant bits of the returned value.

All of which is suggesting to me an interface like:
  (Bit8@, Int@) { Bit8@; } Shl;
  (Bit8@, Int@) { Bit8@; } Shr;
  (Bit8@, Int@) { Bit8@; } Msb;
  (Bit8@, Int@) { Bit8@; } Lsb;
  
Note, the same kind of thing doesn't work for overflow of addition. I think we
need to do the addition to know if there will be overflow.

Int@ in general is useful as a type to identify a number of bits or a
particular bit. How awful would it be to add the dependency on Int@? We could
add a Int@ Sizeof field to facilitate arithmetic.

If we go this route, then to implement a literal, we would want, I think: 0,
1, Shl, Or. Which is slightly less convenient to have to assume and pass
around. You would almost think we want conversion between BitN@ and list of
bits to be the primitive constructor/inspector. Like, a BitN@ is a more
compact implementation of a list of bits.

Now I'm starting to second guess my decision to move things out of a Bits
interface. If we can use generic interfaces that work for arbitrary N, then
it's really nice to have the Bits interface because:
* It standardizes what you can expect to have defined for a bit N type.
* You can factor out the entire implementation of it.
* You don't have to worry about things like: what combination of functions are
  required to implement a polymorphic thing.

A reminder of what's on the other side, if you don't have a Bits interface:
* You don't have to worry about what's goes inside or outside of that
  interface. It's more natural to add new custom functions.
* You don't over-specify what's required for a particular function.
* You can more naturally mix type specific things, like Bit8@ instead of
  Bits.BitN@, and Lsb4 for Bit8.

---

I'm really going back and forth on this question of having a Bits interface
with common bits functions. For testing purposes, it would be great if I could
provide a test suite parameterized by bit type. That really wants a general
Bits interface.

Instead of thinking about every function being part of the Bits interface or
not, we could think about having collections of different subinterfaces.
Anyone can add their own subinterface pretty easily.
