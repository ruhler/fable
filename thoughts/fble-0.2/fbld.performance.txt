Fbld Performance
================

Now that we've completely redesigned and implemented fbld, let's see if we
can't make it run a little faster.

Specifically, there are a lot of library man pages to generate. Each one takes
a couple seconds. That adds up to a long wait time. I would love to cut that
time in half if possible.

$ time ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/config.fbld \
./fbld/version.fbld ../fbld/man.fbld ../fbld/dc.man.fbld \
./include/fble/FbleEval.3.fbld
real    0m1.750s

Fble based profiling says:
 44% in /Fbld/Parse%.Parse
 36% in /Fbld/Eval%.Eval
 15% in /Core/Stream/IStream%.GetChar

perf based profiling says:
 72% in /Fbld/Parse%.Parse

It's hard to get more clarity than that from the perf profile. I kind of wish
everything didn't go through FbleThreadCall. That really muddies the picture.

Let's focus on the parser. It's tough to tell from the profile, because
everything goes through Do. Once again, it would be nice if there was some way
to eliminate that to avoid muddying the picture.

Ideas from reading the code:
* Do we need a State monad? Can we use just a Reader monad?
  - It's not like we are writing output.
  - I don't think we can use a Reader monad. We need to track unconsumed
    output.
* Compute GetL_ up front as a list of CharL@?
  - To avoid having to redo GetL_ over and over every time we Try_ or Test_
    and fail?

Remember that perf profiling shows most of the time due to allocations. Can we
see who is doing those allocations? Should we increase the cost of
instructions that allocate for prof purposes?

According to perf, there are 909 allocations. They are:

         63       70           FbleNewStructValue_.constprop.7[0025]
         94      122           FbleNewLiteralValue[0005]
        150      159           FbleNewUnionValue[0010]
        266      292           FbleNewFuncValue[001e]
        328      349           FbleNewStructValue[0027]

This is odd. Let's focus on FbleNewFuncValue, which normally I don't expect to
see much of.


         17       17           _2f_Fbld_2f_Result_25__2e_Do_21_.000d[0057]
         23       24           _2f_Fbld_2f_Parse_2f_M_25__2e_GetL_21_.0058[003f]
         25       31           _2f_Core_2f_List_2f_Ord_25__2e_Ord_21_.0009[001d]
         32       33           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21_.002b[0047]
         43       46           _2f_Fbld_2f_Parse_2f_M_25__2e_Return_21_.0023[002
         54       57           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21__21_.002d[003
**      293      319       26  FbleNewFuncValue[001e] **

Do and Return are kind of expected, because they are structured as functions
that return functions to work with bind syntax. What about List Ord?

List Ord takes the ordering function separately. We should be able to factor
that out. That's the recursive call to Ord itself.

---

It's like the whole monadic construct involves allocations everywhere.
Particularly the parser one. We are allocating every time we 'try' and fail.
For example, allocating an error message that we just ignore in the end.

Is there any way to structure the parser monad so that it doesn't allocate all
over like this?

The reason results are turning into object allocations instead of being small
enough to be packed is because of the error message that we often throw away.
In theory we could use different variants on the function if we know we are
going to throw away the error message.

In the case of success, we need to allocate a heap object because we have to
have space to store the 'tail' pointer with the rest of the text to parse.

Could I change the parser monad to be a parser generator monad? Have it
somehow work out all the smarts to generate an efficient function that doesn't
do unnecessary allocation, while preserving the same kind of description?

I think it might be possible. We'll presumably want some restriction that
decisions made during parsing don't depend on the parsed result. They only
depend on whether parsing succeeded or not.

---

Let's start simpler. Say we have two kinds of parser types:
 * M@ is as today - parse and returns a value or error.
 * Parses@ - parse, but only indicates pass/fail, does not return a value or
   an error message.

Assuming Parses@ is much faster implementation wise, we do as much as we can
in Parses@. How much can we do there in practice?

* Try: wants to ignore error message, but not value.
* Test: Can ignore error message, but not value.
* Or: Could probably change to take M@<Unit@> as argument, then ignore both?
* Try_: Can ignore error message and value.
* Test_: Can ignore error message and value.
* EndOfInput - Can probably ignore error message and value.
* String - Can probably ignore error message and value.

I think I can do this incrementally. Slowly move things over to a Parses@ type
and see how far it gets me.

---

I don't know. It doesn't feel like this is going to be significant. It feels
like micro-optimizing. Maybe better to start by inlining common operations?

Let's focus on Or to start, which is only used with String matches right now.

And Do(Test_(...))?

It's like I want a minilanguage:

String :: String@ -> P@
Or :: P@ -> P@ -> P@
DoTest_ :: P@ -> (Bool@ -> M@<A@>) -> M@<A@>

How much time do we spend in Or? In Test_?

From perf profile, best I can tell is it's the regular use of Do that is
causing allocations which is hurting performance. Can we isolate how much of
those calls to Do could be a simplified Do_ that doesn't return value/error
message?

---

Okay, here's what I'm thinking. We want a variant of M@, call it M_@, that
ignores the error message but keeps the return value. For M_@<Unit@>, it
should be able to pack without having to allocate an object.

Let me work with this and see how far I get.

---

What we get to is:
* Return, Do, etc. glue functions want variants for M@ and M_@.
* Things that cannot return an error may as well use M_@. We can wrap in
  M(...) if we ever need to convert.
* Some functions an return errors, but in some cases we may want to ignore
  those error messages. In those cases, provide both M@ and M_@ variants.

Performance numbers: This is about 30% improvement. The number of object
allocations is cut in half.

Hmm... This feels sad. Do I want to take the change?

One thing we could do is generalize Result@ to include a second kind of error:
an unexplained error. That way we don't need two different types. That makes
it simpler in some cases, but we get less help from the compiler in other
cases.

Could we make it part of the reader? Like, have an extra option which says if
we want to return an error message or not. The functions Try_, Test_, etc.
will all say: run without recording the error message. Do this in combination
with expanding Result@ type.

That way, no need to duplicate any logic. No need to deal with messy types.
Everything can work in both contexts efficiently.

I like that idea. Let me save and back out my current changes and try that
alternative approach.

---

Alternative approach works great. Very minimal changes required.

Also note: avoiding return of errors was only maybe half the 30% improvement.
The other half was using GetL for String instead of Get and Loc.

Here's where we are at now:

time ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/config.fbld
./fbld/version.fbld ../fbld/man.fbld ../fbld/dc.man.fbld
./include/fble/FbleEval.3.fbld
real    0m1.129s


I still see a fair bit of time going into allocations.

         15       15           FbleNewLiteralValue[007a]
        107      116           FbleNewUnionValue[0019]
        167      183           FbleNewStructValue[001d]
        220      246           FbleNewFuncValue[0013]
**      516      567        7  FbleNewHeapObject[0014] **

Upping the frequency of collection to max:

         85      114           _2f_Fbld_2f_Parse_2f_M_25__2e_Return_21_.0026[003
        109      129           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21__21_.0032[003
        127      139           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21_.0030[0046]
**      740      937       51  FbleNewFuncValue[0013] **


Do these function allocations make sense? Any way to avoid them? Let's start
with Return.

Yeah. That makes sense. Not sure what I could do about that. M@ is a function.
Return allocates an M@ based on the provided argument. Each time you call
Return, you'll get a new function back.

The only thing I could think is if we do, like Return(Unit) a lot, we could
factor that out.

Get is defined as GetL + Do + Return. We could inline to avoid things there?

Could we improve the garbage collector? Like, manage memory ourselves to avoid
repeated malloc/free?

---

I tried manually removing FbleThreadCall from the perf profile for fbld. The
results are pretty interesting:
 
* 48% of time is in FbleNewHeapObject. We're spending half the time of the
  program allocating new objects.
* 47% of time is in Eval
  - Map Lookup is a decent chunk of this.
  - The rest mostly looks like fbld commands?
* 39% of time is in Parse

It's still pretty hard to figure out where the bulk of the time is going
otherwise. The way we handle recursion makes things hard to tell. I almost
wish, for cycles in the call graph, we bunch together everything in a cycle
into one blob.

That's two different ideas for how to make profiles more useful:
1. Lump recursive cycles into a single block. So it's easier to see how much
is self time through the cycle versus what the leaf times are.
2. Lump function call-throughs like FbleThreadCall or Do with their callers.
To preserve the relationship between caller and eventual callthrough callee.

I don't have enough information to do this from the profiling output info. But
I should have enough info to do it from the input to the profiler.

---

How much faster would parsing be if we didn't use monads? What would that even
look like?

M@<String@> Name = (R@ r) {
  SM@<Maybe@<Char@>> s = Test(Get);
  s.x.?(nothing: OK@(SM@(s.s, Str|'')));
  IsNameChar(s.x.just).?(false: OK@(SM@(s.s, Str|'')));

It could be kind of the same, except Return takes both arguments at the same
time instead of separately. And we have to inline Do?

What does the generated code look like for this? If we had some form of
inlining in the compiler, could we avoid these function allocations? For
example, if you allocate a function and then immediately apply it and then
throw away the function, can you just inline the function body directly
instead?

Some interesting observations:

* Take something like: Do(Test(Get)).
  We call that once when defining Name. We don't call it every time we invoke
  Name, which is great.
* Take something like: Return(Str|'').
  We call it every time we invoke name (if that branch is taken). We could
  factor that out if we wanted and only end up calling it once. Only
  allocating the function once.
* Take something like: Do(Get), Do(Name). We call these repeatedly.

In other words, there are a bunch of otherwise constant functions that we keep
re-invoking over and over again.

How much faster would it be if we factored out those calls? There's no need to
keep calling the same things over and over again when we can share it.

I'm curious if we can try and see what happens for something here. Actually,
profiling shows not much likely to improve.

Here's another observation for 'Do': The call to do takes two arguments, but
split apart. The typical use of Do is with <- syntax, where we have the second
argument available immediately. There's no need to allocate the intermediate
function.

Is there a reasonable way to make the bind syntax smarter here?

Today the bind syntax is:

 x <- f;
 ...

==>

  f(\x -> ...)

In case of Do, it looks something like:

 x <- f(c);
 ...

==>

 f(c)(\x -> ...)
  
Could we accept functions for <- syntax where we haven't provided enough
arguments? Like, it's shy one argument?

We would have to make the <- syntax part of the function application.

 x <- f(a, b, ...)
 ...

==> f(a, b, ..., \x -> ...)

From the parser's point of view, we have:

 x <- <expr>
 <stmt>

There are a couple cases for <expr>:
1. It is a function application short an argument.
2. It returns a function that takes a single function.

In the case of (1), the <expr> wouldn't be well formed.

Should we require the argument to <- to be function application always? Do we
ever use it a different way? Let me do a survey. And maybe also review where
we came from.

I don't see any explicit decision about this in past notes, though there is
mention of it. Bind is intended to be syntactic sugar for function
application. You'll always have the last argument available. In that case, it
makes sense to me to supply multi-arg functions.

Okay, so you have the choice.

F takes one argument:

x <- F;
...

F takes two arguments:
x <- F(c);
...

Doesn't that make things ambiguous? We could know which is intended if we
looked at the type of F. Only one will make sense. But today we don't have the
type at the time we desugar the bind syntax.

Here's an idea. What if we allowed over-application of functions in general?
Not partial application, because that's hard. But overapplication.

This could be more generally useful syntax. And it makes sense to me.

Say F has type a -> (b -> c)

Today we have to call it F(a)(b).

What if we could call it F(a, b)? That means function application looks at the
type of F, pulls as many arguments as it needs out. If not enough, that's an
error. Any left over arguments it turns into a separate application. If the
type doesn't work out, then you get an error.

Then we could say function bind has two cases:

1. f is function application: inject \x -> ... as an additional argument.
2. otherwise: apply \x -> ...

It's like adding a special case of what we already have. The important part
being that that special case knows how to distinguish what's expected based on
the type of 'f'.

Let me do a survey today of use of '<-'. In what cases would injecting an
additional argument in the case of function application not work?

Every case will work today. It's almost exclusively done using a 'Do' function
or equivalent. The only time I see 'f' not being function application is for
Error<...>, where I want it to avoid evaluating the argument if unnecessary.

What are more cases where it wouldn't be a function application <expr>?
* Do(Get) ==> DoGet. As an attempt to optimize. Could be useful.
* Maybe the glue is passed as an argument.
* Struct access we see a lot: m.do(...), but that's still application.

What are some cases where we don't want to require multiple args?
* If the function is designed to be used for callbacks that expect arguments
  to be split. We want to be able to split that function.
* Worst case, you could always do:
  (A@ a, B@ b) { f(a)(b); }
  But that's pretty tedious syntax.

I think this is worth seriously considering.

Proposal:
* Add support for over-application to functions.
* Add special case for <- in <expr> is a function application to overapply.

The rationale: For bind syntax, which is pretty common, avoids an extra
function allocation.

Maybe I want to do a hacky experiment to gauge how much impact this has on
performance. We already distinguish between functions to functions and multi
arg functions for performance. That sets precedent for special casing <- as
well.

Of course, if we are going this far, the question will be, how about
supporting partial application too?

See partial_application.txt. Sounds like we should support full partial
application as part of this. But defer to fble-0.3.

---

Total build time is up from something like 6 minutes to 27 minutes. I'm sure
it's all from html generation.

Let's do some profiling, see what's up.

Baseline:

time ./out/pkgs/fbld/bin/fbld ./fbld/nobuildstamp.fbld ./out/fbld/version.fbld
./fbld/html.fbld ./tutorials/tutorial.lib.fbld ./tutorials/Features.fbld >
/dev/null

real    7m4.093s

Interestingly, HelloWorld.fbld finishes in about 1 minute, but Features.fbld
takes much longer. Perhaps we can isolate the problem to a particular
syntactic construct?

Maybe it's worth minimizing the test case, see what's really taking so much
time.

There are a few places we could look to fix things:
* html.fbld.
* /Fbld/... source code.
* fble implementation.

I'm inclined to go after /Fbld/... source code first.

The trouble is, Features.fbld takes too long to easily evaluate performance.
With profiling enabled, it's going on 25 minutes now.

I'll let it go in the background. In the meantime, let's profile
HelloWorld.fbld. Hmm... Still taking a long time. Is there a bug with
profiling in compiled code? Did I break something when I inlined the calls to
FbleCall in aarch64? No, Features.fbld just finished.

Looking at Features.prof:

1. 30% self time in Int Ord functions.
Almost entirely from character comparison.
All from List comparison, so must be string comparison?

From Map lookup, and a little bit from modify.
From Eval implementation.

We are spending 3614888 / 5813778 total time in looking up commands.

Before we do anything based off this, let's check perf profile. I can probably
wait 7 minutes for that.

From HelloWorld.perf.txt:

Shows similar: map lookup in eval. In this case closer to 35% than 62%.
Regardless, a good place to see if we can improve.

Some of this is passing arguments. Some of this is looking up commands. Can we
perhaps take a trace and see what value is being looked up most?

I see the following loops:
* EscHtml does a loop
  EscHtml, tail, str, ifeq, str, EscHtmlChar, head, str,
  ifeq, char, ifeq, char, ifeq, char, char, EscHtml
* ContainsChar, tail, list, c, ifeq, list, head, str,
  FblePlainWord, tail, str, ifeq, str, ifeq, IsFbleNonWord, head,
  str, ContainsChar

At one point I had the idea to lookup commands when we define a command, so we
can reuse those lookups. Does that make sense to do?

The idea is this:
* When we @define a command...

Wait. When we @define a variable, without any arguments, do we evaluate it
once, or do we repeatedly evaluate it every time we refer to the variable?

For example:
 @define[foo][] @f[@x]
 @foo
 @foo

Does this evaluate @f[@x] just once, or does it do it twice?

It does it twice. It does it for every use.

Is it safe to try and evaluate the definition of a command first, as much as
possible?

Certainly if there are no arguments, we could evaluate it once. What are the
semantics? Should I define an @let builtin command to do that instead? I kind
of think so.

Okay, let's start by defining @let which evaluates its argument eagerly and
puts it in the map. See what difference, if any, that makes for performance.
Use that anywhere we are currently doing @define[foo][]...

It would be:

@let[name][def][body]

Looks like that cut runtime of HelloWorld.fbld to html in half. Awesome.

Let's see out Features.fbld fares now:

2m29 seconds.

Awesome. Let's see how our 26min build time looks now. I'm hoping its under
10 minutes now.

14 minutes. Much better. But still longer than I want. I would love to get
total build time to under 10 minutes.

---

The other idea I want to explore: when defining a command, look up referenced
commands in the def before installing the implementation in the new
environment. Have a traversal that goes over the markup, looks up any command
that isn't an argument, and saves the implementation directly in place.

So now Command@ becomes something like

@ Command@ = *(Text@ name, Maybe@<Impl@> impl, List@<Markup@> args),

Hmm... this may be too messy tying up Command@ with Impl@.

Maybe we could define Int@ ids for each command, in addition to the name?
Store an Int@ along with the name and do command lookups by that if possible?
Perhaps Env@ is a map from String@ to Int@, and a map from Int@ to Impl@?

Then we could do all the String@ lookups when we define a command, including
args. Because my guess is that Int@ lookups will be significantly faster than
String@ lookups.

Still, it would be nicer if we could do direct pointers to the implementation.

Maybe have the environment for the def of a command to execute in be only
those commands that it would look up? Presumably that's much smaller than the
total number of commands, so many fewer lookups needed?

---

Trimming the environment brought Features.fbld down from 2m19s to 1m54s, which
isn't bad. Unfortunately I don't think we can do this in general. Because we
might have an @eval, which looks up commands in the environment that are
referenced from arg values we don't know about ahead of time.

Profiling shows maybe 30% of total time in allocating objects. A lot of
functions related to the use of Result@ monad. Still more time is spent in the
command Lookup.

I wish I could do the command lookup at @define time instead of invocation
time. In theory we could, if we had a way to represent that in Markup@, but I
don't want to pollute Markup@ with eval implementation details.

Maybe it's okay to pollute Markup@ with eval implementation details? It's not
that hard to define Env@ and Impl@. I just want to add a Maybe@<Impl@> to
Command@ to store an already looked up command. Do the command lookups in
@define.

What do you say? Shall we try it? I think so.

The pollution isn't bad. This drops Features.fbld time from 2m19s to 1m45s, a
25% improvement. That's pretty good considering total time for lookups was 34%
of runtime before.

Time to rebuild all the fbld docs is now 8m52 seconds. The longest docs take
around 3 minutes.
