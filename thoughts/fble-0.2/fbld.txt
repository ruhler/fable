Fbld Improvements
=================

I'm happy with the syntax for fbld. The implementation needs work.

One idea is to try implementing fbld in fble. Partly as an excuse to write
more real world fble programs, partly because it almost feels nicer to use
fble than tcl given things like tracking error locations.

I see two big challenges with using fble for fbld:
1. How to make it easily extensible. Can users write their own front ends and
back ends? Would they have to write fble? Is that too much to ask? How can we
compose with other existing programs like groff?

2. How to deal with bootstrapping dependency of fble on fbld and fbld on fble?
It's a bit annoying, because it's just the help text for fble-stdio. Maybe
have an option to compile fble with and without that usage text and do a mini
bootstrap that way in the build system.

Big things I want to improve for fbld (regardless of what language we use to
implement it):
* Location tracking, so we can point to the exact place in a file where an
  error occurs.

* Composition: so we can easily implement commands directly in terms of other
  commands if we want to, instead of having to convert back into strings and
  re-parsing.

Both those things come down to this challenging aspect of fbld:

The commands have to parse args using fbld, the fbld parser has to execute
commands using the commands. That creates this cycle going back and forth
between the parser and commands where we have to keep track of and pass
location information back and forth. We can't just parse the whole thing all
at once and then execute commands all at once.

---

This should be pretty straight forward I think.

Assume some monad for computation/side effects.

We have generic commands that don't care about the monad. They can be
parametrized by it. The monad lets you call a sequence of commands.

Define fble functions that take typed (post-parsed) arguments.
Define a generic command map that takes non-typed List@<String@> arguments,
parses the arguments appropriately, and forwards that to the typed fble
functions.

The implementation of a command can call either the raw fble function it
wants, or invoke a command via the generic command map.

Typed fble functions for backends can work with specific monads. For example,
maybe we have an HtmlM@ for generating html documents.

We have a block parser and an inline parser that both work solely with generic
monad via the generic command map.

For composition, you create a command map that works for a particular
specialized HtmlM@ monad. You invoke the block parser, which causes all the
commands to be executed recursively in the monad. You can do what you want
with the results of that. Easy.

Note: instead of List@<String@>, we'll want List@<StringL@>, or some variation
that tracks locations for strings. And the block and inline parsers should
operate on StringL@ to maintain file location information.

I think this will be nice, straight forward, very composable. The two issues
I see are:

1. How to get the filename for what we are parsing.
Ideally we can open named files rather than just stdin. Then that's solved. We
want that for fble-md5 too.

2. How to let people write their own extensions to fbld without using fble.
They need some programming language to be flexible enough. For ease of use, it
would be nice to have a programming language with an interpreter. Some options
are: tcl, python, fble, some custom new fbld language. I think, let's not
worry about this now.

---

First step: add support for opening named files. Even before we start on fbld
implementation. We can do this on md5.

Goal is to change the command line interface to md5 by adding an option to
pass one or more file names on the command line (or '-').

Output format is:

<sum>  <filename>

Or, in case of a directory

fble-md5: <name>: Is a directory

Let me see if I can dig up my old thoughts on how to support file names:
* Skip ReadDir to start.
* (String@) { M@<Maybe@<IStream@>>; } Read;

Maybe it's that simple. Just need to implement it.

The return value should be True if all files are found. False if any are not
found.

---

Getting started on fbld, the tricky part is the knot in the middle.
* The parser needs to know how to invoke commands.
* The invoked commands want to call back into the parser.
* Neither parser nor individual command knows what the other commands are.
* You could get an error message at any point while invoking a command or
  running the parser.
* All of parsing and command invocation takes place in some monad M@, which is
  abstract for the parser and some commands, but may be concrete for other
  commands.

Access to the invoke function, possibility of error, and monad M@ are common
between invocation and parser. The string to parse and the state of that is
specific to the parser. I want some monad transformer Fbld@ that stores the
command invocation function and error handling.

Interface from within Fbld@:
  (Loc@, String@) { Fbld@<M@><Unit@>; } error - reports an error.
  (StringL@, List@<StringL@>) { Fbld@<M@><Unit@>; } invoke - invokes a command.
  <@ A@>(M@<A@>) { Fbld@<M@><A@>; } lift - execute an M@.

Is there anything simpler we could do? Like, commands take invoke as their
first argument and return an M@<Maybe@<String@>>? And parser takes invoke as
their first argument? No need for an Fbld@ transformer monad?

Result@ = +(Unit@ ok, String@ err);
Invoke@ = <<@>@ M@>(Invoke@<M@>, StringL@, List@<StringL@>){ M@<Result@>; };

<<@>@ M@>(Invoke@<M@>, StringL@) { M@<Result@>; } Block;
<<@>@ M@>(Invoke@<M@>, StringL@) { M@<Result@>; } Inline;

Cool. Easy.

---

Ugh. Parsing with locations is tedious. Can I make a little Parser@ monad to
help? I want to do things like:

* Get the next character from the input stream, have location advance
  automatically based on the character in question.
* Run a Parser@ computation to get some value at the beginning of the input,
  returning the rest of the input.

(Parser@<A@>, String@) { *(A@ x, String@ tail); } Run

And maybe it can do errors too. And this can be fully separate from M@, right?

Might be nice if we can have a running state though. So:

(Parser@<A@>, S@) { *(A@ x, S@ s); }

Where S@ stores info about the string, the location, errors, etc.

Things to write:
 * Parse a character.
 * Parse plain text before next @... command.
 * Parse to end of matching ']' character.
   - Could result in an error if unmatched.
 * Parse command name.
  
---

I bet I can entirely split the parser off from M@. Write code to parse inline
text into a sequence of commands and block text into a sequence of commands.
No need for M@ at all there.

---

I need to update the parse to pass around the current indent level and allow
parsing of indented strings. Otherwise we'll mess up column numbers when
parsing nested block structured text.

This brings up an interesting question. What should be the location for next
line arg in terms of column number? What if it's an empty line?

Loc@ needs an indent field, which defines what the column at the start of the
next line is. That's a separate issue.

Next line argument should be, for example, column 2. If it's a blank next
line, it should be... doesn't matter I don't think.

I need to come up with a test case to capture locations properly. Maybe, parse
a command. Extract the next line argument. Parse that, verify the locations
are set properly. Yeah.

---

Inline and Block parsing is done. What's next?

Let's review all the current use cases of fbld. I'll want to migrate them over
one by one to fble based fbld, I would think.

* build.lib.tcl - @config, @BuildStamp
* @doc --> html
* @tutorial --> html
* @usage --> man (1)
* @usage --> c header
* doc comment --> man  (3)
  - Extract doc comments from file.
  - Convert doc comments to man.
* Core tags syntax check.
* Core tags html back end.
* Core tags man back end.
* Core tags roff back end.
* @tutorial front end.
* @usage --> help front end.
* @usage --> man front end.
* @usage library

Let's start with Core. Core is an interface. In other words, a struct type
parameterized by abstract monadic type M@.

And we have an instance of Core@ for html, man, text, and optionally 'check'.

We don't currently use 'check' anywhere. So why bother with it?

* Define /Fbld/Core% with Core@, some way to convert typed functions to
  generic fble command invocation.
* Define /Fbld/Core/Html% instance of Core@ on an abstract monad with
  OStream@.
* Try and use fble fbld to generate fbld.html.
 - Will need a solution for @FbleVersion, @BuildStamp

Cool. That sounds like a good next target.

What type do we use for the raw types of functions? Before we said
'post-parsed' arguments.

Anywhere we expect ESCAPED, we should accept /Core/String%.String@;

Anywhere we expect INLINE...
  List@<Command@>?
  M@<Unit@>?

Because we can go:
  String@ --> List@<Command@> --> M@<Unit@>

From fble code, M@<Unit@> is most generic.

String@ --> List@<Command@> is either Inline or Block parse.
List@<Command@> --> Core@.* is either Inline or Block invoke.
Core@.* --> M@ comes from the back end.

Where do we put the logic to know if we should inline or block parse/invoke?

We'll want two different Invoke@ functions: one for inline, one for block. So
we can distinguish between tags depending on whether it's an inline or block
context.

We'll have a function from Core@ to *(Invoke@, Invoke@) that defines how to
sub-parse and sub-invoke each argument.

---

Type inference isn't working:

  <@ A@>(M@<Result@<A@>>)<@ B@>((A@) { M@<Result@<B@>>; }) { M@<Result@<B@>>; }
  DoR = <@ A@>(M@<Result@<A@>> ma)<@ B@>((A@) { M@<Result@<B@>>; } f) {
    Result@<A@> ra <- m.do(ma);
    ra.?(err: m.return(Err<B@>(ra.err)));
    f(ra.ok);
  };

      (Command@ cmd, M@<Result@<Unit@>> mx) {
        Unit@ _ <- DoR(mx);
        inline(inline, block, cmd);
      });

In the application of 'DoR':

../pkgs/fbld/Fbld/Invoke.fble|30 col 24| error: expected type M@<Result@<A@>>, but found M@<Result@<Unit@>>
|| Inferred types:
||   A@: ???

Even if I explicitly give the type of A@ as Unit@. Is this expected?

M@ is fully abstract. There's no way to expand that. Do we know that M@<X@>
equals M@<Y@> if X@ = Y@? Or do we not take advantage of that fact?

Yes. we say poly equal and arg equal.

I should be able to step through this in gdb and trace what's going wrong.
It's just a little tedious to get to the right call in gdb.

Because we should say Result@<A@> equals Result@<Unit@>
Which then should give us A@ equals Unit@
Which should infer things just swell, right? Or, I suppose Result isn't
abstract, so we will say:

+(Unit@ ok, String@ err) versus +(A@ ok, String@ err).

And that should work just fine.

---

Found the issue. I have two different abstract types M@ defined at different
locations. Type error is working as intended aside from not making it clear
the difference between two different types with the same name.

---

How should I implement @FbleVersion and @BuildStamp tags?

The way it's implemented in tcl:
* The build script creates a version.fbld.tcl file to define
  inline_FbleVersion, pulling $::version as the value.
* build.lib.tcl defines inline-BuildStamp which executes the buildstamp
  program whever it is invoked, ensuring we have the latest build stamp
  whenever we run the program.

Let's start with @FbleVersion, which seems slightly simpler.

Options:
A. Define /Fbld/FbleVersion% that hard codes the value of @FbleVersion.
 - Remember to update it any time the version is updated.
B. Define /Fbld/FbleVersion% that reads the raw value from
  /Fbld/FbleVersion/Value%, or some such. Have the build script auto-generate
  that /Fbld/FbleVersion/Value% file as, /Core/String%.Str|'...';
C. Hard code the version, but add a test that it matches what's defined
  elsewhere.

(B) sounds easy enough to me. I can't think of any other sane options. The
only trick with (B) will be figuring out how to add the generated code to the
include path.

Let's think about @Buildstamp next, which is trickier. The goal is to get the
latest value of buildstamp every time you run, e.g. fbld-html-doc.

Maybe that suggests we should read the buildstamp value from a file when
fbld-html-doc starts? Or... should we add an 'exec' option to Stdio@ for
executing other programs?

How do we want generic 'fbld-html-doc' and fble build specific @FbleVersion
and @Buildstamp to work? It's like, how do we add custom tags after the fact?
Should I worry about that for now, or don't care and worry about it another
day?

---

Another challenge: implementing the @code tag. Can we reuse source-highlight
from within fble? How?

I think we need exec support to be able to compose with other programs. That
will let us reuse source-highlight (assuming we can find the path to it in some reasonable way?) and buildstamp (again, assuming we can find the path to it).

I'm thinking, for exec, also have a 'fifo' function to return IStream/OStream
pairs, then pass stdin and stdout streams as an argument to exec along with
command line arguments. It forks off the process to execute in the background.
Maybe we return an M@<Int@> to be able to block on and fetch the exit status of
the process.

Another approach would be to re-implement source-highlight equivalent in fble.
Sounds like an interesting program to try and write. The trouble is, there are
a lot of languages to have to support.

---

I've finished fleshing out the core tags. Going to have to decide what's next
to work on. A few options:

* Add a markdown backend so we can generate README.md.
* Implement mapping from foo.fbld to foo.html for @fbld html backend.
* Figure out how to specify libraries like spec/fble.fbld.tcl
  Where to put the .fble file?
  Do we want a separate binary for this other than fbld-html-doc?
* Figure out how to implement @FbleVersion
* Figure out how to implement @BuildStamp
* Figure out how to do source highlighting for @code

---

We want users to be able to supply their own front ends and own back ends, and
have different front ends and back ends be combined together.

I claim this cannot be done today in fble with a compiled binary.

To pass a new front end or back end to an existing compiled binary, you need
some way to convert the .fble code into its corresponding fble value at
runtime, dynamically. Options:
* Put the .fble code into the compiled binary, pass a name as a reference to
  the code. But this doesn't allow new code, it only allows code in the
  binary.
* Come up with a different syntax for implementing tags that can be parsed
  and loaded from fble. But then we are not using fble to implement tags.
* Implement a parser for fble that can turn it into a value at runtime. But...
  we would need an interpreter in fble?

The point is, unless we have some magic way to take a String describing a
module path or .fble filename and dynamically load it, we can't pass new fble
modules on the command line as arguments.

The other route is for the user to provide the main function. Then they can
pass whatever code they want. They can run the interpreter or compile it if
they so choose.

---

What is a front end? It would be nice to have a specific type for frontends so
you can easily combine them.

Front ends are built on top of Core@, and possibly other front ends. There are
two ways we may want to use a front end:
1. Directly.
2. As markup.

To use a front end directly requires knowing the specific type of the front
end. To use it as markup just needs generic Markup@ for Inline and Block.

Core@ is special because it is an interface, not an implementation. Other
markups can pass around their implementation directly as functions to be
called.

So, I expect a front end is an fble module that takes Core@ as an argument and
defines a function for each tag supported by the front end, and Inline and
Block Markup@.

Having modules be functions is starting to be problematic. Consider a front
end B that depends on front end A, and both depend on core.

B wants to access A directly. We'll end up invoking the A module function
twice: once when defining B, and once at the top level. That does not scale
well.

An alternative is to provide A as an argument to B. But that is like exposing
internal implementation details. Whoever is using B shouldn't have to care
about whether B uses A internally. They do need to care that B uses Core
internally, because the user has to provide the backend implementation of
Core.

Well, it all happens during setup, and these functions aren't called that
much, so maybe don't worry about it for now.

A related thing is having false dependencies. For example, Core module takes
M@ and Monad@<M@>. It exports Core@, which doesn't depend on Monad@<M@>.
That's a false dependency introduced. If the user only uses Core@, it
shouldn't need to provide Monad@<M@>.

We could address this by splitting up Core into the type and the
implementation. Either as separate modules, or within the same module as, e.g.

@(Core@, (Monad@<M@> m) { ... @(InlineMarkup, BlockMarkup); } Impl);

Again, let's not worry about it for the time being.

Next issue: it's slightly annoying to pass around everything as inline or
block. Can we unify somehow? Ideas:

Don't distinguish between inline and block markup. Use a single markup for
everything. This means you can't have a tag interpreted differently for inline
versus block markup. The only place we do this right now is the default inline
and block tags, both called "". But we can give those separate names if we
want, like "_inline@" and "_block@".

That way you could use any markup in either context. Umm... that's a little
concerning. What does it mean to use section@ in an inline context? And @l in
a block context? For it to make sense, we would have to do different
processing for tags in each. No, I prefer to type a tag based on inline or
block context.

The other idea is define a type Markups@, or some such, that has two fields,
inline, block. Then we can more easily name, pass around, and combine things.
We could do the same for Invoke too perhaps. invoke.inline and invoke.block.

---

Say we want to pass around a single invoke function instead of two. How would
we represent that?

@ Context@ = +(Unit@ inline, Unit@ block);
@ I@ = (I@, Context@, Command@) { M@<Result@<Unit@>>; }

Invoke.Inline becomes: invoke(i, Inline, cmd);
Invoke.Block becomes: invoke(i, Block, cmd);

Now markup can become an abstract type where you can insert entries as Inline
or Block.

The reason this is better than what we have today is because we can pass
around a single Markup@ object that stores both inline and block tags. I think
it's worth trying.

Does it make sense to do other cleanup while I'm at it?
* Separate different types out of /Fbld/Types%?
* Make a better monad transformer for M@<Result@<T@>>?
* Get rid of R@ helper type.
* Use a name other than String@ for Fbld String@, so we can using String@ for
  /Core/String%?
  Like, StringL@. That's not bad. Let's do that.

---

I'm thinking we want two different types. I'm not sure what to call them:
1. A function that takes global context and a command and executes the
command.
2. A global context, which is a pair of invocation functions, one for inline,
one for block.

Brainstorm of names:
* Invoke@, Context@.

Anything better than Context@ for (2)?
* Env@ is better.

So, what do define where?
* Command@
* Invoke@, Env@
* Invoke.Inline, Invoke.Block - that parse and execute structured text.
* Is Markup@ the right term to use? Or would that more often refer to the text
  itself rather than the tag definitions?

My vote:
* Command@ should be separate from Invoke@, because Parse doesn't need
  Invoke@.
* Invoke@ and Env@ must be defined together, because they depend on each
  other. I think Invoke% is fine for this.
* Maybe Interpret.Inline, Interpret.Block? Exec? Eval? Impl? Run?
  I like Exec or Run. Let's say Run, because it is a complete word.
* "markup" means the marked up text, not the tags. What other name could we
  use? Markup Language? Language?
  - This brings an interesting idea: should StringL@ be renamed to Markup@?
    No. I don't think so.
  How about Schema@? Tags@? Honestly, I like Markup@... Maybe think of it as
  the definition of what markup elements can be used in the doc. Yeah. Let's
  go with that.

We have our answers. Get to work.

---

The markdown backend makes things difficult:
* You can only define labels on section headers?
* It just feels like a ton of work to do properly.

Is it worth trying to do properly for real? In case it reveals issues with the
general structure of how fbld is implemented? Like, I fear I'll need to track
some other internal state for text wrapping and indentation and nested lists.

Anyway, the generated README.md looks fine to me except for @FbleVersion and
@BuildStamp not being implemented yet. Maybe let's focus on those first. If I
can generate README.md, then set up the template to have it checked in and
tested against the generated version, that will be great.

Actually, I should remove @BuildStamp, because we don't want it changing every
time. I don't think @BuildStamp is so important for that. That means we just
need to figure out @FbleVersion.

---

@FbleVersion is taken care of now. Let's revisit @BuildStamp.

@BuildStamp is intended to capture the version of the documentation. If we
left it in, we want it to execute buildstamp at the time the document is
produced, not at the time we build fbld-html-doc or whatever.

There are two ways to do that:
1. Pass it as a command line argument when we run.
2. Have fbld-html-doc be able to execute buildstamp at runtime.

For README.md, I'm thinking of checking it into source code. It's pretty
annoying if we keep modifying it for every buildstamp.

The fact is, we don't expect README.md to change for every git sha.

Could we update buildstamp and only check it in when README.md changes
otherwise? That way the buildstamp would be accurate, we would have a build
stamp.

On the other hand, buildstamp of index.html kind of represents the buildstamp
of the entire set of documentation, which certainly could have changed. And I
like having the build stamp in the website documentation. It's really nice
that it has the date along with the documentation.

What if, instead of saying README.md is a source file and we check it against
a generated file, instead we generate README.md from ninja directly to source?

But that means the build process dirties the source, which could mess up
buildstamp for other docs.

I could be really hacky and not implement buildstamp especially for markdown
backend. Because README.md is only expected to be read from github when you
can easily check what the git sha is.

I'm going to want to implement @BuildStamp eventually. How about this:
* Pass @BuildStamp as an argument to the main function. We can easily call it
  as part of the build command. For README.md we can pass "" as the build
  stamp, or HEAD, or something like that.

Sounds good.

---

Looks like my @FbleVersion implementation is breaking ninja. Ninja is going
into a loop and gives up after 100 tries. For the time being, let's use the
same approach for @FbleVersion we use for @Buildstamp, passing it on the
command line.

---

The biggest blocker for using fble based fbld right now is @code syntax
highlighting. Specifically for 'fble' and 'fbld' languages. If I could support
those two languages, that would be enough. Aside from that, txt, tcl, sh, not
so important I don't think.

Both fble and fbld should be pretty straight forward to do syntax highlighting
for. How would that look?

We want to separate two parts:
1. text to highlight regions (backend independent)
2. implementation of highlight regions (backend dependent)

We can use fbld markup to annotate the highlight regions in a backend
independent way. So the syntax highlighter could convert StringL@ to MRU@,
where that MRU@ is inline text.

We would want the following kind of markup:

@comment[INLINE]
@label[INLINE]
@string[INLINE]
@type[INLINE]
@symbol[INLINE]
@special[INLINE]
@include[INLINE]
@identifier[INLINE]

Maybe we want an intermediate implementation that maps all those to colors,
and then we just need:

@highlight[style][INLINE text]

Where style is color, or maybe we want bold, underline, italic, etc.

Note: this brings up an issue that's been in the back of my mind for fbld:
namespace control. How do we make sure people don't pick conflicting tag
names? For example, @label means something else to Core fbld.

Anyway, the part to figure out is how to convert text to marked up highlighted
text.

Start with fbld:
* @[a-zA-Z0-9_]\+ for a tag
* [, ] for bracket
* \[, \], \@, \\ for escape

No need for regex for that. We can code a lexer very easily. In fact, now that
you mention it, we could code a full parser if we wanted. I'm not sure it
makes a difference in this case. Maybe for fble.

How about fble? I don't think it would be too hard.

Let's start with fbld, implement it, wire it up, see how it looks.

Maybe we use a single tag, like

@hi[class][INLINE text]

---

Working on tutorial front end. I need to think more about how the helper
functions should work and the fble functions.

For example: BuildStamp function. No arguments. But it wants a Loc@ perhaps to
form a location for whatever text it calls. But that depends on the
implementation of BuildStamp.

Consider tutorial's Tutorial function. This wants to reuse BuildStamp and
Version. Should it take those as arguments, or take Env@ assuming those?

Take A2. Some functions might want access to the command location. Most I
would say. What about Env@? It needs that because some wrapper functions want
to convert structured text.

---

Possible next steps:
* Work on syntax highlighting for 'fbld' language.
* Add usage, usage.help, and usage.man front ends.
* Doc Comments frontend.
* Add man backend.
* Add txt backend.

---

For syntax highlighting, I vote we start simple.

Interface is String@ to List@<*(Class@ class, String@ text)>. Assume to start
that we don't have nested highlighting items. Start with fbld, then go from
there.

---

The man backend doesn't implement @doc, it replaces it with a different typed
@man tag. How am I supposed to handle that? I takes extra 'section' and
'source' arguments.

Maybe we implement @doc, but give an error message saying man backend does not
support @doc tag, use @man tag instead? And export both Core and Man top
level values from the backend?

The implementation of fbld in fble is feeling pretty messy right now. Things
like that, and:
* Having to write lots of invoke utility functions.
* Overheads of passing M@ and m everywhere.
* Inability to do dynamic front ends or back ends.
* Not knowing when to pass Loc@ to functions.

---

Let's imagine a different world for fbld.

Most front end tags could be described as simple transformations. For example:

@define[AbstractSyntax][content]
 @definition[Abstract Syntax][@code[txt][@content]]

Okay, so that one doesn't work great, because we wouldn't substitute @content
inside an @code block. Unless it's a special thing for @define.

Try again:

@define[tutorial][name][content]
 @doc[@name]
   @FbleVersion (@BuildStamp)

  @content

That doesn't say how we would implement @FbleVersion or @BuildStamp. Maybe
define those with auto-generated code:

@define[FbleVersion][fble-0.2]
@define[BuildStamp][asdfasefase]

Assume we come up with a reasonable approach for this, now we have an easy way
to define new front ends dynamically. You just write @defines into your docs.
And maybe have an @include too, to help arrange things.

Imagine we have sufficient type information about each tag that we can know
how many arguments it expects, and whether those arguments are 'RAW',
'INLINE', or 'BLOCK' structured text. Then we could parse the entire document
in memory as something like:

@ Fble@ = +(StringL@ raw, Command@ command, List@<Fble@> list),
@ Command@ = *(StringL@ name, List@<Fble@> args);

Front end tags are reductions we can apply. Apply all possible reductions.

At this point, hopefully you are left with only tags that your backend knows
how to handle. Pass it off to the backend. We could enumerate all the tags
left over and call that the overall 'type' of the document, and find a backend
that supports that type. Or just have the backend fail if it sees a tag it
doesn't expect.

There are a few corner cases to double check:
* @fbld tag and similar which can take one or two arguments.
  Have support for optional or default arguments?
  Or come up with different names for the different overloadings of the tags
  like this? @i versus @ii, for example. @fbld[...] versus
  @link[fbld][label][...]? Or @fbld[...][...] versus @fbld[...][]?
* usage man page front end generates @man back end tag instead of @doc.
  I don't see any problems here.
* doc comment processor processes tags twice with different interpretation.
  It's not a problem if it's backend, because we can do custom processing
  there. Maybe we want a way to do more custom front end tag processing
  though.

To simplify the problem to start, let's imagine front end tags can be defined
different ways. Via @define. Via custom fble code. Via custom tcl code. Via
custom process. Whatever.

Could we start by just revamping the current implementation of fbld in fble,
without changing the fbld spec? Instead of passing monads around everywhere,
pass around the 'value' of the doc?

We have two ways of processing a document:
1. Front end: apply reductions until you can't anymore.
No monads required. Stays in Fbld@ -> Fble@ land.

2. Back end: convert to backend specific types.
Maybe we want a monad. Maybe more specific typing?

Alternatively, if we think of a backend as always producing text of some form,
and we say transformations can go Fbld@ to raw String@, then backend need not
be special?

All my current backends are straight up string translations. I've structured
them so they don't need any intermediate state. I don't know if that's what we
can expect for things like markdown or txt, where we want text wrapping and
need to know state of nesting and other things like that.

What I'm saying is, it would be nice to unify front end and back end
transformations. But that gets you back, I think, towards where we are today.

Let's explore more. Say a document is one of:

1. Raw string: there is nothing you can do to simplify further.
2. Command: You can convert the command to another document. 
3. List of ???: 
  You can keep as a list, or concatenate into a raw string?

---

After much thought, I've ended up pretty close to where we are today. Some
minor differences:

* Maybe we want to add implicit @[] commands for block and inline text that
  wraps the list of comments in a sequence.
* We should pass OStream to all parts of commands, and allow subinvocations to
  provide alternative OStream implementation. This will allow us, for example,
  to reflow or trim text produced by subcommands.
* Consider defining a Markup backend that has a tag like
  @define[command][arg1][arg2]...[body]
  As a syntax for describing user defined commands. Not as part of the doc
  itself, but as an input to the fbld processor to describe markup to add to
  the processor.
* Maybe separate OStream and CharOutputStream, rather than assume we always
  want to write to byte streams instead of directly to char streams.

This way a front end and back end are kind of the same thing. We could
implement a backend as a direct string translation just like front end.

If we had the ability for tags to modify the environment, we could potentially
define things like @define to use within docs, but don't worry about that for
now. Having an @define syntax for describing markup would allow us to have
dynamic front ends and back ends. It's limited to whatever we provide for
string manipulations in the language. You could imagine a full programming
language, or something tcl like that lets you do full string manipulation. Or,
for the time being, just allow simple substitutions of text.

Let's keep forging ahead as is. Keep in mind the above changes, but wait until
there's a clear use case to make those changes.

---

I changed Core.doc to take a Loc@, which is needed for the man backend to
report the error message for that. This suggests every Core method should take
a loc.

If every method takes a Loc@, what is the value of having Core@ type? Why not
just construct a command directly by name?

The idea was calling directly would give better type info: number and type of
args. But type of args is just StringL@ or MRU@. It doesn't give that much
info.

I suppose we like being able to pass a monadic computation as an MRU@
argument.

Imagine if we referenced, e.g. Core@ only from Env@. Is that better or worse?
No need to depend on Core, but on the other hand, easier to make type errors
in terms of typos or wrong number/type of arguments.

A third option: we could provide an abstract instance of Core@ that takes from
the environment instead of being the ultimate backend?

---

On further thought, we don't want front ends to be able to write strings
directly to the output stream, right? We only want them to write things via
Text backend tags so that backends have full control over what's being written
to the output stream. ?

---

For usage.lib front end, I implemented it as direct fbld text parsing. Some
interesting things came out:
* No need to depend on Core or other things within fble code, which is nice.
* Ran into @config missing error, which is the other side of the coin for
  that.
* The location tracking doesn't work, because I'm giving the location for
  where the tag is called, not the location for where the tag is being
  defined.

---

How to handle @config?

Even if I had a way to read tags using @define, that doesn't give me a way to
implement @config, which wants to switch on the argument value.

It's getting a little old to pass everything like this on the command line,
but I suppose that would be the natural continuation. Pass a config.txt or
some such file to fble-fbld, it can read it and define the @config tag that
way.

Options are:
A. Pass config.tcl or equivalent to fble-fbld.
B. Figure out build issue to support generated fble source files.
C. Come up with a sophisticated enough language for defining tags in fbld that
we can generate and describe the @config tag that way.

For the fun of it, let me spend some time thinking about (C).

---

Say we want a full fledged fbld based programming language. Sufficient to
define almost any tag you would want. It would be something like tcl: string
based arguments. What do we need to support?

* Variable definitions.
* Tag definitions.
* Conditionals
 - Presumably based on string comparison.
* String operations
 - Concatenation, search/substring to support structures.
* Loops and/or recursion.
 - Integer arithmetic to support loops and/or recursion?

Say @foo[...] returns a string, and can modify the environment of
tags/variables. The top level document computes the value of the top level tag
and returns it. A sequence of tags results in a concatenation operation (maybe
some implicit tag we can redefine).

We need some primitive tags to support lets, conditions, string operations,
etc. We probably need namespace support in practice: allow some special
characters in tag names. An @include tag would likely be useful to organize
code into different files.

The overall vision here is to define a programming language, instead of a
markup language. Using the syntax of fbld. You run that program to convert the
document.

It's not bad. I don't see any major downsides. I think the syntax should work
fine for a command/string-based programming language.

A variable is the same as a zero-argument command.

Function definition:

@define[name][arg1][arg2]...[body]

Adds a tag @name to scope.

Order of evaluation? Do we pass the arguments into the body first before
evaluating, or evaluate first? Or do we just define a tag that's visible in
the body? This matters for things like:

@define[ConcreteSyntax][content]
 @definition[Concrete Syntax]
  @code[txt][@content]

1. If we just define a tag inside the body, @content will never be expanded.
2. If we inline the contents before evaluation, then nothing inside the
   content will be expanded.
3. If we evaluate the contents before inlining, then all the contents will be
   expanded.

What if we have argument types:
* raw: arg not evaluated before inlining.
* inline: arg parsed as inline structure before inlining.
* block: arg parsed as block structure before inlining.

If the second argument to code is raw, how can we possibly define substitution
in there?

What if the result of inline or block structured text is structured text?
Somehow that sounds not reasonable. Like the result should always be in raw
text form.

Let's execute:

    @ConcreteSyntax[@foo[bar]]

Do a substitution:

 @definition[Concrete Syntax]
  @code[txt][@foo[bar]]

Evaluate the body (still as part of @define/application)

We cannot execute the argument before knowing how to interpret it, either as
raw or structured text.

Okay. So what trick could we use to have @foo[bar] expanded before becoming
code?

Tcl distinguishes between {...} and "...". Maybe that's the idea: when passing
an argument, where you do application, you can specify if you want to evaluate
the argument first as block, inline, or raw text. That probably needs a
language syntax change, but it could be useful. For example, maybe use [] for
raw, [[]] for inline, [[[]]] for block? Now you have full control?

Or, maybe we have a separate tag for that. Like:

@apply[@code[txt]][@foo[bar]]

Turns into: @code[txt][expanded_foo_bar]

No new syntax needed. But maybe a bit clunky to specify what we want.

---

What would variable scoping look like? Because if we don't interpret an
argument until it's expand into a function, then we need the caller's scope to
be visible from that function.

We have three different expression syntax:
1. raw string.
2. inline structure.
3. block structure.

What we really want is a way to specify which syntax you are using when you
pass an argument. We've already seen examples where, for the same tag, you may
want different syntax used.

In the current fble, the syntax to use is implicit, based on the command it's
being passed to. Implicit is nice, because it doesn't clutter the source text.

How do we reconcile implicit nice versus wanting more control? We saw one
approach above: @apply. Maybe there are different variants on it we could use.

In practice, do I ever put block structured text in an argument that is not
a next line argument? Well, I put raw text in next line argument.

Could we distinguish between different ways of invoking commands? For example,
we have @foo, which passes a command, which will be interpreted wherever it
finally lands. What if we had $foo, which says you interpret the command
before passing it down?

Let's go back to the example in question:

 @ConcreteSyntax[@foo[bar]]

 @definition[Concrete Syntax]
  @code[txt][@foo[bar]]

Assume second arg of @code expects raw text.
Assume arg of @ConcreteSyntax expects raw text. Then it works fine, but we
have a new problem.

How can we pass substituted text where raw text is expected?

Simplify the example:

  @l[@foo]

In one case, I want this to refer to the literal string "@foo". In another, I
want this to refer to the result of interpreting @foo as inline text.

  @inline[@foo][@l[$1]]
  @let[x][@foo][@l[$x]]
  @inline[x=@foo][@l[$x]]
  @l[$foo]
 
  @let[x][@inline[@foo]][@l[$x]]

  @l[:@foo]
  @l[[@foo]]


  @a[...][...] ...
   ...

By default args are raw text. We want some way to override individual args
separately. Some syntax we can sneak in there.

  @a[$ ...][$ ...] $ ...
   $
   ...

Idea here: if the text of the argument starts with $<space>, it is first
interpreted as inline structured text. Maybe !<space> for block structured, or
something.

Now, if we do this, commands are no longer side effecting things, they are
value returning things? Otherwise it's not clear where the side effects of
those commands should take place.

  @inline[x][@foo][@l[$x]]
  @block[x][@foo][@l[$x]]

  @inline[cfg][@config[hello]][@l[$cfg]]

I feel like this kind of thing is the cleanest approach. Not very readable,
but get's the job done, no special syntax required. Arguments continue to be
interpreted by the command running them.

If we wanted to, we could factor this out:

 @define[ll][text][@inline[x][@text][@l[$x]]]

And now use: @ll[@foo] or @l[@foo] depending on which we want.
I guess, similarly, we could do:

 @code[...][RAW ...]

or @code_i[...][INLINE ...]
or @code_b[...][BLOCK ...]

Define wrapper functions like this. Less flexible, but entirely capable.

---

Taking a step back to summarize:
* We have syntax for block and inline structured text.
* I'm very happy with writing docs in this syntax.
* The current question is how to implement tags.
* Proposal is to reuse the same syntax for writing docs as defining tags.
* We've established that arguments are not parsed before passing them to
  commands. Commands interpret those arguments as they see fit.
* Next step is to come up with a collection of primitives that let us define
  the kinds of commands we want.
* Two classes to consider: front ends, back ends.

If I want to go this route, a reasonable next step would be to sketch the
implementation of all our current front ends. That will help iron out details.
After that, sketch the implementation of a back end like html, to see what
that entails.

Top level user flow: let's assume you pass a list of .fbld files, which are
conceptually concatenated together in order and read as block structured text.
That way we can mix and match back ends with documents. For example, you might
have:

  fbld html.fbld spec.lib.fbld spec.fbld

We could have spec.fbld directly import spec.lib.fbld. That makes sense. But
it doesn't make sense for spec.fbld to directly import html.fbld, because we
may want different backends.

---

fble.fbld.tcl:

@define[AbstractSyntax][content]
 @definition[Abstract Syntax]
  @code[txt][$content]

@define[ConcreteSyntax][content]
 @definition[Concrete Syntax]
  @code[txt][$content]

@define[Example][content]
 @definition[Example]
  @code[fble][$content]

This requires our first primitive, @define.
  
  @define[name][arg1][arg2]...[body]

Defines a new command @name that expects the given number of arguments.

Execution of that new command substitutes arg values into the body based on
$name, then executes the result as block structured text.

Questions:
* This defines a block command. How do we define an inline command? Presumably
  have different names. Maybe @define_b, @define_i, or something weird like
  that. It's not too important right now.
* What if we want to use $... somewhere in the body without matching an arg?
  - Pick a different name for the arg so it doesn't conflict.
  - Say only matched $name are substituted, others are left as is?

---

tutorial.tcl:

@define[tutorial][name][content]
 @doc[$name]
   @FbleVersion (@Buildstamp)

   $content

@define[exercise][content]
 $content
  
Works great. Nothing more needed.

---

usage.lib.tcl:

@define[GenericProgramInfo]
 @subsection[Generic Program Information]
  @opt[@l[-h], @l[--help]]
   display this help text and exit

  @opt[@l[-v], @l[--version]]
   display version information and exit

@define[ModuleInput]
 @subsection[Module Input]
  @opt[@l[-I] @a[DIR]]
   add @a[DIR] to the module search path

  @opt[@l[-p], @l[--package] @a[PACKAGE]]
   add @a[PACKAGE] to the module search path

  @opt[@l[-m], @l[--module] @a[MODULE_PATH]]
   the module path of the input module

  Packages are searched for in the package search path specified by the
  @l[FBLE_PACKAGE_PATH] environment variable followed by the system default
  package search path. Modules are searched for in the module search path.

  For example, if @l[FBLE_PACKAGE_PATH] is @l[/fble/pkgs] and you provide
  command line options @l[-p core -m /Core/Unit%], the @l[-p core] option adds
  @l[/fble/pkgs/core] to the module search path, and the @l[-m /Core/Unit%]
  option will look for the module at @l[/fble/pkgs/core/Core/Unit.fble].

  The system default package search path is @config[datadir]/fble.


Works great.

---

How to define interfaces? Don't worry about it. Programs are expected to be
short and fast. It's fine to give an error 'tag not defined' in that case. Or
define a null backend that supports the tags you expect to have and check the
doc that way.

---

@define_i[FbleVersion]
 fble-0.2

So yes, we want different ways to define for block and inline commands.

I think I want inline define to be a block tag, not an inline tag. Any better
name ideas?

@defblock[...]
@definline[...]

@defb[...]
@defi[...]

Yeah, I like that. Unlikely to conflict with a user chosen tag, explicit about
block versus inline.

---

usagel.help.tcl

@defb[usage][name][brief][content]
 @doc[$name][$name - $brief]
  $content

How do we 'trim' $brief? Why is that there right now?

We pass this as a same line argument to a block command. There should be no
need to trim that. But, let's imagine. For example, imagine we want to convert
the brief to uppercase? How would we do it? And imagine we have an inline
command @upper.

@defb[usage][name][brief][content]
 @doc[$name][$name - @upper[$brief]]
  $content

That's not hard.

---

build.lib.tcl

Here's an interesting one. @config[name] is supposed to return the value. How
would we define that?

@defi[config][name]
 @if[@eq[bindir][$name]][/usr/local/bin]

But that doesn't scale up to a bunch of options. It would be one, huge, long
line? Well, we could sneak some newlines in, right?
 @if[@eq[bindir][$name]
   ][/usr/local/bin][
 @if[@eq[mandir][$name]
  ...

That's a little tedious. If we use nested if like this, we have lots of braces
to close at the end. How could we do elseif?

The way tcl does it is accept multiple var args to if:

 @if[a
   ][va][
  elseif][b
   ][vb][
  elseif][c
   ][vc][
  else
   ][???]

The important thing here is that I'm putting newlines in the conditions and
places where we could ignore whitespace, not in the actual values themselves.

Could we somehow define a list of pairs instead and implement a function to
search that list?

Or, maybe we have a command that takes block structured text, evaluates it,
and then interprets it as inline structured text? But @defi already does that.
So we should be able to say:

 @if[a] va
  @if[b] vb
   @if[c] vc
    ???

No worry about braces, just a bunch of indent.

Or maybe we define, like, an @concat command that turns block structured text
into inline text?

 @concat
  @if[a]
   va
  @if[b]
   vb
  @if[c]
   vc
 
Which works if a, b, and c are mutually exclusive, but doesn't give us a final
else branch.

If we had primitives for working with dictionaries, we could do:
  @defi[configs]
   @dict[bindir][/usr/local/bin][mandir][...]
  @lookup[@configs][$name]
  
But, again, this annoying thing of not being able to split up a single inline
command easily across multiple lines. If we had that ability, it would be
great.

---

I want to start with implementing @defb and @defi, and use it to replace all
of the current front ends. Let's go incrementally.

1. Implement a defb/defi helper functions in fble. It takes a list of argument
names and a string body, and returns an Invoke function capable of doing the
desired substitution. We can use this to replace the implementation of all our
front end markup.

2. Once switched over to defb/defi helper functions, we should no longer need
to pass around interfaces like Core@, Usage@, Man@. Everything relies on the
environment to invoke sub commands. So we can clean up the code based on that.

3. I'd like to transition away from abstract monad M@, and towards concrete
String@ result. That way we can define a concrete monadic type that allows
state update to Env@.

4. Add defb/defi markup commands.

5. Move frontends from .fble to .fbld.

We can worry about the rest later. That will be a good start.

Remember: defi and defb are both block commands, but defi interprets the
result as inline and defb interprets the result as block.

---

One thing that's tricky is keeping track of locations after substitution.
Should we not worry about it for now? Make up a useless location?

---

One challenge with defb/defi: when we substitute into a block context, we may
need to indent. For example:

@code
 $content

If $content spans multiple lines, the appropriate way to substitute in would
be to make sure all those lines are indented.

How do we know when it's safe to do that though? Because if we wrote instead

@code[$content]

Then we shouldn't be indenting all the lines. Maybe we ask the user to provide
that by passing '$$' for indent substitution and '$' for direct substitution?

A better example might be:

@code[
 $content
]

Do we want to indent here? I actually don't know.

Let's ask the user to do it for now. That's not to onerous I don't think.

---

A little bit of weirdness about $ versus $$: you can't use $ if it has
multiple lines and you have indented at all. For example:

@foo
 @code[txt][$content]

Does not work, because it wraps to column 0 instead of column 1, so part of
the content is read as the command after @foo instead of the body of @foo.

I hope we don't get into situations where neither of $ nor $$ are good enough.
Maybe we could always indent by whatever amount of whitespace is on that same
line? Let's see what issues we run into first and consider cleaning this up
later.

---

I would really like to pull @defi and @defb out of fble into fbld directly.

To do that requires we make the environment modifiable.

Which means I want to define an Fbld@ monad and get off of this abstract M@
monad.

The Fbld@ monad should have the following capabilities:
* Return error messages.
* Return computed String@ values. Except I think we'll want some smarter
  string type that supports fast append, concat, and location tracking.
* Access and update the environment.

Invoke@ would then become: (Command@) { Fbld@<FbldString@>; }

Let me define the better string type first. We can start with a direct wrapper
around String@ and improve efficiency later. This should replace StringL@. I
wish I had a better name.

Once we have that, do that major shift to new Fbld@ monadic type. Then we can
implement @defi and @defb commands, and pull out all of the frontends and
backends, hopefully.

---

I have defined a Text@ type. There's plenty of work to do to clean that up,
but can we move on to an Fbld@ monad?

It means having all of the documents loading into memory at the same time, but
we kind of do that currently anyway.

Let's give it a try. I'm hoping this will simplify the code a lot.

Fbld@ is a function from Env@ to Result@<(Env@, A@)>.
Env@ is a pair of Invoke@.
Invoke@ is a function from Env@, Command@ to Result@<(Env@, Text@)>
 aka. A function from Command@ to Fbld@<Text@>;

Interesting question: can we actually define this type in fble? I'm pretty
sure yes, but we might have to start by defining Fbld@<Text@> as a separate
type.

---

Okay, so made the switch to a concrete Fbld@<Text@> monad. It was a big
change. I haven't bothered about location tracking yet. The code takes a long
time to compile. This should put is in a good place to define an @defi, @defb
commands and start pulling things out of the implementation and into separate
.fbld files.

---

In order to support dynamic insertion of commands into the environment, can we
have the environment be Markup@? Is there any reason we wouldn't want to use a
map?

One reason is because that would make Fbld@ even more tied up into a knot.
Maybe I can not worry about it for now. Just do a linear chain search.

---

Time to see if it works out. Let's remove one of the easier front ends from
fble, port it to fbld. Let's say tutorial? Except, that doesn't seem to work
at the moment, and I don't have any location info to help. Hmm...

README works. Can we start with that? Or should I just do some manual things?

With a simple manual test... it totally works. That's cool.

---

Indent issue with defb:

@defb[synopsis][text]
 @par[Usage: $text]

Will that work? Maybe?

I think that's okay. It's the nested version that wouldn't work:

@defb[...][text]
 @foo
  @par[Usage: $text]

---

Looking into performance of fbld. It takes a couple minutes to convert
fble.fbld to fble.html. It should take only a couple of seconds.

Profiling shows, as expected, most of the time is in Text Append. In this
case, mostly from calling HtmlText.

Let's update Text@ to support fast append.

---

Next step is to figure out @config tag.

I want a switch. How about something like:

@defb[config][arg]
 @switch[$arg]
  @case[bindir] /usr/local/bin
  @case[builddir] .
  @case[datadir] /usr/local/share
  ...
  @default
   @error Unknown Config Option: $arg
   
That's a very nice syntax for string switch. It suggests the body of @switch
is interpreted as block structured text with @case and @default tags defined
and nothing else. Anything wrong with that?

It uses a different 'concat' operator. Instead of Concat, it takes the first
nonempty string value. Importantly, the @default is only taken if no other
branch is taken.

I say we go for it.

Can we do this as pure string manipulation? All except @default handling,
where it needs some context about what happened before to know if it should be
included or not.

How about as one long @switch command?

 @switch[$arg]
  [bindir] /usr/local/bin
  [builddir] .
  [datadir] /usr/local/share
  [@error Unknown Config Option: $arg]

Here I'm using a new syntax that says we can start arg sequences by using [ at
the start of the next line. Maybe we want \ character or something? Not sure.

I like the version with @case much better from a read/write point of view.

Imagine we had a tag @first that takes block structured text and returns the
first non-empty element. And we had an @ifeq primitive:
  @ifeq[a][b][body]

It returns body if a and b are equal, empty otherwise. Then we could write:

@defi[config][arg]
 @first
  @ifeq[$arg][bindir] /usr/local/bin
  @ifeq[$arg][builddir] .
  ...
  @error Unknown config option: $arg

That's a simpler set of primitives, don't you think? If I knew how to
implement it under the hood.

@error is easy to implement.
@ifeq is easy to implement.

@first is difficult, especially if @error has an immediate side effect instead
of just returning text.

You almost want Run.Block and Run.Invoke to return List@<Fbld@<Text@>>. Then
we could handle things pretty easily. Go one at a time until we get non-empty
(or error).

---

Let's go back to the notion of a command having side effects.  Say it has side
effects and returns a string value. The challenge with if/else is mainly
syntactic. It doesn't naturally fit into 'list of commands' syntax because it
has two separate bodies.

We saw in fble how to get around that: make the next thing after the if be the
else branch. That's if you are using more of a functional style. The other
approach would be to allow commands to influence control flow: @return, for
example.

Because everything is a string, and I want to write the string directly
instead of writing @return[...], let's go with the functional approach. How
would this look? Can it be meaningfully specified?

@if[p] then body
 else body

Would lead to:
@if[p] blah
 @if[q] blah
  @if[z] blah
   @blah

As we've already mentioned. I don't know a way to say @if[p] blah returns
right away, breaking out of the body of what's being executed. But what's to
stop us from that?

Then we consider:

@par[foo] returns foo and continues.
@return[foo] returns foo and does not continue.

Or, @par[foo] adds foo to the current list of results and continues.
@return[foo] adds foo to the current list of results and does not continue.
@defb[...] does not add anything to the current list of results and continues.

So, when we interpret block structured text (or inline structured text?), the
result is a list of strings. Whichever command interprets that list of strings
can combine them however they want. By default, they are concatenated.

Any reason to do anything other than concatenate?

And, again, we have the problem of not know exactly which context a command
takes effect in.

---

How to move forward? I don't want to get stuck trying to design a perfect new
programming language. I want to make progress.

Question: Should we implement minimum primitives to support a general purpose
language  and build everything else as a library, or should we implement high
level primitives that do what I need?

The fear of minimum primitives is performance will be terrible.

Answer: I want support for general purpose programming in .fbld language.
Let's start with that. If performance turns out to be a problem, we can add
some higher level primitives built in.

So, focusing on the minimum primitives to start, what I want to be able to do:
* general purpose programming. If I can figure that out.
* @config[...] implementation.
* html escape implementation.

Really it's the last two that matter for making forward progress. The first
one is something to keep in mind.

When coming up with primitives, there are two things to figure out:
1. What primitives we want.
2. The class of behaviors that a primitive can have.

For (2), we currently have:
* Return a string.
* Add a new tag to the environment.
* @defb/@defi do argument substitution before evaluation.

The things we'll need for general purpose programming:
* struct literal, struct access
* union literal, union access, conditional execution
* functions
* recursion: types and functions.

We have a single built in, recursive, struct/literal type: string. That's
general purpose enough from the type point of view. What operations we need on
strings:

* string literal
* string concat (provides "struct literal")
* string head/tail (provides "struct access")
* string switch (provides conditional execution)

Maybe I also want variables, or equivalent. Like a @let tag.

Arguments to primitives can be arbitrary strings interpreted in arbitrary
ways. For consistency, it would be good not to have arbitrary different syntax
for different primitives. Try to reuse inline and block structured syntax as
much as we can.

For @defi,@defb, we've also introduced variable syntax $... Is that necessary?
I think it's useful to have. Distinguish between '$' substitution, which
happens when @defi,@defb is applied, versus '@' substitution, which happens
when the text in question is eventually, finally, evaluated.

There's still the open question of dealing with indentation for $
substitution.

Let's come up with some straw proposals. Really the big ones are switch and
head/tail.

What I want to implement, in practice:
* switch for config@.
* map for htmlescape.

The map@ command wants list argument. So list is something that would be nice
to have.

We already have a natural syntax for lists: block structured text (newline
separated), and inline structured text (command separated). Imagine then:

@list
 First item.

 Second item.

 Third item.

Or: 
@list
 @item First item.
 @item Second item.
 @item Third item.

Or:
@list[@item[First item.]@item[Second item]@item[Third item]]

Or:
@list[First item][Second item][Third item]
 
The list we are interested in is a list of pairs. Using tcl's approach for
mappings, it's a single flat list.

@list[&][&amp;][<][&lt;][>][&gt;]

@defb[htmlescape][text]
 @let[mapping] @list[&][&amp;][<][&lt;][>][&gt;]
  @map[$mapping][$text]

Or, perhaps:
 @split[ ][& &amp; < &lt; > &gt;]

I guess @split would translate that to the string:
 @item &
 @item &amp;
 @item <
 @item &lt;
 @item >
 @item &gt;

How to implement map@?

@defb[map][mapping][body]
 @if[@startswith[@first[$mapping]][$body]]
  @second[$mapping]...

I kind of want pattern matched based head/tail. So, either: head and tail,
given a pattern to match as counting as the head. Or head and tail given a
pattern to split on. I think the first is better.

So, say we have a notion of a pattern.

@head[pattern][string]
 
Returns as much of the beginning of the given string that matches the given
pattern.

@tail[pattern][string]

Returns the rest of the string not matched by @head[pattern][string].

Maybe pattern can be glob:
  ?, *, [chars], \x

Glob is generally useful syntax for dealing with strings. What if we make that
the core? Instead of ifeq, we have a glob match for if. head/tail take a glob
pattern to try and match against.

For example:
  @head[?][$x]

Returns the first character of $x.

If we want to match everything up until the first space, something like:

@defi[first][pattern][str]
 @let[head] @head[$pattern][$str]
   @if[$head] $head
    @first[$pattern][$tail[?][$str]]

Proposed primitives:
* @let[var][def][body]
  Returns the value of body with "$var" replaced with the value of def
* @if[condition][then][else]
  If condition is non-empty, returns value of then, else returns value of
  else.
* @head[pattern][str]
  Returns as much of the beginning of str as matches pattern.
* @tail[pattern][str]
  Strips as much of the beginning of str as matches pattern.

The syntax for @if nesting isn't entirely idea, but should be workable for the
time being.

Thus, we have:

@config[x]
 @if[@tail[

Uh, it's harder to match an entire string this way. I want something like:
 @if[@match[bindir][$x]] /usr/local/bin
  @if[@match[builddir][$x]] .
   @if[@match[datadir][$x]] /usr/local/share

And so on.

Why not make it much simpler? Skip the glob. All we need is:

* @let[var][def][body]
* @if[condition][then][else]
* @head[str] - returns first character of the string, or empty.
* @tail[str] - returns all but first character of the string.

We can implement @eq or @match ourselves with this.

@defi[eq][a][b]
 @if[@head[$a]][
  @if[@head[...

No. We loose character equality test that way, which is annoying.

How to match an entire string is easy: if @head matches and @tail is empty.

@defi[match][pattern][str]
 @and[@head[pattern][str]][@not[@tail[pattern][str]]]
  
---

After some more thought:
* Don't worry about fancy matching support.
* Provide exact string equals (which is as simple as character equals)
* Provide @head and @tail that return first character of a string and rest of
  a string respectively.

Another key idea: let's expand the syntax of fbld to provide better support
for multiple block arguments. The real challenge with @if is we don't have a
nice syntax for multiple block arguments.

Proposed new syntax:

A single '@' on the line following a block command extends the command. You
can put another same line argument on the same line, and a next line argument
on the next line.

For example:

@if @eq[$a][$b]
 This is what to do if condition holds.
@
 This is what to do if condition doesn't hold.

We could do a list of single line args:

@foo
@ Single line arg
@ Single line arg
@ Single line arg...

We could do a list of multi line args:

@foo
@
 First of 
 a multi line arg.
@
 Yet another multi
 line arg.
@
 And another.

Second proposed syntax: '@@' on the line following a block command causes the
rest of the text to be passed as another argument to the block command.

For example, instead of:

@if @eq[$a][$b]
 This is what to do if condition holds.
@
 This is what to do if condition doesn't hold.

We could write equivalent:

@if @eq[$a][$b]
 This is what to do if condition holds.
@@

This is what to do if condition doesn't hold.
 
Perhaps more useful example would be @let or @defi

@let[var][def][body]

Instead of:

@let[x][@foo]
 The body with $x.

We can write:

@let[x][@foo]
@@

The body with $x.

Now we have full control over glue logic, like <- in fble, conditional
statements, let, etc.

I like this idea syntax wise. No more need for crazy primitives that have
intermediate state if we just have this more complete syntax.

If we wanted, we could write a switch tag:

@switch[$x]
@ bindir
 /usr/local/bin
@ builddir
 .
@ datadir
 /usr/local/share


I guess it makes me want to also be able to write:
@switch[$x]
@[bindir] /usr/local/bin
@[builddir] .
@[datadir] /usr/local/share

So maybe @ resets to what you could write after the initial @switch tag.

If we have more widespread use of multi args like this, we'll want a decent
way to define vararg tags. Most likely they'll want to take a list argument,
like how tcl args works.

With this syntax, no need for tags to modify a global environment. They can
set up a local environment for their arguments. It's more functional that way.

The other open question is about multi-line variable substitution. Instead of
'$' and '$$', can we just have one that behaves exactly as you almost always
want?

@foo
 @bar
  Foo $x

It indents to the same indent as the line it's on. So, if x is "abc\def",
substitution becomes:

Embedding in inline text:

@foo
 @bar
  Foo abc
  def

Embedding in block text:

@foo
 @bar
  abc
  def

The only weird thing is if you want a different indent for the start compared
to the end.

@foo
 @bar
   abc
  def

When would you ever want that? Let's assume you wouldn't and see if I ever run
into such a case.

In summary, updates to syntax for fbld:
* @ on a line after block command extends the command's args.
* @@ on a line after block command adds one last block command arg.
* $ is only substitution supported, it indents to same indentation as current
  line.

Proposed primitives:
* @eq[a][b] - returns "true" if equal, empty string otherwise.
* @if[p][a][b] - returns a if p non-empty, b otherwise.
* @head[str] - returns first char of str, empty if str is empty.
* @tail[str] - returns all but first char of str.

Question: should @let be different from @defi, @defb? Probably.

* @let[var][def][body]

For backends
* @raw[str]
  Evaluates str as block text, returns the result directly.

There is still some confusion about what happens in a block context versus an
inline context. For example, @if is a block tag, then a and b are interpreted
as block. But maybe @if is also an inline tag, so a and b are interpreted as
inline. For example, in block body of @defi, maybe we want an if statement in
block form that returns inline text? Maybe we just need:

* @block[str]
  Inline tag that interprets str as block text.
* @inline[str]
  Block tag that interprets str as inline text.

In that case, we have the following cross products to specify when defining a
tag:
1. Is the tag being defined as an inline tag or block tag?
2. Does the tag interpret its arguments/result as raw, inline, block?

For (2), we can just use @raw, @inline, @block to specify. Assume the body of
a definition is always specified using block structured text.

That suggests, instead of @defi, @defb, how about a single @def that takes
inline or block as one of it's arguments.  Maybe i, b, or ib both?

@def[i][FbleVersion]
 @inline[fble-0.2]

Not sure. I'm not convinced I see much advantage to that.

Summary of proposed tags:

Block:
* @let[var][def][body]
* @defi[tag][args...][body]
* @defb[tag][args...][body]
* @raw[str]
* @inline[str]
* @block[str]
* @if[p][a][b]

Inline:
* @eq[a][b]
* @if[p][a][b]
* @head[str]
* @tail[str]
* @raw[str]
* @inline[str]
* @block[str]

Where @let, @defi, and @defb all use $ substitution with indentation as
described above.

Now:

@defi[config][x]
 @if[@eq[$x][bindir]] /usr/local/bin
 @@
 @if[@eq[$x][builddir]] .
 @@
 @if[@eq[$x][datadir]] /usr/local/share
 ...

Alternatively, instead of @if and @eq, we could have @switch?

@defi[eq][a][b]
 @switch[$a][$b][true][]

@defi[if][p][a][b]
 @switch[$p][][$a][$b]

Yeah. Even better.
  
Block:
* @let[var][def][body]
* @defi[tag][args...][body]
* @defb[tag][args...][body]
* @raw[str]
* @inline[str]
* @block[str]
* @switch[arg][branches...][default]
* @error[msg]

Inline:
* @head[str]
* @tail[str]
* @raw[str]
* @inline[str]
* @block[str]
* @switch[arg][branches...][default]
* @error[msg]


@defi[config][x]
 @switch[$x]
 @[bindir] /usr/local/bin
 @[builddir] .
 @[datadir] /usr/local/share
 @ @error[No such config: $x]

@defi[html][str]
 @raw
  @let[t][@html[@tail[$str]]]
  @@
  @switch[@head[$str]]
  @[][]
  @[&] &amp;$t
  @[<] &lt;$t
  @[>] &gt;$t
  @ $t

Now we get into trouble again with substitution. In the above example, assume
$t has newlines in it, which it will. $t doesn't get indented appropriately.
Unless we put it on a separate line?

@defi[html][str]
 @raw
  @let[t][@html[@tail[$str]]]
  @@
  @switch[@head[$str]]
  @[][]
  @[&]
   &amp;$t
  @[<]
   &lt;$t
  @[>]
   &gt;$t
  @
   $t
 
You almost want newlines in substituted values to not be treated as newlines
for the sake of parsing whatever was substituted into? Like, imagine we have
an escape for newline. \n. Then part of substitution is to change newlines
into \n? Is that what we really want though? In some cases $t might be a
sequence of commands we definitely want to be able to parse as such.

@defi[html][str]
 @raw
  @let[t][@html[@tail[$str]]]
  @@
  @switch[@head[$str]]
  @[][]
  @[&][&amp;$t]
  @[<][&lt;$t]
  @[>][&gt;$t]
  @[$t]

That works, I think. Just needs a bit of care.

Okay? We have a plan to move forward.
