Fble Compilation Challenge
--------------------------
Should we compile fble to machine code before evaluating it, or should we
continue to use an interpreter?

Advantage of compilation:
* Potential for substantial performance improvements.

Disadvantage of compilation:
* It's more tedious to run a program if you have to invoke a c compiler, for
  example.

What about AOT vs JIT vs ...?

That's more a question of what representation a program should be distributed
in. The tradeoff is: too high level a representation requires compilation /
interpretation to be done somewhere. too low level a representation takes up
more space and is limited in where it can be run.

I guess today the representation is .fble, which is relatively compact, but
takes some time to compile and is slow to interpret. If we compile, the
representation would be .o, which may or may not be as compact, but is much
faster to run.

Regardless, we need to go to well specified formats (.fble, .c, .o, etc.), and
in general it's good for the framework to support separate pre-processing
passes, so I think this is worth introducing.

This is an end user API change. Instead of loading a .fble program, you want
to load a .o file. Or maybe you want to link against a compiled .o program.

Proposal: given a .fble program, compile to an object file that exposes the
following API:

FbleValue* Eval(FbleValueHeap* heap);

And in general, once you have an FbleValue, you can run it efficiently with
the existing FbleApply and FbleExec APIs, it just that code will be
represented as pointers to machine code instead of pointers to instruction
blocks.

---------

It seems worth doing some more detailed research and up front design for this.
Consider the following options to start:
* Generate c code.
* Use llvm.
* Use c--.
* Generate a custom, compact, efficient bytecode.

C-- looks obsolete now, and I'm not inspired by Haskell's performance.

llvm looks like a very complex entity I'll have to learn a lot more about if I
want to use it.

C code will be pretty clunky.

My vote is to experiment with llvm to start, and maybe spend some time
thinking about a custom bytecode.

Things to figure out how to do in particular in llvm:
* call into an fble runtime library
* tail call behavior - calling into a function as if by goto instead of
  pushing something on the stack
* storing pointers to code

Perhaps a nice way to avoid a direct dependency on llvm would be to generate
llvm's text format assembly language. Not sure. We'll have to see.

---

It seems fine to use llvm as the bytecode instead of a custom bytecode. It's
presumably just as compressible as whatever I come up with, but has the
advantage that there is existing technology to convert it to machine code.

We still want to have control over threading, however. Which means llvm isn't
at the bottom of the execution stack, it's in the middle, between the thread
scheduler and the actual computation.

Let me start calling llvm native code. The value of using native code over an
interpreted bytecode is you get hardware support for incrementing the pc,
fetching the next instruction, predicting branches. That could easily give you
10x improvement over interpreting instructions.

The concern is that the thread scheduler, or interaction with it, may start to
dominate performance overhead. I expect the native code would be a function
you could call that would do some finite amount of work on a thread and then
return a continuation. This means we can't use the native stack, unless the OS
supports some form of voluntary context switching we could leverage. And it
means we'll end up with a scheduling loop on top things that doesn't take
advantage of hardware support for fast pc incrementing, etc. If the amount of
work that gets run on a thread is small, that would be bad. Which suggests we
want to inline as much as we can to get decent size chunks of uninterrupted
native code.

In summary, some things to think about:
* Inlining to get decent size chunks of interrupted thread execution.
* See if llvm has any support for cooperative multithreading, or manipulation
  of the thread.

---

In terms of a custom bytecode. If we wanted all instructions to have the same
size, we could do it with:

FbleInstrTag tag;
bool exit;
FbleLoc loc;
size_t tag; // or blockid, or pc.
FbleFrameIndexV args;
FbleLocalIndexV dests;
FbleInstrBlock* code;

Notes
* exit could be incorporated into tag
* args is a vector for struct and scope, otherwise it could be 2.
* dests is a vector for fork, otherwise it could be 1.
* code could be a pc in theory, if all the instructions were saved into one
  big array. Though I guess we would need some other way to initialize the
  locals memory.

---
It looks like llvm has a tailcc calling convention that can be used to support
tail calls. Good.

It doesn't look like llvm has any general support for multithreading.

Let's see if we can implement our own multithreading mechanism. The idea is to
separate execution into three parts:

1. thread scheduling. Decides which thread to execute next.
2. cooperative multithreading. A way to run a computation for a fixed amount
of time (in terms of instructions executed probably), then snapshot the thread
state in a way that can be later restored.
3. running computation for a single thread.

(1) is easy enough, assuming we have a way to represent thread state.
(3) is easy enough. Just compile to native machine code. Take full advantage
of hardware support for pc, instruction fetch, stack manipulation.

(2) may be doable. Here's what I envision: at the start of each block, we add
a 'yield' instruction. The yield instruction says how many instructions are in
the block. It checks to see if the current time slice is up, and if so,
magically calls into the thread scheduler. For example, maybe it causes
registers to be saved to the stack, records the stack pointer and pc
somewhere, then switch out the stack pointer and pc for another thread to
resume.

I expect thread scheduling to be low overhead, because switching threads
happens so rarely. The big cost here is having to keep track of how many
instructions we have executed and whether we need to yield or not. Because
that needs to be run very often.

I wonder if there's any value to yielding randomly instead of strictly by
count. That way we don't have to pass the count around everywhere. We would
still need to compute a random number, but we could use a global variable for
that? We could use the same random number as we do for profile sampling? As
long as it is pseudo-random, it will be deterministic. Is that meaningful? At
least is is runtime environment agnostic, if not agnostic to the generated
instructions.

---

gprof suggests there isn't that much to gain from compiling. Because the
execution time is dominated by allocations and memory management work.

---

Threading revisited: we can use pthreads if we want, or pth, or getcontext.
For pthreads, we just need a mutex that a thread has to acquire before it can
do any work. That way only one thread runs at a time.

Problems with pthreads:
* non-deterministic behavior is, right out of the box, causing lots of
  problems. Bugs go away under the debugger. Different bugs from different
  runs. That's going to be a debugging nightmare as long as we use pthreads.
* limits on number of threads. Like, 32K number on linux? That's not really
  acceptable. Max user processes of 7314 by ulimit. Definitely not acceptable.
* only the main thread has a dynamically sized stack. Though I'm not sure how
  we could ever get around that limitation. gcc's -fsplit-stack is not
  supported by the gcc preinstalled on my computer.

---

Idea is to compile blocks of instructions together that we can execute
atomically, while still using the interpreter to drive execution. Hopefully
this cuts down noticeably on the cost of instruction dispatch, while still
supporting full control over thread execution.

We can start by converting the existing instructions to abstract functions
that the interpreter calls. This will make the interpreter compatible with
abstract functions operating on thread state. The compiler can then generate
some new abstract functions and that works fine with the interpreter.

The concern is there will be too much overhead to calling these abstract
functions. Let's do some experiments.

The most commonly called instruction at the moment in fble-bench, by far, is
FBLE_RELEASE_INSTR. It accounts for over 40% of instructions executed.

One nice benefit of putting instructions into functions is then profiling can
tell us which instructions take a lot of time, and how much time is spent on
instruction dispatch.

Baseline fble bench: 2m9.762s

Experiment:
  Access thread state through thread pointer instead of local variables.

  fble bench: 2m10.890s

Experiment: 
  Pull ReleaseInstr into its own function.

  fble bench: 2m12.889s
  
If we assume all instructions regress the same amount, then total I would
expect 7.5s overhead changing all instructions to functions. But then we can
get rid of the binary search from the switch statement, which I expect would
get us all of that back. We also get the advantage of per-instruction
profiling breakdowns and the opportunity for compilation. I think it's worth
trying. Let's do it.

After converting a bunch more instructions over:
  fble bench: 2m12.251s

After converting all the rest of the instructions and getting rid of the
switch statement entirely:
  fble bench: 2m16.704s

That's too bad. Total regression of 7s, even after removing the case
expression.

Well, if it's any consolation, we get 1m12.644s with -O3 turned on and no
profiling arcs.

---

Starting up again to try and get an fble compiler.

Let's start as simple and basic as we can. Generate C code which the user can
compile using a C compiler. No need to bother with llvm at this point. I can
make my own abstractions for representing C programs if I don't want to just
output strings everywhere.

Primary goal: replace the interpreter while loop with straight line code that
calls the same exact exec functions the current interpreter does.

Today we have two kinds of functions:
* FbleFuncValueTc,
* FbleCompiledFuncValueTc

The first represents a function value as an abstract syntax tree of values
after type check. The second represents a function value as an fble
instruction block. Maybe we can add a third type that represents a function
value as pointer to a C function to call?

It's starting to come back to me now.

Goal is to separate thread scheduling from function execution. We currently
track thread state in thunk values. It would be nice if we could have two
kinds of thunk values: one tracking func, pc, and locals for interpreted code,
and one with a function pointer and locals for compiled code?

Maybe what we want is an abstract data type representing thing that can be
partially computed.

 Inputs: 'this', heap, thread, io_activity
 Results: updated 'this' or finished or aborted, etc.

With a single method to 'run some finite amount of time'.

Or, change instructions to hold the actual C function pointer for executing a
function. Then the compiler could merge adjacent instructions that can be
compiled together into new C functions and insert them into the instruction
stream?

I don't know. I need to think on it a bunch more.

---

Two big challenges for compiling fble that I keep getting stuck on:
1. Fine grained multithreading. Which is painful, because I feel like I could
get away without it fble in practice, but that we really want it in theory.

2. General support for tail calls not smashing the stack. I double checked.
gcc and llvm can be made to support a special kind of tail call where the
function being called has the same prototype as the caller. Is this good
enough? Maybe, depending on how we abstract away function calls.

But I can't get a simple example to work for gcc using -O2 and
-foptimize-sibling-calls if the calls span across multiple .o files. Seems
frighteningly restrictive.

