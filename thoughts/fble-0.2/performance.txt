Fble 0.2 Performance
====================
Working on optimizing the sat solver, it's showing 50% time in Lt/Ord
operation on Int@. That .fble code looks about as straight forward as it could
get. Anything worthwhile we can do in the fble implementation to speed things
up here?

A few ideas come to mind:
* Remove profiling logic from generated code?
 - Maybe put it behind a compile time flag?
 - Because it seems silly to slow things down all the time for a rarely used
   use case.
 - It's probably worth at least measuring how significant this is.
* Remove explicit error checking from generated code?
 - Maybe we just segfault and ask users to run a debugger to see the issue?
 - Maybe we can use signals somehow to trap and handle these cases implicitly?
 - Probably worth at least measuring how significant this is for performance.
* Improve value packing algorithm.
 - Though honestly I don't think that will help at all for Lt/Ord, because I
   don't expect any allocations there.
* Something about function call/tail call optimization?

---

Reviewing perf profile for the sat problem in question:

It shows only 25% time spent in Ord, and about as much time in
FbleNewHeapObject. It could be hard to interpret the profile, given how tight
a recursion is going on here.

Self time shows 8.5% in the Ord function. Everything else is runtime code:

    %     self  block
 8.49     4110  _Run.0x22673a48._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_[0029]
 6.50     3145  MarkRef[0043]
 5.86     2838  FbleRetainHeapObject[0010]
 5.70     2760  FbleThreadTailCall[002e]
 4.86     2350  FbleStrictValue[002d]
 4.64     2248  FbleUnionValueTag[002c]
 4.54     2196  FbleThreadCall[0001]
 3.80     1840  FbleThreadSample[003e]
 3.70     1790  FbleReleaseHeapObject[003f]
 3.48     1682  IncrGc.part.0[001a]
 3.39     1639  Refs[004c]

Reading through the assembly, I feel like we could inline parts of things like
FbleReleaseValue, FbleRetainValue, FbleStrictValue, etc. Avoid the function
call overhead and having to save/restore regs.

My vote is we do some exploration. Don't worry about breaking things or
features. Use a separate branch. Remove profiling and error checking code.
Maybe try inlining things. Focus on aarch64 compiler target. See if we can
flesh out what we can do in terms of generating optimized code and how big a
different it has a potential to make.

After the exploration, we can consider if we want to change anything for
production.

---

First thing, establish a benchmark.

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m37.040s
user    1m36.508s
sys     0m0.469s

Next: Let's strip out all the profiling code. Anything that would be in the
critical path for runtime, including stuff in execute.c and generated code.

Commenting out profiling related code, all except tracking profile block
offset stuff for functions:

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m31.467s
user    1m30.843s
sys     0m0.537s

Attempted again:

So, Maybe 7% performance overhead from having the profiling code in when
profiling is not enabled.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m33.136s
user    1m32.554s
sys     0m0.508s

Next: Let's strip out all the generated error checking code.

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m22.338s
user    1m21.828s
sys     0m0.476s

Next: let's look at the generated assembly code for IntP@ Ord, see how it
looks.

* The jump table looks potentially more expensive than need be?
.L._Run_0x31b06a48.0.pcs:
  .xword .L._Run_0x31b06a48.pc.1
  .xword .L._Run_0x31b06a48.pc.5
  .xword .L._Run_0x31b06a48.pc.15
  ...
  
  bl FbleUnionValueTag
  lsl x0, x0, #3
  adrp x1, .L._Run_0x31b06a48.0.pcs
  add x1, x1, :lo12:.L._Run_0x31b06a48.0.pcs
  add x0, x0, x1
  ldr x0, [x0]
  br x0

  This is a load followed by a branch. Difficult for branch prediction to
  predict anything from I would guess. Don't you think it would be much faster
  to do a binary search based on the tag?

  For example, consider a linear search on the 3 branches in this case:

  bl FbleUnionValueTag
  b.eq .L._Run_0x31b06a48.pc.1 x0 0
  b.eq .L._Run_0x31b06a48.pc.5 x0 1
  b .L._Run_0x31b06a48.pc.15

  No memory load required. We can take full advantage of branch prediction.

  And, if we use this approach, or a binary search, we can take advantage of
  the default option the user provides. So, character equality, for example,
  instead of being a 64*64 split branch, would be a 64 * 2 branch split. That
  sounds like a pretty big deal.
 
* Lots of calls to FbleRetainValue, inlining could help I think. In most cases
  this is either skipped entirely for packed values, or just increment, right?
  
Let's explore the generate of union select first. The right way to do it, I
think, is pass info about default branches down to the bytecode, and use
binary search. That's tricky to do and get right. For a first pass, just to
gauge how sensitive performance is to this, let me switch to a linear approach
like above. It should work fine for small branch counts. Won't work great for
this like character conversion.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m21.529s
user    1m21.114s
sys     0m0.398s

---

Another opportunity to improve performance I just saw: for union access, don't
check the tag value. Assume it's the right value. Hard to productize this,
but would at least be nice to gauge the cost.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m19.422s
user    1m18.999s
sys     0m0.381s

---

Trouble: my numbers aren't very reliable. I see swings +- 2s in some cases
when running the same thing, and sometimes I forgot to build in the change I
intended.

Looks like error checking code is more costly than I first thought. That
changes things.

I kind of wish I had a more extensive set of benchmarks than just the sat
solver. Can we add md5? Is there an invaders benchmark?

Anyway, I couple ideas I think are worth exploring:
* Changing how we generate union select statements. Even if the numbers above
  don't suggest much difference, it feels like the right thing to do.
* Finding some alternative to error checking.
  For the most part it's NULL checking. We could use memory protection and
  signals to handle that?

---

Examples of what gcc generates for switch statements:

  switch (x) {
    case 0: printf("0\n"); break;
    case 4: printf("4\n"); break;
    case 5: printf("5\n"); break;
    case 6: printf("6\n"); break;
    case 7: printf("7\n"); break;
    case 12: printf("12\n"); break;
    default: printf("?\n");
  }

Turns into the equivalent of:
  if (x == 5) {
    printf("5\n"); break;
  }

  if (x > 5) {
    if (x == 7) {
      printf("7\n"); break;
    }
    if (x < 7) {
      printf("6\n"); break;
    }
    if (x == 12) {
      printf("12\n"); break;
    }
    printf("?\n"); break;
  }

  if (x == 0) {
    printf("0\n"); break;
  }
  if (x == 4) {
    printf("4\n"); break;
  }
  printf("?\n"); break;

It uses 'beq', 'bhi', and 'bcc' for the conditional branches. That is binary
search. 'CC' stands for 'Carry clear', same as less than. 'HI' stands for
'unsigned higher', same as greater than.

---

Migrating to new union select.

Benchmarks for awareness:

time fble-sat < rope_0002.shuffled.cnf: 1m39s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m42s
time yes | head -n 40000 | fble-md5: 0m59s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m41.013s

Initial draft status:
* Need to update union_select_tc everywhere.
* Need to update compilation from union select tc to instr.
* Want to update aarch64 code gen to do binary instead of linear search.

---

First draft done. It uses binary search for interpreter, but linear search for
aarch64 code.

time fble-sat < rope_0002.shuffled.cnf: 1m39s ==> 1m36s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m42s ==> 41s
time yes | head -n 40000 | fble-md5: 0m59s ==> 53s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m41s ==> 0m39s

So, modest improvement in performance, which is nice. Note: these benchmarks
don't capture the pathological case of large number of tags. Probably should
have wrote something up for that. Oh well.

Next step: binary search aarch64.

---

I made an fble-cat program. I think this should be good to check the
pathological case of large tags, because it converts characters to and from
ascii.

Let's see, for example:

time yes '!!!!!!!!!' | head -n 10000 | fble-cat > /dev/null: 6s
time yes '~~~~~~~~~' | head -n 10000 | fble-cat > /dev/null: 8s

Hmm... It's hard to know how much of this is union select as opposed to what
characters happen to be where in the tree.

---

How to implement codegen aarch64 for binary search based union select. There
are a number of puzzle pieces to put together:

1. We want to know the max tag value possible.
Imagine you have

  goto x.?(1: a, 2: b, 3: c, : d);

You know min tag value is 0. You don't know if tag 4 is possible or not.
Binary search starts checking on '2', if greater than '2', there are two
possibilities:
 i. 3 is the max tag: jump direct to c
 ii. 4 is the max tag: need to compare tag to 3.

Thus we want to know max tag. Add that as a field to select_instr.

2. Determine if a tag interval range has one or more targets.
Inputs:
* list of targets with tags in interval range. e.g. [1: a, 2: b, 3: c]
* possible tag values. e.g. [0, 4]
* default target. e.g. d

Outputs:
  Either the single possible branch target, or note that there is no single
  possible branch target.

There are two cases when there is a single possible branch target:
 i. The list of targets with tags is empty. Default is the single possible target.
 ii. There is a single element int targets, and that tag is the only tag in
     the list of possible tag values.

3. The top level list of explicit targets may be empty.

For example:
  @ T@ = +(Unit@ t); 
  T@ x = ...
  x.?(t: ...);

Or:
  Char@ c = ...
  c.?(: ...);   

I'm not sure if the language allows the second example, but it certainly
should allow the first. We should make sure there are spec tests for these two
interesting cases.

4. You must explicitly branch to all targets. You can't assume the next target
is the next pc.

Who knows what changes we'll make to compile.c. From code.h point of view, the
targets could be unrelated to the current pc. So, in the case of (3) for
example, we require an explicit branch instructions to get to the target. We
can't assume the target is the next pc.
 
5. When generated code, you'll find yourself in one of two situations:
 i. You have space for only one instruction here, any more has to be
elsewhere.
 ii. You have space for as many instructions as you need here.


Putting all these things together, there are really four cases depending on
the case for (2) and the case for (5):

  * Space for one instr, need one instr: just write the instruction. Done.
  * Space for one instr, need multiple: write a branch to some target, add
    todo item when you have space available to do the multiple.
  * Space for multiple instrs, need one: just write the instruction. Done.
  * Space for multiple instrs, need multiple: write the multiple.

Pseudocode starting at the top level:
  Add info to todo.
  While more todo:
    F(MULTI, next todo)

Where F(space, info):
  if info needs only one instr:
    Write the instruction and return.
  else if space is single:
    Alloc a new unique id.
    Write branch to new unique id.
    Add todo for multi.
    Return.
  else 
    cmp x0 mid_tag
    b.eq mid_target
    b.lt F(single, low info)
    F(multi, high info)

Which brings out one more piece of the puzzle: for the 'single' case, you have
to provide the label, the branch instruction mnemonic comes from the parent.
    
Maybe split into two separate functions:
  SINGLE - outputs a label, optionally adds to todo.
  MULTI - outputs instructions.
    
In terms of todo list, we need to keep track of a unique id to use for
internal jumps.

It's a lot of info to manage all at once. We're talking about having a queue
and doing recursive function calls. Any way to do only as a queue?

Queue entry has:
 uid, info

Top level:
 Push (NONE, info)

To process:
 output label
 if single target info
   b.eq target; continue
 cmp x0 mid_tag
 b.eq mid_target
 if single target low info
   b.lt target
 else
   alloc id, push low info.
   b.lt id
   push (NONE, high info)

Make it a stack. This should work. All we need is the helper function to find
the single possible target, if any.
   
Or... instead of needing an explicit stack, use function stack?

Top level:
 F(NONE, info)

F(label, info):
 output label
 if single target info
   b.eq target; return;
 cmp x0 mid_tag
 b.eq mid_target
 if single target low info
   b.lt target
   F (NONE, high info)
 else
   b.lt id
   F (NONE, high info)
   F (id, low info)
 
Yeah. Let's go with that. Just need to pass some shared state about intervals,
targets, etc.
 
---

The results:

time fble-sat < rope_0002.shuffled.cnf: 1m36s ==> 1m36s
time yes | head -n 40000 | fble-md5: 53s ==> 54s

Interesting to note that md5 actually does have a pretty big switch statement
in it, of 25 tags at least. Anyway, no major regression, so I'm happy.

---

Actually, the code generated for those 24 tags doesn't look right to me. Too
many references to the default target. May be worth walking through it to
double check.

Yeah, I'm pretty sure something is wrong. Case should be
 [0, 20], [18:, 19:], :

I expect:
 cmp x0, 18, but getting cmp x0, 19.

Found the issue. Off by one due to high interval being exclusive instead of
inclusive. Should be fixed now. Re-running fble-md5:

time yes | head -n 40000 | fble-md5: 53s ==> 54s ==> 53s.

Good.

---

For the fun of it, let me try another perf deep dive. I'm thinking next thing
to try will be moving profiling and error code out of line of the main code,
in the hopes it avoids spoiling the instruction cache and is easier to
branch predict. But first, a deep dive...

Flat Profile by Self Time
-------------------------
    %     self  block
 7.62     3714 _Run.0x28c3e078._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_[00
 6.44     3138  MarkRef[002e]
 6.04     2944  FbleRetainHeapObject[0022]
 6.00     2923  FbleThreadTailCall[003d]
 4.76     2321  FbleStrictValue[0039]
 4.58     2232  FbleThreadCall[0001]
 4.53     2206  FbleUnionValueTag[002f]
 3.93     1914  FbleReleaseHeapObject[0032]
 3.81     1857  FbleThreadSample[0048]
 3.46     1688  IncrGc.part.0[001b]
 3.39     1654  Refs[0043]

Let's see where that 7.62% is going to.s

perf annotate -n _Run.0x28c3e078._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_

3714 samples total.

* 398 of that in function preamble.
* 344 of that in function post-amble.
* bl to FbleThreadSample, loading from args array, checking error condition.

Hey, you know what's crazy? Perf annotate is showing .fble code. That's
awesome. Anyway...

It feels like the biggest hits are those that come after long jumps. And load
instructions. Maybe not surprising. Presumably it's all related to memory
operations and what's in the cache or not.

Is there a way to perf sample by cache miss event instead of frequency
sampling? I wonder if I can get an annotation for that and align it.

There are lots of possible events, maybe:
  cache-misses
  major-faults
  minor-faults
  page-faults

Let me try page-faults to start?

perf record -e page-faults -d ...

Hmm... It wants a frequency. I wonder what it's counting then?

Interesting. Looks like it works like I think. Instead of sampling on
frequency, it counts events.

All the page-faults are happening in malloc. That's not surprising. Let's try
cache-misses instead.

Most of those are in RetainValue and MarkRef. Not surprisingly I guess.

I'm not sure what to make of that though.

The frequency must be in terms of events, not time. So frequency of 500
means sample once every, uh... ??

Anyway, biggest thing in Ord is just that it's a function that's being called
a whole lot, and there is overhead to the function call.

So, either figure out a way to reduce that overhead, or call the function
less? Like, seems like function inlining could help a bunch here? Or a calling
convention to avoid having to save so many values to the stack?

Registers we use that we need to therefore save before using:
  heap, thread, locals, args, statics, profile_base_id, scratch_0, scratch_1

Hmm... What if I turned tail recursion into a while loop? No need to save and
restore the registers every iteration.

(Ordering@) { Ordering@; } RoundLt = (Ordering@ x) {
  x.?(eq: Lt, : x);
};

(Ordering@) { Ordering@; } RoundGt = (Ordering@ x) {
  x.?(eq: Gt, : x);
};

Ord@<IntP@> Ord = (IntP@ a, IntP@ b) {
  a.?(1: b.?(1: Eq, 2p0: Lt, 2p1: Lt),
      2p0: b.?(1: Gt, 2p0: Ord(a.2p0, b.2p0), 2p1: RoundLt(Ord(a.2p0, b.2p1))),
      2p1: b.?(1: Gt, 2p0: RoundGt(Ord(a.2p1, b.2p0)), 2p1: Ord(a.2p1, b.2p1)));
};

It's not all tail recursion. But could we write by removing function calls?
Like, if you were to write it in C?

Let's say we didn't have RoundLt or RoundGt. Then it's straight tail
recursion.

  while (true) {
    if a == 1:
      return b.?(1: Eq, 2p0: Lt, 2p1: Lt);
    if b == 1:
      return a.?(1: Eq, 2p0: Gt, 2p1: Gt);
    a = a.2px;
    b = b.2px;
  }

To add the rounding bit? Keep track of a function, and how things are applied.

By default: Id.  RoundGt turns eq to gt. Round lt turns eq to lt. So
composition looks like:

Id . RoundGt = RoundGt
RoundGt . Id = RoundGt
Id . RoundLt = RoundLt
RoundLt . Id = RoundLt

RoundGt . RoundLt = RoundLt
RoundLt . RoundGt = RoundGt

So, we can keep track like that, collapse things down. In other words, yes, I
could totally write this is a single function call that is a while loop.

That's much better than function inlining, because we avoid code duplication
and we get 'full' inlining in the sense of never having to call a function
once we are inside.

A couple bits here:
1. Transforming it to pure tail calls via clever manipulation of RoundGt,
RoundLt.
2. Given a pure tail call, that is, a tail call to the same function we are
in, you could jump past the pre-amble and skip the overhead of pre and post
ambles.

---

Some things to think about more generally:
* Replace per-instruction sampling with sampling on enter. Then bypass all of
  profiling with a branch instruction based on value of profile pointer.
* Move profiling and error code out of the main path.
* Can we have a special calling convention for run functions, given
  FbleThreadCall is the only place we ever invoke them?
  - With the idea that we change to all registers caller saved, and avoid any
    caller save registers in the run functions.

Let me read about calling conventions. Does gcc have a way to specify an
alternative calling convention to use? What are the options? Any thing
equivalent to "caller save all"?

Looks like intel has some different options, documented under "Function
Attributes": fastcall, thiscall. But I don't see anything for aarch64.

Oh well. Probably a bad idea to be that hacky anyway.

---

Moving 'DoAbort' code out of mainline path:

time fble-sat < rope_0002.shuffled.cnf: 1m36s ==> 1m35s
time yes | head -n 40000 | fble-md5: 53s ==> 51s

Very modest improvement, but I'll take it.

---

Turns out we weren't using R_SCRATCH_1 for anything. Let's see what we get
from removing that.

time fble-sat < rope_0002.shuffled.cnf: 1m35s ==> 1m33s
time yes | head -n 40000 | fble-md5: 51s ==> 51s

---

Anyway, clearly the next thing I want to do is consolidate and outline the
profiling calls. We're spending a lot of instructions on that, even when
profiling is disabled.

Questions:
* Can I move profiling calls out of FbleThreadCall?
* Can I do samples per block rather than per instruction?
 - Is there any case in which instructions belong to some specific profile
   block aren't always going to be executed once we enter that profile block?

Because if we have those, then we can move everything behind a flag. Use an
R_PROFILE to know if we should enable profiling or not, and if so, jump to a
separate profiling section of the code.

---

Let's not make this hard. First step: change profile sampling to be an
explicit profile op instead of done for each instruction. And because it's too
painful to me to add a sample profile op to each instruction, when I add
explicit sampling profile ops, coalesces them into as few as possible.

Let me review some code to see if I notice anything obvious about when it's
safe to group profiling things together.

* Instruction with profile enter op gets one sample.
* Straight line code can all be coalesced.
* We have a fair number of profile enter/exit, probably from let
  statements. We'll have to sample these separately.
* Note: some functions don't have any profile enter/exit, because those are
  taken care of by FbleThreadCall. 
* return statements shouldn't have profiling follow them.
* Might be nice to do sample as part of:
  - Explicit sample for function entrance.
  - Profile Enter.
  - Profile Replace.
* The 'goto' instruction following a union select branch after profile exit
  should be attributed to the block outside the branch. Assuming we care about
  counting that as part of the profile.

I think this is totally doable. We keep track of blocks. Any time we push a
block, we have an associated profiling instruction. Any an instruction,
increment the count of whatever the current block's profiling instruction is.

I'll need to restructure things a little bit to keep whatever pointers we want
as part of the stack, but otherwise, easy.

In summary:
* Add an FBLE_PROFILE_SAMPLE_OP.
* Add a count field to profile ops for SAMPLE, ENTER, REPLACE.
* Change interpretation of ENTER, REPLACE to always sample.
* Track profile op as part of block stack.
* Any time we append an instruction, increment the count of the top profile
  op.

Decide what I want to do in terms of sampling a block of instructions at once.
There is some math.

For example:

if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1

Is different from

if rand%1024 == 0 sample 5

Is different from
 
if rand%1024 < 5 sample 1

---

I claim we can associate all profiling ops with one of two places:
1. The beginning of a function.
2. The beginning of a union select branch body.

How about I switch to an explicit profile instruction, and emit one for each
of those places. Or... make it part of FbleCode and FbleUnionSelectInstr
directly?

It may be more general and easy to work with as an explicit profile
instruction that we emit.

---

In terms of 

if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1

versus

if rand%1024 == 0 sample 2

versus

if rand%1024 < 2 sample 1

The property we want ideally is that
  F(a) + F(b) = F(a + b).

We'll only get that with the first case.

But the expensive bit is the sampling, not the rand part. So I vote we do as
follows:

F(a):
  s = 0
  for (int i = 0; i < a; ++i)
    if rand%1024 == 0
      s++
  if s > 0
    sample s

Nice properties:
* Single sample per call of F.
* F is linear, so doesn't matter how we group things.

I guess the question is: can we come up with a decent appropriation of F that
is O(1) in the size of a instead of O(N) in the size of a?

---

Updated thoughts:
1. Make sampling explicit profile op.
 - So that compiler can optimize how profiling is done and no special cases
   are needed for that versus other profiling ops in the back ends.
2. Coalesce profiling ops where possible.
 - Start of function, start of branch, end of union select all we need?
 - Maintain order of ops. Don't reorder sample ops: doesn't seem worth the
   added effort even though sampling can be reordered in ways enter/exit
   can't.
3. Don't make a separate instruction for profile. There are some potential
   benefits to it: avoid the need for nop, avoid the need to check profiling
   at every instruction in the interpreter. But that feels too invasive to the
   program. Like, if you want to count number of instructions, that shouldn't
   change with profiling enabled/disabled.

Let me start with (1). I think auto-add a sample op every time we add an
instruction. That's the easiest way.

Interesting note: making sample ops explicit changes where the samples are
applied to, because the sample comes after other profile ops rather than
before. I think it's better this way.

---

For coalescing, we'll need to reset after function calls, because those have
profiling side effects.

---

Now that we coalesce profiling ops as much as I currently desire, let's take
advantage of that for code gen. In all three backends (interpreter, c,
aarch64), the idea is to jump over all profiling ops if profiling is disabled.

I would love if I could use R_PROFILE_BASE_ID to quickly detect if profiling
is enabled or not. We already pass that around everywhere. Normal value means
profiling is enabled. If profiling is disabled, we don't need that value at
all.

The question is, can I use 0 as the 'disabled' value? That would be super nice
for aarch64, because we could use cbz/cbnz to jump past, instead of needing
a cmp followed by a branch. Or, is there some magic instruction to test for -1
maybe?

Maybe TBNZ, TBZ, which test a bit of a register? Syntax is:

  tbz x20, #4, foo

To test bit 4 of reg x20 and jump to foo.

So, we could test for -1 by testing bit 63, right? Cool.

Now, what can profile_base_id be normally?

Uh, profile_block_offset is the new name?

I'm not sure this is okay. Because we need profile_block_offset to be non-zero
if we want to switch between profiling enabled/disabled with the same
allocated objects. Is that possible?

Actually, -1 may be safe to use. If profiling is disabled at link time... It's
subtle. Using -1 means any functions we allocate without profiling enabled
would be broken if you enable profiling. That sounds dangerous.

Can we just use thread->profile for the time being? Except that's internal
state. We don't have access to that.

How about: add an additional argument to the run function which is a bool to
say if profiling is enabled or not.

---

Now it's time to see what benefit we got. I expect performance improvements
across the board. I don't have all the baselines you could want, but the
sampling of what we have should be nice.

time fble-sat < rope_0002.shuffled.cnf: 1m33s ==> 1m26s
time yes | head -n 40000 | fble-md5: 51s ==> 45s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m39s ==> 0m35s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m41s ==> 0m38s
time fble-sat --profile -- < rope_0002.shuffled.cnf: 4m00 ==> 4m02

Reviewing ideal runtime without any profiling or error related code, that was
down to 1m19 seconds as opposed to 1m26s. Decent progress made.

---

Performance idea: better lifetime management.

If you access field y of value x, and you know that x will live at least as
long as y needs to, no need to do explicit release on y.

That should be easy to track in the compiler. I'm hoping we have lots of cases
where x is an argument to a function or a static, so we know it outlives y.

Some questions:
* Who retains y in the first place currently?
 - The implementation of the ACCESS instruction after getting the field via
   FbleStructValueAccess or FbleUnionValueAccess.
 - Good, this should be easy to change to an explicit retain.
* What percent of the time in programs is spent in Retain/Release and where?
 6.89     2831  FbleRetainHeapObject[0019]
 4.29     1762  FbleReleaseHeapObject[003a]
 3.18     1307  FbleReleaseValues[0040]
 2.20      904  FbleRetainValue[0044]
 Yeah, so that's a lot of opportunity to improve.
* How many of those Retain/Release could be avoid using the proposed rule?
 - Apparently all of FbleRetainValue comes from FbleThreadCall?
 - FbleRelease is much more what I was hoping. Lot's of calls from generated
   code.

Examples of generated code's calls to FbleRelease:
 Map modify!:
  
    Ordering@ ordering = ord(key, map.map.key);

  12.  l0 = a0.1;  @ pkgs/core/Core/Map/Map.fble:459:39
  13.  l1 = l0.0;  @ pkgs/core/Core/Map/Map.fble:459:43
  14.  release l0;
  15.  l0 = s0(a1, l1);  @ pkgs/core/Core/Map/Map.fble:459:26
  16.  release l1;

 In this case, 'map' is an argument. We get map.map, then map.map.key. First
 thing we do is release map.map. Then we call the ord function, then release
 map.map.key.

 A perfect example of unnecessary retain/release.

You want another example? /Md5/Rounds%.cls function I bet has an absurd number
of these, because it does things like a.hi.hi.hi.hi.hi.

Awesome. Let's try this. First, how does it look in terms of implementation?
1. Change ACCESS_INSTR to not retain what it accesses, add an explicit RETAIN
   instruction for that. Same with COPY instr. And CALL, and RETURN.
 - In other words, do all calls to RETAIN explicitly in instructions. So we
   can give the compiler control over them.
2. Add an OWNERSHIP field to Local
 - This points to a Local that will stay alive at least as long as this local
   and cause this local to be retained. The owner could be the local itself as
   a special case.
3. Add an OWNED field to Local.
 - This is a list of locals that have this local as its owner (except for
   itself).

Operations are:
* Initially, statics and args have self ownership and refcount 1.
* Access instruction gives ownership to what was accessed from.
* When refcount drops to zero:
  - If owned by someone else, remove that ownership. No explicit release.
    For each owned, transfer ownership of them to owner.
  - If owned by self, for each owned, explicitly retain and set owned by self.
    Then release this local.

Just to be clear, at this point I'm going to try extend the life of an owner
object beyond what it would otherwise be to avoid transfering ownership to an
owned object. That can be a future optimization.

This sounds entirely reasonable to implement, and looks promising in terms of
potential optimization. Let's do it.

Step 1: making release explicit.

---

Baseline:

time yes | head -n 60000 | fble-md5: 1m08s
time yes | head -n 40000 | fble-stdio -m /Md5/Main%: 1m10s
time fble-sat < rope_0002.shuffled.cnf: 1m21s
time fble-stdio -m /Sat/Main% < rope_0002.shuffled.cnf: 2m01s

---

Instructions not to implicitly do retain on:
* struct access - done
* union access - done
* call - done
* copy - done
* return - done

Let's do one at a time. Start with copy, which should be simplest I think?

Return is tricky, because we used to explicitly release on abort. Now it
doesn't make sense to release on abort, because we don't know that we need to.
But then who is supposed to release it?

I'll need to think more about this. Is it as simple as always releasing in
return? Yeah, that makes sense to me. We will already have explicitly retained
it otherwise.

---

How to pull out 'retain' from call instruction? What gets retained today:

* tail call retains func, args
  Note: func is the 'strict' function value, not necessarily the original?

Okay, this is interesting. Tail call retains things and releases things and
then does the call. If we retain things before hand, maybe we can release
things before hand too.

Non-tail call is straight forward. No retain/release involved.

Let's focus on tail call then:

* retain func.
* retain each argument.
* if func is a local variable, release the original value.
  - It's clear we need to release the original value.
  - Could we have retained the original value too?
* Release args that are local variables.

The point is, FbleThreadTailCall takes ownership of func and args.
For any non-local func and args, we need to retain.
For local func and args, we'll transfer ownership instead.

It's as easy as that.

Nope. Not as easy as that. Two things that make this hard:
1. The values for func, args might show up more than once. If so, we can only
use 'transfer' for one instance of that ownership. We need retains for the
rest. We need to keep track of that.

2. There's potentially a difference between the original func argument and the
strictified func argument passed to the tail call. So we can't just transfer
ownership for func to the tail call, because tail call is taking ownership of
the strict func value, not the original func value. We don't currently have
access to the strict func value outside of the tail call instruction.

(1) is straight forward, if slightly tedious, to support. (2) is less fun,
because it seems like we need an extra retain/release in the common case where
the original func value is the strict func value, just to handle the rare case
when it isn't. Or maybe it usually is... anyway, it's no fun to have to do an
extra retain/release in this case.

Options are:
* Keep the same retain/release calls we were. Then the tail call will have to
  be responsible for retaining/releasing the func. In other words: tail call
  instruction consumes the non-strict func if it's a local, transferring
  ownership of the strict func.
* Come up with some better way to structure tail calls that avoids this
  problem.

Let's try the first option for now, to keep things simple.

Say tail call instruction consumes the original func value. It assumes args
are properly retained. Does it check if it's a local variable though? Hmm...

Cases are:
* non-tail call: nothing to worry about.
* tail-call, non-local func: nothing to worry about.
  Takes ownership of strict func.
  args will have been retained.
* tail-call, local func: 
  Converts ownership of non-strict func to ownership of strict func.
  args will have been retained.

What's interesting is we don't need to access the func in the call instruction
for anything except detecting abort. In theory I could pass the non-strict
func to FbleThread*Call and have it transfer ownership to the strict func if
desired.

We already do this for things like FbleStructValueAccess, so it makes sense.

In other words, strictify the func value in the call instruction for error
reporting, but pass the original to tail call. We duplicate the
strictification at runtime, but the benefit is we put that logic in one place
instead of re-implementing it for all of the backends.

Under this new proposal:
* FbleThreadTailCall takes original func value as an argument. It still
  consumes.

So we have, for the tail call case:
* Compiler arrange for original func and args to be retained. 
  - static/arg retained.
  - local transferred.

That's it. Easy. I like it.

It would be nice if FbleThreadCall took the non-strict version too I feel
like. Not sure.

---

Cool. Retains are all explicit now. For the fun of it, let's see where we are
at performance wise. I expect some loss of performance due to reloading local
variables before retaining and calling FbleStrictValue in FbleThreadTailCall.
But some gain in performance for avoiding retain/release call for tail call on
already-strict local functions.

Honestly? I bet it's a minor hit to performance.


time yes | head -n 60000 | fble-md5: 1m08s ==> 1m15s
time yes | head -n 40000 | fble-stdio -m /Md5/Main%: 1m10s ==> 1m16s
time fble-sat < rope_0002.shuffled.cnf: 1m21s ==> 1m27s
time fble-stdio -m /Sat/Main% < rope_0002.shuffled.cnf: 2m01s ==> 2m09s

Hmm... Steeper than I had hoped. What could it have been again?
 - Extra call to FbleStrictValue in FbleThreadTailCall
 - Reloading values for FbleRetainValue instead of using what's in a reg
 - Increased number of instructions to execute - impacts interpreter.

What else could it have been?

For the first one, maybe we could inline or something to avoid the extra call
overhead?

For the second one, a general solution that stores some values in regs
directly could be quite nice.

We could also coalesce retain instructions like we do release instructions.

I still think it's good for retain instructions to be explicit. More
opportunity to optimize down the line, avoids duplicating complex logic in all
the backends.

---

Now that I think about it, maybe we could handle retain/release for tail calls
differently. If func/args are static, there should be no need to
retain/release them? Unless we are in a tail call that's going to go away...
No. Sounds too hard right now.

How about I run perf to see if it can narrow down the regression. 

It's hard to tell from the perf profiles.

---

I remember now that we did a sequence of changes to get to these numbers, not
just a single change. Let's break it down.

     baseline  ==> copy      ==> access    ==> return    ==> (tail) call
md5: 1m8.191s  ==> 1m8.563s  ==> 1m8.901s  ==> 1m11.641s ==> 1m15.184s
sat: 1m20.486s ==> 1m21.778s ==> 1m20.354s ==> 1m26.533s ==> 1m27.706s

The only change for 'return' was having to reload the value from memory. I bet
that explains all the significant regression here, and it's just that return
and call are called more often. This, in theory, could be improved by making
better use of registers between fble instructions. Except, doesn't that
increase the cost of saving/restoring registers between functions?

Let me think about this a little. Consider all the different kinds of
instructions we have:
  
DATA_TYPE, STRUCT_VALUE, UNION_VALUE, STRUCT_ACCESS, UNION_ACCESS, FUNC_VALUE,
CALL, COPY, REF_VALUE, TYPE, LIST, LITERAL:
  All of these leave the newly created value in a register naturally. We could
  arrange for this to be x0.

GOTO, NOP:
  These don't effect what's in any register.

UNION_SELECT, REF_DEF, RETAIN, RELEASE:
  These clobber whatever was in the register. But they don't necessarily have
  to. For example, RETAIN could arrange to leave the retained value in x0.
  That doesn't make sense for union select or release, because they don't
  produce any values that we expect to be consumed. How about ref_def? What do
  we tend to do after that? After ref_def, looks like we almost always release
  the original value (not the newly defined value).

Here's what I'm proposing. For aarch64 codegen, we arrange for everything
except UNION_SELECT and RELEASE to leave an interesting value in x0. While
emitting instructions, we keep track of what var is in x0. When we do
GetFrameVar, check if it's already in x0. If so, use that directly instead of
reloading from memory.

My claim is that this should recover all the performance lost from doing
explicit retain, and then some. And no additional overhead, because we aren't
using any more registers.

Let me try it.

One problem: if profiling is enabled, it would clobber this register. Maybe we
need a dedicated register to use for this instead of x0? Right now have I have
two callee saved registers to spare: x27 and x28. But I think this is worth
the optimization. So, yeah, let's try using one.

In theory I could know when we are using that register and avoid moving data
between registers unnecessarily, but let's be honest, moving data between
registers is probably free. Don't worry about that level of detail right now.

One concern: if we are jumping to different instructions, then rvar may not
represent the right value. Like, if we are the target of a jump, we may not
have come from the previous instruction.

That happens for branches, which are terminated by jump instructions or return
instructions. So make sure to clear rvar any time we change flow of control.

Cool. The code for this is pretty nice, and I see a fair bit of opportunity
for reusing this cached value more places. I'm optimistic this could make a
noticeable performance improvement.

time yes | head -n 60000 | fble-md5: 1m08s ==> 1m15s ==> 1m13s
time fble-sat < rope_0002.shuffled.cnf: 1m21s ==> 1m27s ==> 1m26s

Hmm... That's unfortunate. Not nearly the savings I was hoping for. Is there a
bug? Are we not applying it correctly or at all?

Oh. We aren't saving it in the case of non-local FbleRetainValue, right? Yeah.
Let's add that.

With that:

time yes | head -n 60000 | fble-md5: 1m08s ==> 1m15s ==> 1m13s ==> 1m11s
time fble-sat < rope_0002.shuffled.cnf: 1m21s ==> 1m27s ==> 1m26s ==> 1m26s

I don't understand. Where is that 6/7 seconds coming from for fble-sat?

I'm getting a bunch of things like:

   mov     x0, x19
   ldr     x1, [x23, #40]
   bl      4b0ca8 <FbleRetainValue>
   mov     x0, x19
   ldr     x1, [x23, #40]
   bl      4b0cc8 <FbleReleaseValue>

Where is that from? This is in the ord function. Why retain followed by
release of the exact same value? And why don't I see similar retain followed
by release in the fble assembly code for this module?

It's in abort code. That's fine. Doesn't explain the slowdown.

Could we explain the slowdown due to no longer batching calls to Retain value?
That's hard for me to believe.

I think I should reproduce the before/after 6 seconds for moving retain out of
return. Make sure I'm looking at the right code.

---

I can reproduce the 6 second slowdown from 

commit 36b18407487fe26e2519391c6fa2db38c4d4d00b
Author: Richard Uhler <ruhler@degralder.com>
Date:   Sat Apr 22 09:31:14 2023 -0700

    Don't do retain as part of FBLE_RETURN_INSTR.

The only thing that changed was generated code. So here's what I'm going to
do:

1. Run perf record. Figure out which generated code was effected the most.
2. Compare the generated assembly code for that generated code before and
after.

There are two cases: local return and non-local return. So do the comparison
for each case separately.

---

Here's an explanation for why saving the result from a previous instruction in
a register doesn't help much: that result will end up being pushed to and from
the stack anyway when we do a function call. So we don't actually save a load
instruction. We just moving from locals to the stack. Let's ditch that
attempted optimization.

I still want to understand those 6 seconds though.

From perf, we see big differences in FbleThreadTailCall and
FbleReleaseHeapObject. Interesting, since the code for those hasn't changed at
all and the number of times they are called surely hasn't changed at all.

The top generated code by runtime is IntP Ord!.

Let's compare code, just to make sure it hasn't changed.

FbleThreadTailCall:
* There's a ccmp instruction that gets 212 versus 1230 samples.
* There's a ldp instruction that gets 497 versus 2341 samples.

Looking at the ldp instruction, that's restoring register values from the
stack in preparation for returning. In this case x21, x22 from sp #32. The
ccmp instruction is also related to x21 it looks like, which holds the 'func'
argument to FbleThreadTailCall.
 
FbleReleaseHeapObject:
* Biggest differences is right at the start, reading the object argument and
  its refcount.

Difference in Ord!:

Pre:
       0 :   462074: ldr     x26, [x23]
      26 :   462078: mov     x0, x19
      19 :   46207c: mov     x1, x26
      19 :   462080: bl      4a3a18 <FbleRetainValue>
      52 :   462084: mov     x0, x26

Post
       1 :   462a6c: mov     x0, x19
      24 :   462a70: ldr     x1, [x23]
       0 :   462a74: bl      4a5ad0 <FbleRetainValue>
      54 :   462a78: ldr     x0, [x23]

That's it. All the differences in mainline code are that. There is some extra
outline code in post.

The difference, obviously, is that we reload x0 from [x23] instead of from
x26.

This must be due to cache misses, right? Can I do some perf record based on
cache misses and see the difference in counter values?

Or, if we look at the implementation of FbleRetainValue, maybe it doesn't use
x26, so that doesn't have to come from the stack at all. But maybe it uses
x27, which is why my rvar approach doesn't help? No. RetainValue doesn't use
any of those callee save registers.

So, must be due to cache misses, right?

And, in particular, it must be that FbleRetainValue primes the cache, the
load of [x23] pollutes the cache, and then the subsequent instruction,
whatever that may be, takes the hit because it has to refetch the object
pointer. What do you want to bet that next instruction is related to
FbleThreadTailCall or FbleReleaseHeapObject?

What follows retain/release is always going to be an exit instruction. In this
case we don't expect tail call. We call PopStackFrame, which releases 'func'.

perf shows a lot of cache misses. I don't know.

---

'perf stat' command looks interesting. Let me try that.

perf stat -d -d, there is one very interesting number that stands out:

pre:     303,332,602      L1-icache-load-misses:u   #    0.59% of all L1-icache accesses  (54.55%)
post:  1,000,996,703      L1-icache-load-misses:u   #    1.93% of all L1-icache accesses  (54.55%)

That's three times the instruction cache load misses.

What does that translate into in terms of seconds based on the stats?

pre:

    51,704,738,117      L1-icache-loads:u         #  625.275 M/sec                    (63.65%)

That's 51,704,738,117 / 625.275M = 82.82365642865724. Oh. It's derived from
math in the first place. So we don't know how much more expensive an
icache-load-miss is.

Can I perf record to see where these icache load misses are happening?

Maybe we can ask the other way: how many cycles does each icache miss have to
take to explain the increase in total cycles? Does that number look
reasonable? I would think L1 cache miss is what, on the order of 10 cycles?

Total increase in number of cycles:
  100,577,117,296 - 93,507,923,309 = 7,069,193,987 

Total increase in number of icache misses:
  999,347,721 - 315,915,793 = 683,431,928

Total number of cycles per icache miss, assuming icache miss explains all the
cycle increases: ~10 cycles.

Use perf to report where the difference happens, I see increase in icache
lining up directly with samples in FbleThreadTailCall.

I could believe the icache misses explain the 6 seconds. But why did we start
getting more icache misses?

The only place we add instructions is that Retain/Release thing in abort
functions.

For IntP Ord, check out pc 2 (retain) followed by pc 3 (release) of Ord!
function:
   2.  retain s0;
   3.  return s0;

This is new, because before we could optimize away the retain followed by
release as they were in a single return instruction.

Well, we can test this hypothesis pretty easily. Strip out the DoAbort code.
See what difference that makes.

No difference. I don't understand. Did we just get really unlucky and some
random change we made to the code is leading to an icache conflict on the
critical path that we didn't have before?

But that would throw all my experimental results in question, right? Any
improvement could have been due to the desired improvement, or we just got
lucky?

I give up. Maybe we'll get lucky again later on. Otherwise, don't worry about
it. Forge ahead with the original goal: avoiding calls to FbleRetain and
FbleRelease entirely.

