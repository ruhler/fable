Fble Profiling
==============
Revisiting fble profiling, based on the following:
* Can we eliminate the overhead of profiling when it is disabled?
* We could implement 'call' counts as another form of profiling time, rather
  than treat it separately. Every function call counts as 1. To find call to a
  particular function, look at its self time.
* Challenges with over-abstraction currently: Lots of calls of a->b->c and
  d->b->e will make it look like a calls e a lot, when in reality it never
  does.
* Recently added -s option to fble-perf-profile.

For example, imagine I change the implementation of profiling to collect the
set of samples, e.g. {(a->b->c, 1000), (d->b->e), 200), ...}. We could do
everything in post processing.

The set will be large. We can save memory by representing it as a tree
perhaps, assuming most samples share common prefixes. But we still have the
challenges of recursion and tail calls that can make it get too large.

Is there some clever way I can encode things to bound the size of the profile
data based on the size of the code rather than the duration of execution?

Let's start by thinking about recursion.

  a -> b -> b -> b -> c

Potentially b could be an unbounded length. Say 1000 calls. And say we do a
sample for every call. That leaves 1000 different sequences to store. Sadness.
But is it useful? Can we use some sort of abstraction to save space without
losing anything interesting?

The current implementation uses the idea that... well, there are two parts.
One is time spent calling a function, and one is time spent calling into a
function from another function. As soon as we call function 'b', no need to
add more calls to b on the stack, because it won't make a difference to the
final numbers we care about. As soon as we add a call a->b, no need to add
more calls a->b to the stack, because it won't make a difference to the final
numbers we care about. But now I say we care about more things than we did
before. Is there a happy medium?

In the example above, I'm fine to merge b->b into b. It doesn't add any more
info. For example, assuming we are counting calls for simplicity:

 a: 1
 a->b: 1
 a->b->b: 1
 a->b->b->b: 1
 a->b->b->b->c: 1

May as well be saved as:
 a: 1
 a->b: 3
 a->b->c: 1

We see that a is called once, b is called 3 times, c is called once. I guess
what we are missing here is that b is called three times, twice by b and once
by a. So really we need:

 a: 1
 a->b: 1
 a->b->b: 2
 a->b->b->c: 1

Where, in this case, b->b stands for a recursive cycle?

How about any time we see a recursive cycle, create a new name for the cycle
and use that.

a->b->b: here we see a cycle: b->b. We want to keep track of entrance into the
cycle separately from cycling into it. Let's say B = b->b:

a->b->B->c ?

Not sure.

---

Proposal for new profiling format and viewer.

We describe a profile as a tree of nodes. Each node is labeled with the
profiling block it corresponds to and a sample count.

We can convert the tree of nodes to a set of samples sequences. There is one
sequence per node, and that sequence occurs the number of times of the sample
count of the node.

For example, you might have something like:

 -> a 1 -> b 1 -> b 2 -> c 1

This corresponds to the set of samples:

 -> a 1
 -> a -> b 1
 -> a -> b -> b 2
 -> a -> b -> b -> c 1

To bound the size of the tree, we impose a restriction that X -> Y for some
particular X and Y does not appear more than once on any given sample path.
For example, the following sequence is disallowed:

   -> a -> b -> a -> b -> c

Instead we abstract it as:

   -> a -> b -> a
            \-> c

This abstraction is a loss of information for the purposes of keeping the
profiling data more compact. In theory we could pick any length sequence
X -> Y -> Z -> ... to use as the restriction. I think a 2 element sequence is
most useful in practice.

For example,

 a 1 -> b 1 -> b 1 -> b 1 -> c 1

If we used a single element sequence for the restrction, this would be:

 a 1 -> b 3 -> c 1

That suggests a calls b 3 times, and b never calls b. With a two element
restriction, we have:

 a 1 -> b 1 -> b 2 -> c 1

Which makes it clear a calls b one time, and b calls itself twice.

A three element restriction doesn't appear to give us any more useful
information:

 a 1 -> b 1 -> b 1 -> b 1 -> c 1

Though I imagine there is some sequence you could come up with where you lose
useful information with a 2 element restriction instead of a 3 element
restriction.

We put the restriction on each path of the tree rather than the tree as a
whole to avoid confusing sequences from different paths. For example:

  -> a -> b -> c
  -> e -> b -> c -> d

We don't want this to be abstracted as:

  -> a -> b -> c -> d
  -> e -> b ->/

Because that suggests there is some path from a to d when there is not.

The bounds on tree size aren't great. It's something like O(N^N), but I bet in
practice it will be much better than that because the majority of callsites
have a constant number of callees.

Okay, so the profile format is clear. Here's how we track it at runtime:

The profiling thread has two data structure:
* The current real callstack. Each entry is a pair of profile id and pointer
  into the profile tree of the location corresponding to this stack entry.
* The current profile tree.

We keep a current pointer into the current profile tree. Call it P.

Push X:
  Push X onto the stack.
  If P -> X exists on the path from the root to P, go to that X. Otherwise add
  '-> X' at the current place in the tree and go to X.

Pop:
  Pop from the real stack. Update P to point to whatever is now on the top of
  the real stack.

Replace X:
  Pop from the real stack. Then push like you normally would.

Sample:
  Increment current sample value of P.

The only tricky part is detecting if P -> X exists on the path from the root
to P. But we already have something like that. We don't switch paths randomly,
we always pop everything off one path before going to the next, so we should
be fine there.

The last part is how to present the information. I vote for a web server so
you can use browser as a UI. The variety of information  we may want to
present is too big to have a single static text file.

Each page has the following options:
* List of 'selected' profile blocks. Removes from the profile tree any paths
  that do not contain all of the selected profile blocks.
* List of 'excluded' profile blocks. Removes from the profile tree any paths
  that contain any of the excluded profile blocks.
* List of 'folded' profile blocks. Replaces any sequences of the form
  a -> X -> y for folded block X with a -> y (details need ironing out).

Those options all act as modifiers on the profiling data.

To view profiling data, we have the following pages:
* Self times: For each profiling block, reports the self time of the block.
  That is, the sum of sample values of nodes with that block id.
* Dominated times: For each profiling block, the sum of sample values for
  every path that contains that profile block.
* Specific profile block X: 
   Going in: For each other block Y, counts number of samples with Y -> X.
   That's how much time you get back if you remove Y -> X.
   Going out: For each other block Y, counts number of samples with X -> Y.
   That's how much time you get back if you remove X -> Y.

For any profile block X mentioned on any page, make it easy to:
* Jump to the specific profile block X
* Add/remove X to/from 'selected', 'excluded' and 'folded' block options.

Make it easy to jump to self time pages and dominated time pages.

Have a list of currently selected, excluded, folded blocks to make it obvious
what they are looking at and be able to remove those blocks from those lists.

That's it!

Oh, and consider having different kinds of profiles: call count, time, allocs.
Whatever counter you want. Those will require different implementations at
runtime. It might be easiest if a profile tracked only one thing at a time.
Produce separate profiles for separate metrics? Not sure.

---

Development plan:
1. Settle on a file format for recording the profile.
2. Generate a properly formatted perf based profile.
3. Write a viewer. See if it's more useful or not. Iterate as needed.
4. Update the profiling implementation in fble to generate a profile.

For (1), ideally use an existing standard format. If not, I guess make my own.
Some key questions:
* Should it support multiple metrics or just one?
* How to store profiling block info: file, line number, etc.

Before I go down that path, I want to review some of the challenges I have
with the existing profiles and see if we have a path to solve them here.

1. Dispatch function overabstraction.

-> a -> Call -> b
-> c -> Call -> d

In the profile ends up with:

  a,c -> Call -> b,d

Makes it look like a calls d or c call b, when that's not true.

Example:

      59194   451688           /Fbld/Eval%.Eval!.sequence[084eX]   
         90   451988           /Fbld/Builtin/Define%.Define!!![090fX]   
       7166   451990           /Fbld/Builtin/Define%.Let!!![0921X]   
          1   453490           /Fbld/Main%.Main!!!.:.result[09d6X]   
**  5687121   453490    11226  /Fbld/Result%.Do![0497X] **

**  5687121   453490    16573  /Fbld/Result%.Do!.:.rb[049aX] **
          1   453490           /Fbld/Main%.Main!!!.:.result![09d7X]   
       7166   451990           /Fbld/Builtin/Define%.Let!![0920X]   
         45   451988           /Fbld/Builtin/Define%.Define!!![090fX]   
      59194   451688           /Fbld/Eval%.Eval!.sequence![084fX]   

I have two potential solutions here with my new format:
1. fold 'Do', and all the in between helpers.
2. If we start looking at Eval!.sequence!, then go to Do!., then add a filter
Do!. -> Eval!.sequence!, then you should see only the inputs Eval!.sequence
into Do! when you look at it.

This is slightly different from before, where we said we would limit all
traces to, say, ones containing Eval!.sequence!. Now I'm saying as I explore
up (or down?) from a node, add the path being followed as a filter.

2. Continuation/Dispatch functions showing up high in list.
For example:

**     7166   451990       28  /Fbld/Builtin/Define%.Let![091fX] **

So much time is spent in Let! because after we call Let!, the next thing it
does is evaluate its body, and everything is in the body. Let! has no
influence at all over what is executed in the body, so it's not useful to
associate the body time with the function Let!.

I almost want to set: only charge Let! for code under its control, not the
'Eval(env, Get(cmd.args, 1))' part. Just because so much of the rest of the program is
stuffed into body Eval part.

I'm not convinced we can figure out in general when we care about the
subroutine call and when not. For example, List.Length, the cost of the tail
entirely depends on the structure of the list argument. Clearly we want to
count that towards the cost of List.Length. But how is Let evaluation any
different? Depending on the structure of its argument, it has a different
tail.

Maybe the challenge here is the annoying structure of the fbld evaluation code
where Eval calls into everything and everything calls into Eval. It's a big
recursive knot that is difficult to untangle.

Any solutions for this? I wonder if we could group the recursive cycle into a
knot and then subdivide it into its child parts?

3. Recursive functions.

        226      148           /Fbld/Builtin/Define%.PreLookup!.command.:.:.impl.:[0902X]   
    3125991   101562           /Fbld/Env%.Lookup!.:.:[0469X]   
     981307   134898           /Fbld/Eval%.Eval!.command.impl.:[084bX]   
**  4107524   135046     4028  /Fbld/Env%.Lookup![0465X] **
    4107414   134081           /Fbld/Env%.Lookup!.:[0467X]   
        110        0           /Fbld/Env%.Lookup!.nil[0466X]   

Lookup calls into itself. Looking at entries like this don't feel terribly
useful. Self time is 4028. So where is all that 134081 going to? I guess in
this case we can follow it down and see it's the call to /Core/List/Eq%.Eq!,
which is outside of the recursive loop. Once again, it feels like we want to
separate 'recursive loop' from 'children of recursive loop'. Maybe that's okay
for now, it's just a bit tedious to traverse the graph and manually separate
out which parts are in the recursive loop and which parts are not.

4. Excessive traversal.

**        3   526454        0  <root>[0000X] **
          1   526384           /Core/Monad/State%.Monad![0976X]   
          1       70           <main>[0001X]   
          1        0           /Core/Stdio/IO%.Run![09a2X]   

**   282549   526384      615  /Core/Monad/State%.Monad![0976X] **
      65573   526384           /Core/Monad/State%.Monad![0976X]   
          1   505499           /Fbld/Main%.Main!!![09d2X]   
     216976    22757           /Core/Monad/State%.Monad!.ra[0977X]   

**        1   505499        0  /Fbld/Main%.Main!!![09d2X] **
          1   505499           /Fbld/Main%.Main!!!.:[09d5X]   

**        1   505499        0  /Fbld/Main%.Main!!!.:[09d5X] **
          1   503627           /Fbld/Main%.Main!!!.:.result[09d6X]   

**        1   503627        0  /Fbld/Main%.Main!!!.:.result[09d6X] **
          1   453490           /Fbld/Result%.Do![0497X]   
          1    50137           /Fbld/Parse%.Parse![080fX]   

As I traverse down, it's commonly the case that the majority of time is spent
in a particular child call. Maybe it would be nice to merge those all
together. For example, in this case, something like merging
/Fbld/Main%.Main!!! and /Fbld/Main%.Main!!!. into a single frame, so we don't
have to click through stuff we don't care about.

I think we could get pretty close if we pick something like 95% time, and
accumulate that as we go down. We could let the user pick the percentage to
join on.

Sounds like overall there's plenty to explore in the UI for viewing profiles.
Good news is I think the new profile format preserves all the information we
need for playing around with these things, unlike the current profiling
format.

---

On to profiling format. 

What I want:
* Describe a tree whose values are block id, sample count(s).
* Include for each block id, name, file, line number, column number.

Worst case, we could have a list of sample sequences instead of a tree. Just
seems like the tree is the more efficient way to store the data.

Survey of existing formats:
Linux Perf:
  tools/perf/Documentation/perf.data-file-format.txt
  It's very specific to the linux kernel. We could probably use it, but feels
  rather out of the way.

gperftools:
  https://gperftools.github.io/gperftools/cpuprofile-fileformat.html
  Basically a list of (sample count, list of PCs)
  Followed by a text map, which maps PCs to build number and filename. Looks
  like addrtoline can then be used separately to extract line number info.

Python Fil profiler:
 
Python cProfile:
  Has not compatibility or stability guarantees. So don't use this.

google/pprof:
  profile.proto format
  Uses proto3 syntax.
  Supports multiple custom metrics.
  Looks like it supports exactly the information we want:
  - multiple metrics per sample.
  - sample as a list of location ids.
  - separate section for giving info about location ids, including file and
    line and column number.
  Already has support for converting linux perf to pprof format.
  
I'm tempted to use google/pprof format, except that it's a bit complicated in
depending on things like proto3 and gzip. So it could be harder to write a
standalone tool for it.

Maybe I can use textproto format instead of binary? Or some other format pprof
supports?

Looks like proto3 has a way to convert between proto3 and json file format?

It looks like the proto3 wire format is fairly straight forward though. So
perhaps fine for me to write to it directly. There's just a lot of layers
involved: binary -> protoscope -> proto3 -> pprof.

I vote we eventually support pprof format, but to start, if we want, I can
define my own simpler format. We can provide a library in my format to
save/load as whatever syntax I want to start, and pprof eventually in the end.
In other words: for now write a library for my profiling format. Worry about
the file format later. But let's keep pprof format it mind for question's
about things like multiple metrics.

Cool. Next step is to draft the data structure / library for representing my
profiles in C. I can add code to load from linux perf directly to start rather
than have a separate file syntax to go through.

---

Re-hashing the same algorithm I think I described above for compressing and
computing traces:

We build a tree of locations with the property that no forward path from root
to leaf in the tree has the same location twice. Nodes are allows to have
back edges to ancestors in the tree. The children of a node are sorted in
order they were first encountered.

Start at a special root node. When you enter a new location:
* If a child already exists for that location, go to the existing child.
* If that location already exists as an ancestor, there is exactly 1 ancestor
  with the same location. Add a back edge to that ancestor and go there.
* Otherwise create a new child at this node to the new location.

That tracks our current location. I guess we'll need some way to 'pop' back as
well when leaving stack frames. That's doable.

A sample is listed as an unrolled version of the tree which you get starting
at the root, following first children... Hmm... Maybe there are multiple
backedges possible. Maybe we can track the sample as we go in a single path
separately. We might need that for popping the stack too? Sounds hard.

But my previous attempt to explain the algorithm appears to have a way to deal
with that. It stores the sample counts in the tree. I'm a little bit worried
about how to extract the samples from that though.

Anyway, I would love to have fble profiling output pprof or similar format, so
long as we have a decent way to view it. Let me try pprof viewer, see if it
works well enough for my use case. If so, we can update fble profiling first.
If not, I should write my own pprof viewer first.

---

I give up on trying to use pprof. The install dependency stack is too high:
pprof ==> perf_data_converter ==> bazel ==> ???

Let me write my own viewer first. If I'm going to write my own viewer and not
have pprof to play around with, let me use my own format to start. Something
that should be easily convertible to and from pprof later.

The plan to use pprof suggests we should collapse paths rather than keep my
internal tree representation. That could lead to duplication, but don't worry
about it.

So, basic data structure is:
* List of samples. Each sample is a list of metric values and a list of
  location ids.
* Mapping from location id to location info: name, file, line, column

Let's say location id is an integer into the location mapping table.
Let's say we have a text format to start with, for ease of debug during
development. Somewhere we need to define the metrics.

Let's think about how I'll deal with metrics. Time is a thing. What about
count? Yeah, that should be easy too. To start, we can track both and put them
in. I like the idea of not treating count specially. But that means we
probably want to define the name of the metric in my output file. Maybe:

First line is a whitespace separated list of metric names.
Or use ';' for the separator character? Or ','?

metrics: count,time

How human readable do I want this to be?

A sample is: List of metric values followed by list of location ids, all space
separated? Or list of metric values followed by list of location ids, where
the location ids are comma separated. I like space separated for metric values
to simplify sorting by metric value.

3 5 x,y,... 

Finally we have the location definitions:

x: name file:line:col


So, for example:

metrics: count,time
3 5 0000X,0001X
0000X: <root> :0:0
0001X: <main> ../lib/link.c:23:12
0002X: /SpecTests/Nat% ../spec/SpecTests/Builtin.fble.@:4:10
...

How do we know when samples start and locations end? Should we put locations
first?

Maybe the first line is:

<num_locations> <metric1> <metric2> ...
loc1: ...
loc2: ...
...
sample1
sample2
...

But it would be nice to be a little more human navigable. And I'm used to
having locations at the end. Locations at the start might be nicer for a tool?
Not sure.

m: ... ...           A list of metric names.
l: ... ...           A location definition.
s: ... ... l,l,...   A sample definition.

Metrics is a whitespace separated list of metrics names.
Location is whitespace separated: id name file:line:col
Sample is whitespace, where last element is comma separated locations, first
elements are metrics.

Easy to parse. Okay to read. Has all the info we care about to start.

I would like to start with perf profiles, to take generation of the profiles
out of the picture. Perf profiles don't need to worry about recursion stuff to
start. I can assume they don't do any abstraction of that sort and not worry
about it. If we want, I can add a pass to do that kind of abstraction in the
viewer and see what we see.

Okay? Then plan is:
1. Write converter from perf raw data to my format.
Actually, to start, we can just read perf raw data directly into the internal
data structures for the profile viewer. Which means no need to worry about a
custom format to start, right?

2. Implement viewer for internal format.

Internal format is:
List of metric names.
List of sample traces.
Hash map from location id to name, file.

Do I want to start with an html viewer, or there's something I can do with
text to start? Anything to get feature parity with current text output?

Coverage is easy: % of locations referenced in some sample.

Flat profile... needs to be by count and by time? Like, a flat profile for
each different metric?

Locations is easy.

How to do the call graph?

I want to avoid having a whole bunch of magic options you have to remember to
specify to get something meaningful. That's why a website is nicer. It shows
you all the options you can explore from.

Whatever I end up with will be based on a set of queries. Maybe the output of
a query is formatted for text or html. Maybe a page is a collection of
queries. Let's think about the queries first.

* Locations: returns the list of locations in the trace.
* Coverage: Returns list of uncovered blocks. And/or percent of blocks
  covered.
* Flat profile - lists locations sorted by some value.
  The value assigned to a location could be self or overall, and any
  combination of the metrics. Maybe we want to show multiple metric values.
  Maybe we want to sort and subsort various ways.

Ugh, I'm spoiled by SQL now. Flat profile request could be described in SQL
pretty easily, right?
  
How about call graph? Is call graph like a variation on a flat profile? For
example:
 * Value is time we called into a location from a particular node.

What if I came up with my own query language, a subset of SQL? The start by
letting you issue queries on the command line, it outputs the result of your
query. Everything is based around individual locations to start. What options
do we have?

"Columns":
 - Metric values: metric * {self, overall}
 - Location info: id, name, file
  
Once I have a table with those columns, we could do standard SQL operations on
them: select the columns of interest, allow some math, filter rows.

Let's put aside that part of the problem then. That is, given a table with
metric values and location info, one row per location, how to present the data
to the user.

That leaves us with: given a profile, how to generate a table. Sample things
to generate:
* The global table.
  Gives us:
  - Flat Profile by Overall Time
  - Flat Profile by Self Time
  - Block Locations
  - Coverage

How do we do a call graph entry? I want a way to say something like:

Do a count over samples containing A -> x, for given location A. Do it for all
locations x. Then our filter is a sequence of locations with a single hole
where some location would be.

We could think of it as, given some sequence A -> B, compute the self and
retained metric info for that sequence (not sure what 'self' means in this
context though?). Then have a loop to produce the table, for each location x,
compute value for A -> x.

What does self time mean for a sequence A -> B? Self time means when the trace
ends with A -> B. That's meaningful. If the trace doesn't end with A -> B, it
counts towards overall time, not self time.

What kind of filters can we do with this? From before, the kind I want are:
* fold: remove location A from all samples. More general, remove a sequence.
* include: remove samples not containing the given sequence.
* exclude: remove samples containing the given sequence.

How does call stacks make sense here? I want to look at all traces A -> x. I
need a sequence with a hole for that. This is independent of folding. We want
folding to happen first. It gives you 'include', but maybe you want include
done separately.

I'm thinking three different phases of the query:
1. Profile manipulation. Given a list of sequences to fold, include and
exclude, gives you a different set of samples to consider.

2. Location search. Given a single sequence containing a hole, produces a flat
table by location with self and overall time for all metrics. Self time counts
for those samples that end with the given sequence. Overall time is for those
samples that contain the given sequence.

3. Display query. Use a SQL query like expression to select what parts of the
table in (2) to show in what order.

A useful UI will guide us in selecting what queries to execute next. Which
unfortunately complicates things a little bit in terms of presentation. But I
think I can come up with a general solution. For example, you start with (1).
Your next query is going to be a single step away from (1). And it's
optionally going to be modified by a specific location selected to based on
the row in (2). And you can modify the display query in (3).

Profile format is well defined. Profile manipulation is well defined. Location
search is well defined, except I'm not sure how to specify the hole. Display
query I'm not sure how much we want to support. I'm thinking:
* select the set and order of columns (metrics or location info). 
* choose which column to sort by.
* filter out or in certain values?

But there's the question of whether we want to sort or filter by arbitrary
expressions, and if so what kinds of expressions are supported. And there's
the question of things like grouping. Maybe skip grouping for now. And what
about columns making up your own expression values?

Anyway, what this all means for my implementation is, I think, that I can get
started writing some functions and test cases:
* Representation of a profile.
* Apply a fold, exclude, or include on a profile to get a new profile.
* Apply a location search on a profile to get a table.

Once I have that as part of the library, I ought to be able to write fble code
to generate a profile report just like I have today when running -profile.
That puts me at parity with the fble profile report. Which means I can
transition over to re-implementing fble profiling to output in my new proposed
format.

---

How to implement location search?
* Use a special LocId for the hole? For example, empty string.
* Allow the special location to appear multiple times in the sequence?
  I can't think of a use case, but it seems easier to support than worry about
  handling the case reasonably otherwise.

Hmm... In theory, a single sample could match multiple different locations. I
feel like to start maybe just do a single subsequence search at a time?

For example, consider:
  a -> b -> a -> c

I search for: a -> *. That is, I want to count how much time we would save by
removing calls to location X from a. In this case, this sample shows how much
we would save by removing b, and how much we would save by removing c.

That simplifies the interface. Don't worry about performance to start. I can
worry about that plenty soon enough later.

So, what we want is: given a subsequence, compute the self and overall metric
values from the samples in the profile. Easy, no?

What do I call this? /Pprof/Profile/Summarize? Before I called it a location
search. That doesn't quite fit now. How about Count?

---

I have Count implemented. Can I start to write queries now and see how it
looks?

I really ought to start writing pure tests to give me confidence things are
working. I should be able to reuse test cases from the existing fble profile
tests.

    // <root> -> 1 -> 2 -> 3
    //                  -> 4
    //             -> 3

10: 1 
20: 1,2
30: 1,2,3
40: 1,2,4
31: 1,3



@(Tests);
