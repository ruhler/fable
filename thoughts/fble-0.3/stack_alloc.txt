Stack Allocate
==============
Exploring whether we can do stack allocations to improve performance of fble.

The idea stems from the problem of function allocation.

The following allocates an intermediate function object:

  foo(a)(b)

The following does not:

  foo(a, b)

One way to think about the difference is in the second case we effectively
allocate an intermediate function object, but we do so on the stack as a pair
(foo, a). Thus we never need to allocate a heap object, so we don't incur the
costs of malloc or GC.

Can we somehow enable stack allocations for foo(a)(b) so we don't have to pay
the extra cost there too? And in particular in more complicated cases such as
monadic bind where the application of argument a is separate from the
application of argument b in the code.

Review of memory management. There are a few different categories of kinds of
objects:
* Packed values.
 No need to manage, we copy everywhere.
* Explicitly owned reference
 There is one clear owner of an object. The memory for the object can be
 reclaimed when the owner is done with it.
* Reference counting
 There is no single owner of an object. The memory for the object can be
 reclaimed when all the owners are done with it.
* Cycles
 There are reference cycles in the object without a clear owner. Typically we
 need more advanced GC to handle this case.

Currently in fble we treat everything as either packed values or potentially
having reference cycles. We don't take advantage of explicit ownership or
reference counting opportunities.

Memory management is made hard by references. Without references, everything
would be passed by value, you make copies.

References work as follows:
* A packed value 'ref' copied around everywhere.
* An allocated object 'obj' that stays put.
* The lifetime of 'obj' must be at least as long as the lifetime of 'ref'.
* The lifetime of 'obj' should not be significantly longer than all of the
  references to it.

The reference requirements are ensured by the current gc algorithm. If you
take a new reference to an object, that causes the obj to stay alive at least
as long as the reference. Once all the references to an object are gone, the
object will (eventually) be freed.

Consider a function. It has:
* Input arguments, guaranteed to stay alive longer than the function.
* Output, required to stay alive longer than the function.
* Local variables
 - If not captured by the output, don't need to stay alive longer than the
   function.

A key question about an object is: is it (potentially) captured by the output.
If the answer is no, I claim it could be stack allocated and automatically
freed when the function exits.

Let's pretend we have some memory heap associated with every function call,
and we can dynamically allocate as much memory to that heap as we want. We can
label these heaps based on position in the stack. Say the label is the number
of stack frames below this stack frame, so that inner calls have a higher
number id than outer calls.

If you have two heaps with ids x and y, if x > y, then the lifetime of all
objects in y is greater than the lifetime of all objects in x.

An object in heap x can freely refer to an object in frame y without
additional tracking. An object in heap x can refer to an object in frame x,
assuming we free everything in frame x all at once. An object in frame y
cannot safely refer to an object in frame x, because the reference in y would
outlive the object in x.

Let's say, when a function is called, it is provided the heap where the result
of the function should be allocated. Maybe it also needs to know the heap
where the arguments are allocated.

If there aren't reference cycles, and some object a references some other
object b, we can say the heap id of a >= the heap id of b.

In a function, when an object is allocated, we ask if there is a possibility
of the object being captured by the output. If yes, allocate it to the heap of
the output object. If no, allocate it to the current function's heap.

When calling a function, if the result of the callee is possibly captured by
the output of the caller, then pass the caller output heap as the callee
output heap. Otherwise pass the caller heap as the callee output heap.

We have also the case where the output of a function captures the inputs to
the function. If we already knew this, then the inputs to the function would
be allocated in heap that lives as long as the output to the function already.

Now the question is: is this enough to avoid memory leaks? In particular, I
worry about the 'possibility of capture' case. What if there is a small
possibility of capture, so we end up allocating a lot of garbage to an upper
frame?

Or, could we do accurate dynamic tracking?

For example, have a notion of virtual heap, still associated with a stack
frame. Virtual to make it so we can easy move an object from one heap to
another.

An object is allocated in the current heap. If there is a reference taken from
an object in y to an object in x, and x >= y, then the object in x is moved to
y, along with any other x references that are in heaps >= y. When we return
from a function, we move the return value to the caller's heap.

I claim this will be too expensive. Think about the case when allocating a
long linked list. Every time we return, we have to iterate over the entire
list to move all the objects. We need it to be constant time to return a value
to the caller.

That suggests we need to live with "possibility of capture" to allocate
objects to a safe heap to start with.

We haven't talked about tail recursion yet.

Say we are in heap 5. The result will go to heap 4. We have some local
variables in heap 5 that are being passed to the tail call. In theory, we need
to keep the local variables captured by the tail call and drop all the other
variables. Otherwise we'll have a memory leak. Perhaps functions that do tail
calls should have two heaps: the heap of objects potentially captured by the
tail call, and the heap of objects not captured by the tail call.

Can I come up with an example where potentially captured leads to a memory
leak? Almost surely. For example:

(Int@) { Int@; } F = (Int@ x) {
  Int@ y = decr(x);
  IsZero(y).?(true: x);
  F(y);
};

Contrived, yes. The variable x should only be captured at the end, but it has
the possibility of being captured every time. We end up allocating every value
on the outer caller's heap, thus using O(N) memory instead of O(1).

Can we flip the condition around and at least get some benefit? For example,
if I'm certain that an object isn't captured by the output, then we can
allocate it on the function heap. If that means we reduce the number of
objects participating in gc heap, that's great, right?

So, say for each object we can say: I guarantee this object will not live
longer than 'x'. If an object x references an object y, where x >= y, then we
don't need to track the reference?

Or maybe we keep track of a range of lifetimes. The lifetime for an object is
somewhere in the range [a, b]. It will not live longer than a. It will live
for at least as long as b.

Say an object [a, b] takes a reference to an object [m, n]. What happens? The
lifetime of the object [a, b] does not change (assuming no cycles for the
moment). How does the object [m, n] change?

If a >= n, no change. This means the [m, n] object will always live long
enough.

I guess it would be: [min(a, m), max(b, n)], right? It will not live longer
than both the objects. It may live as long as either of the objects. But,
again, if you change the value on the object [m, n], you have to change the
value of every other object it references recursively, which is too costly.

If an object has a single owner, we could say the lifetime of the object is
the lifetime of its owner. An object can only get a second owner through its
original owner.

If an object has multiple owners, and one owner is guaranteed to live longer
than all the other owners, then the object essentially has a single owner
still: the owner to live the longest.

Perhaps owners of an object could coordinate. Say I have object A with owner
X. Then, via X, we take a second reference to A from object Y. If X knows
about Y, and Y knows about X, then they could coordinate. Whoever is first to
be freed tells the other they have been freed and they are transferring
ownership. That's basically reference counting...

Makes you wonder if it's worth trying to go back to a reference counting based
GC. The language is simpler now. No links anymore. The only cycles will be
through RefValues.

---

Let me assume the high cost of memory management today is because objects are
allocated using malloc and an associated call into the gc. That suggests
switching to a reference count based approach will not help: we would still be
using malloc for objects.

Idea for allocating things on the stack:

Assume we reorganize values so that if you know the type of a value, you know
its shallow size. And assume we worry about polymorphic functions later or
have a way to know concrete types when they are invoked.

Have a function return its value as a copy. So the caller allocates space for
the return value on the stack, the callee fills in that space. That space
could contain references to other objects. Those other objects could be
allocated on the heap by the callee, allocated on the stack by the caller, or
allocated elsewhere with a lifetime that will outlast this function call.

Assume callee allocated objects transfer a reference to the caller as part of
this.

For local variables that would have been released when the function exits, the
function should go through and release any callee allocated parts of it. No
need to do anything else. Then we have successfully managed to allocate the
value on the stack and avoid malloc/free/gc for it.

Otherwise the object is returned to the caller of the function. For each local
and child allocated object referenced, heap allocate those as new objects and
copy that all over to the caller. We've successfully managed to allocate the
value on the stack and avoid malloc/free/gc for it, but we may have had to do
heap allocation for some things it referenced.

I claim this is good enough to allow stack allocation of the function value in
the monadic example stdout(byte).

To make this work, we need an easy way for the function to tell if an object
is:
a. heap allocated by a child.
b. locally allocated.
c. owned by the caller (either via the stack or heap).

How can we take a reference to an object owned by the caller from a child heap
allocated object if the object owned by the caller was stack allocated?

---

Important observation: Some things need to be allocated on the heap, even if
their lifetime is tied to the stack. For example, say we allocate a long list.
Because it's a long list, we can't allocate it on the stack. Then we use that
list as a local variable from some function f. The lifetime of the list is
clearly bounded by the call to f.

This means there are cases where heap allocated objects can safely refer to
stack allocated objects.

I think it's pretty clear conceptually how things should work. It's just
figuring out the details. Local variables should be allocated on the stack if
they aren't captured.

The details in question are:
* Can we allocate returned variables on the stack?
* Do we detect what's captured statically? Dynamically?
* Do we have a way to heap allocate objects that aren't gc'd?
* Will some combination of the above solve the fble-cat allocation case?

Next step, I think, should be to walk through the code for the simplified
fble-cat example. Understand what allocations we could stack allocate with
what knowledge and whether all the functions could be stack allocated.

---

Deep dive into monadic fble-cat code.

Function allocations are:

11. l1 = s2(l3);     stdout_(char)
13. l3 = l0(l1);     Do(stdout_)
15. l0 = func;       (Unit@ _) -> MCat
17. return l3(l0)

Let's start with the first one.

   0.  l0 = func /Core/Stdio/FastCat%.Main!.stdout_!![0017] [s0, a0];
   1.  return l0;

This is an anonymous function. We capture l3 as a static variable. So,
conceptually this is the pair (stdout, char). This is definitely captured by
the result of stdout_(char). To allocate this on the stack, we want to support
allocation of return results on the stack. If we did that, we end up with
(stdout, char) on the stack. No need for a heap allocation.

Note: we'll need to figure out how to allocate space for a function on the
stack when it could require an arbitrary number of static variables.

Next for do. Again, we allocate space for the result on the stack. The do
function at this point literally just captures the argument and returns a
function. So, in this case, we return (do!, (stdout, char)). No reason that
couldn't be allocated on the caller stack directly.

Except: are these functions potentially captured by the return result? I know
nothing about l3 statically without type information or some kind of inlining.
f(l0) could potentially capture l0 for arbitrary f.

Let's try working backwards. What is l3?

It's the result of calling do. A function that captures stdout(out). The
result of that will be another function that captures l0. So l3(l0) captures
l0 for sure. It captures stdout(out) too. So here is a case where the function
we want to return references other local variables. We would need to allocate
all of these local variables on the callers stack. Or copy them over to the
callers stack.

The resulting function is then passed to m.do(stdin).

---

Lots of thoughts on this. No major breakthroughs yet. Some highlights:
* We could allocate everything on the stack maybe, if we are willing to deep
  copy the result from the callee to the caller.
* Current GC is almost like copying the entire heap every time we finish a
  round of GC.
* How will we manage the API for external references to FbleValue* if we
  allocate everything on the stack?

It seems like the incremental improvement approach would be to try to
conservatively allocate values on the stack. But that won't eliminate the
function allocations in the cat example.

The aggressive improvement approach would be to try to do away with GC almost
entirely. Allocate everything on the stack. I don't have a good answer for the
pathological case of constructing a list though.


---

Did another round of thought on GC in performance.txt. It suggests stack based
allocation is worth trying, if only to get the following:
* Avoid having short lived objects put pressure on the stack after they are
  done being used.
* Avoid smashing the cache to free short lived objects.
* Avoid repeatedly smashing the cache when traversing long lived objects.

There are many different ways I could go about this. I fear getting bogged
down in details. How about we start like this:

* What is the easiest way we could adjust the heap allocator to have the
  properties above? Implement that and see what impact it has on performance.

Easiest way to start would be if we could restrict all the changes to heap.c.
Keep the same API for allocating objects and interacting with the heap. How
would that look?

Say we keep allocated objects in a linked list, maintaining the order they are
allocated in. To allocate a new object, you put it at the end of the list.

Say we magically make it so that new objects can refer to old objects, but old
objects can't refer to new objects.

When we release a heap object, if it's the last object in the list, we can
free it.

Could we traverse the heap from the back always?

---

Let's try generational gc. See gen_gc.txt for discussion.

In theory we now have all the following:
* Avoid having short lived objects put pressure on the stack after they are
  done being used.
* Avoid smashing the cache to free short lived objects.
* Avoid repeatedly smashing the cache when traversing long lived objects.

I'm not sure how to verify. It only helped performance a tiny bit though.
Like, 3% performance improvement.

Actually, turns out it was a bug in generational GC. With the fix, we now see
18% improvement and better memory behavior. I think we have achieved the goals
above.

Let's give memory management a break for a little while since the switch to
generational GC. Focus on partial application or other approaches to avoid or
reduce the cost of function allocations rather than a rework of overall memory
allocation for now.

---

We are again back to the high cost of allocations. Specifically a struct,
union, and function allocation required as part of the Result@ monad. Every
statement of Result@ monad requires an allocation.

I really wish we could allocate these directly on the stack. I think that
would make it much cheaper.

<@ A@>(Result@<A@>)<@ B@>((A@) { Result@<B@>; }) { Result@<B@>; }
Do = <@ A@>(Result@<A@> ra)<@ B@>((A@) { Result@<B@>; } f) {
  ra.value.?(nothing: Raise<B@>(ra));

  Result@<B@> rb = f(ra.value.just);
  Result@(Append(ra.errors, rb.errors), Or(ra.failed, rb.failed), rb.value);
};

Look at rb here. We know rb is a struct. We know that we only access the
individual fields of rb, after that we are done. From the Do function's point
of view, rb is short lived. Though we don't know if anyone else is using it
for other reasons.

In practice, that function f is likely to be Do itself, allocating a brand new
result.

Let's say we could return values in three different ways:
1. packed.
2. stack allocated.
3. heap allocated.

We always prefer packed over stack over heap.

To stack allocate, we are specifically talking about results of functions. The
caller should allocate space on the stack for the presumably shallow
allocation of the object. The callee knows about that and fills it in on the
stack if possible.

We just need to work out when to do which kind of allocation and how to manage
references.

---

Three approaches for returning objects on the stack:
1. Return the entire object on the stack.
2. Return the shallow object on the stack, anything else it references on the
heap.
3. Return up to a fixed amount of memory for the object on the stack, anything
else on the heap.

I want to try (1) to start, because it's so far in a different direction. With
(1), everything is allocated on the stack. There is no heap.

The big concern with (1) is the pathological case of building a large object,
like a linked list. Because we copy back every value return, and we are
copying N values back O(N) times, that's O(N^2) runtime to allocate an O(N)
list.

Heap allocations are expensive to allocate and cheap to move around. Stack
allocations are hopefully cheap to allocate but expensive to move around.

For (1), we need a managed stack, because standard calling conventions have no
way for a callee to return an unbounded amount of data on the stack to the
caller.

One idea that may make it easier is to have two stacks that we alternate
between for every call. If A calls B calls C calls D calls E calls F:

First stack: A C E
Second stack: B D F

This way, for example, when F is returning, it can return directly to the end
of E's stack frame without worrying about clobbering things. Otherwise it may
have to worry about clobbering itself. That is, compacting is easier if you
are compacting to a different memory region.

The API for the new proposed value heap:

FbleNewValueHeap, FbleFreeValueHeap
  As before, except Free cleans up any remaining objects.
FbleNewStructValue, FbleNewFuncValue, FbleNewUnionValue - as before.

void FbleValueHeapCall();
  Sets up a new stack frame on the heap, recording the current position.

FbleValue* FbleValueHeapReturn(FbleValue* return);
  Returns to the previous stack frame on the heap. Frees every object
  allocated on the current stack frame that is not reachable from the return
  value. Moves everything reachable from the return value allocated on the
  current stack frame back to the previous stack frame. Returns the new
  address to use for the returned value.

To manage objects in the C API, it will have to manage the stack as if it was
a bunch of stack frames. The easiest would be have everything it uses in the
first stack frame, which stays alive until FbleFreeValueHeap is called.
FbleEval can allocate a new stack frame to start so its only returning what it
uses.

We can still have packed values if we want. RefValues are not allowed to span
across stack frames, but that should come naturally. No need for
retain/free/addref.

Open question: how to properly free function values? Any way we can avoid the
need to free function values? Seems like that would make things easier in
general. Perhaps during compacting we iterate through all the allocated
objects (they should be next to each other on the heap) and free the ones not
moved. We could keep functions separately so its easier to iterate through
just those.

This approach sounds doable to me. I'm not convinced it's viable though,
because of these two things:
1. The pathological O(N^2) runtime to construct a N element list.
2. The need to iterate through all functions to clean up their executables.

Is it worth trying? Or should we find a solution to (1) and (2) first?

To address (1), we would want to limit to a constant amount of data returned
to the caller. Put the rest on the heap. If we do that, then we are basically
going to original approach (3). No need for a managed stack anymore. We
allocate a fixed amount of space for the callee to return values to. Anything
that doesn't fit in that fixed space we allocate on the heap.

The hope is this efficiently handles the case of small, short lived
allocations that are slowing us down with monadic code today. I fear it makes
inefficient use of stack space. All that potential space that won't be reused.

Honestly, I think we should try (1). Give it a chance to see its full
potential before worrying about how to address the bad cases. See how close it
gets to the current approach and if it solves any problems we are having
currently.

---

Here's an idea for how to avoid iterating through all the objects to clean
them up. Change FuncValue to directly store num_args, num_statics,
tail_call_buffer_size, profile_block_id, and run functions. Then we no longer
need to do anything on free of functions.

To support destructors, define a new internal value type NativeValue which is
a wrapper around a native object. NativeValue participates in GC, it's
basically raw data and an on_free function. Allocate things like file
descriptors and FbleExecutable (for interpreter) in NativeValue objects that
belong to statics of a function value. The run function can assume those
objects exist and have the desired native type.

The hope is that in normal execution, we very rarely have to deal with
NativeValue objects. Add some extra data structure for NativeValue objects,
such as an embedded doubly linked list, that makes it easy to traverse just
the native objects for on free.

I'm feeling good about this approach. We could implement it with the current
heap garbage collector if we wanted to. I'm not sure it gets us much. I
suppose the main downside is we can't share num args, num statics, tail call
buffer size, profile block id and run functions. So, if lots of functions are
instantiated with the same parameters, we end up with duplicate data. Perhaps
memory for that is expensive. On the other hand, when do you expect a bunch of
different functions to have the same executable? It would be functions
allocated by other functions. Maybe it's not that common.

---

First question: how to implement the stack data structure?

A. Use the existing native stack.

I'm not sure this is legit, but, for example, maybe we use alloca to allocate
on the callee stack. Then return the resulting pointer to the caller. The
caller used alloca to keep track of its local allocations. To access the
returned data from the callee, call alloca with the needed size to get access
to whatever was on the callee stack when it returned.

Downsides:
* It's pretty hacky. Probably not portable.

Assuming the hacks work, we could compact the stack as follows:
* The callee returns the value from its stack without any compaction.
* Immediately on return, the caller uses alloca to capture the callee's stack.
* We call a Compact function giving it a pointer to the original caller stack
  and the new caller stack. It can copy over everything.
* Somehow we need a way to realloc to free up the unused space from the
  callee. Hmm.. Sounds tricky.

B. Implement a single continuous address space.

This sounds nice, because:
* it's cheap to allocate and free: increment and decrement a pointer. 
* We can easily check if a pointer is in the compacting region of the stack or
  not, so traversal is easy. Just do pointer comparison.

How can we get a single continuous address space?
* Pass a fixed size heap when we create FbleValueHeap. 
  Sounds reasonable in theory. We could pick a default large heap.
* Use something like MAP_GROWSDOWN?
  Is that portable? How are you supposed to use that?
* mmap on demand and hope you can get continuous regions?

Some experiments to try:
1. What does the virtual address space look like for a mostly empty c program?
2. What does the virtual address space look like for, say, fbld while it's running?
3. What's the largest single region I can successfully mmap in one attempt?

(1):

55800b0000-55800c2000 r-xp  /home/richard/scratch/stack/main
55bab88000-55baba9000 rw-p  [heap]
7fa96b1000-7faa0b1000 rw-p  My 10MB mmap.
7faa0b1000-7faa21e000 ***p  /usr/lib/aarch64-linux-gnu/libc-2.28.so
7fde589000-7fde5aa000 rw-p  [stack]

(2):
  0bb34000-  10e9b000 rw-p  [heap]
7fd93ca000-7fda670000 rw-p  [stack]

(3):

Somewhere between 512MB and 1GB. I wonder if it's trying to reserve the space.

Let's try with MAP_NORESERVE now.

That gives us somewhere between 256GB and 512GB.

Cool. So with MAP_NORESERVE, we can get plenty of virtual address space for
the stack. Looks like in my case it allocates memory at the end of the
available region.

Any way to pick a decent default value for the heap size? I guess I could do
what I'm doing now: keeping trying 2x until we fail to allocate that much
virtual memory.

I suppose a more portable way to do this would be to malloc 2x until we fail,
use the last successful size as the stack.

Using malloc, the biggest allocation I get is 512MB.

If I search more precisely, the biggest allocation I get is around 1009MB.

I don't see any need to use mmap myself. Malloc a big region sounds fine to
me. We can search for what's available. Or take a fixed size.

---

Random different idea: what if we implement a copying collector instead of
mark sweep?

The stack allocation approach has already abandoned the idea of incremental
garbage collection. We know we care about maximum memory. So how about
something simple like this:

Start with some reasonable smallish heap size. Say 1MB. When we fill up that
1MB, then we do a big copy traversal of the entire heap.

A couple problems with this:
* We need to update all external references to use the new pointers. We can't
  do that as easily when there are external references beyond the one we are
  returning.
* How/when do we grow the heap? It's only after we copy that we know how much
  space we need for it.

Maybe we say at 1MB, copy over. If the result is less than 512KB, fine. If the
result is more than 512KB, then the next time we allocate a new region,
allocate it as a 2MB region?

In other words, we have heap size (after compaction), heap max (allocated
space). We set the next heap max based on heap size after compaction.

The nice thing about this approach is:
* It should be really fast when we aren't compacting.
* It's based specifically around the idea of minimizing the max memory usage.

Downsides:
* It doesn't work well with caching. Short lived objects will go out of cache.
* Big stop the world GC events.

---

Anyway, back to previously proposed stack allocation approach. We can use
malloc to allocate a single large enough region of memory up front. Allocation
is bumping a pointer. On 'enter', save the current stack pointer so we can
jump back to it. On traverse, we can use the pointer values to check what is
included in the traversal and what is down the stack.

---

Time to try implementing my stack allocation approach for real. The hope is we
remove all the time for IncrGc, malloc, free, ReleaseValue, and AddRef. We
replace it with time to traverse objects when returning. There's no doubt the
traversal will be expensive. But how will it compare to what we save? That's
what we want to find out.

Use md5sum as a best case example. I don't expect any large objects returned
there.

Where to start? Implement the heap API. Which will be:

FbleNewValueHeap, FbleFreeValueHeap,
FbleValueHeapCall, FbleValueHeapReturn
FbleNewStructValue, etc.

No need for FbleRetain/FbleRelease. No need for FbleValueAddRef. Start by
assuming we can make one big allocation for the heap space, if that's easier.
No obligation.

There are three major parts to this:
1. Implementing the new heap API
2. Switching to the new heap API
3. Cleaning up now unnecessary retain/release.

(3) we can do last. Just implement them as no-ops to start. That will be fun
cleanup/performance optimization.

High level sketch of implementation:
* Value is vtag + data
  - struct value: vtag + fieldc + fields
  - union value: vtag + tag + arg
  - func value is vtag + function + statics
  - ref value is vtag + value
  - native value is vtag + data + on_free
* Two stack regions, alternating back and forth for frames
* Each region has:
  - Pointer to the top, for new allocations.
  - Pointer to the base -> which should form a linked list of stack frames.
* We keep track of the current 'to' and 'from' regions for allocation.
* New heap: allocates and initializes the regions.
* Free heap: frees the regions.
* New value: increments the top of the current region and returns the previous
  top. All values are now properly aligned, so no worry about alignment here.
* Call: swap to/from regions, push a new frame onto the new to region.
* Return: Traverse/Copy everything on the top frame of the from region to the
  too region. Pop the top frame. Call native destructors on everything in the
  top frame being popped.

Let's say the 'base' of a stack frame points to a single pointer which is the
base of the previous frame on that stack. That's how we store the linked list
of frames.

That leaves two tricky parts:
* Keeping track of native values whose destructors need to be called.
  Ideally make this really fast assuming no native values, and reasonably fast
  assuming some native values.
* Implementing the traverse/copy logic.

For the traversal, implement a recursive function that, given a value, copies
it over to the new space and returns the new pointer. Case:
* The value is in an old space. Return it.
* The value hasn't been seen yet.
  Allocate space for it on the new stack. Set its type on the 'from' space to
  COPIED or some such and store a pointer to where it was copied to.
  Recursively copy over all the fields.
* The value has been seen. It's marked COPIED. Return the saved pointer for
  that value.

Easy.

For native values, keep them in a singly linked list. Use a field of the
native which isn't one that will be overwritten by the 'COPIED' value. After
the traversal, walk through the linked list. Skip over any values marked
'COPIED'. Call on_free on the rest.

That's it. Easy.

---

I drafted the code for implementing the new value heap. It's all very straight
forward and nice. Should be low overhead, if only it weren't for the bad
complexity of traversing the entire result every time we return from a call.

---

What to call my Call/Return, Push/Pop functions? Let's brainstorm.

FbleValueHeapCall, FbleValueHeapReturn
FblePushValueFrame, FblePopValueFrame
FblePushFrame, FblePopFrame
FbleValueHeapPush, FbleValueHeapPop
FbleNewFrame, FbleFreeFrame
FbleFramePush, FbleFrameReturn
FblePushValueFrame, FbleReturnValueFrame

I like Push/Pop. I like Frame. How to link that to the heap and values?

FblePushHeapFrame, FblePopHeapFrame.
FblePushStackFrame, FblePopStackFrame.
FbleNewFrame, FbleReleaseFrame

FbleNewStackFrame
FbleNewDataStackFrame

I like New too. Maybe better than 'Push'.

FbleNewHeapFrame, FbleReleaseHeapFrame
FbleNewValueFrame, FblePopValueFrame
FblePushValueFrame, FblePopValueFrame

FbleCallFrame, FbleReturnFromFrame

I like Return.

FbleNewHeapFrame, FbleExitHeapFrame
FbleNewHeapFrame, FbleReleaseHeapFrame

FbleReturnValue - This is good.

FblePushFrame, FbleReturnValue

Let's go with: FblePushFrame, FblePopFrame.

---

How to implement tail call?

I can bundle up the function and arguments into a struct value so it gets
copied back to the caller frame appropriately. But then I'll want to add more
args to the back of it.

Cases when we add num_unused to the back:
* If it's a tail call. Always that case.

How about we make num_unused and unused available for FbleTailCall, it can
bundle in those automatically? I kind of like that idea.

I think it's worth defining a special internal ThunkValue for this, so it's
easier to access the fields. We don't have to worry about packing. We can have
separate accessors for the function and args.

---

Trouble with tail call. We can't return the func and args to the caller frame,
because that memory will not end up getting reclaimed if we are in a loop.
Every subsequent tail call we do will add more and more to the caller frame
leaking memory.

We need the func and args for the tail call to end up on the callee's frame.
How do we manage that?

The other thing to figure out: what convention should I use for who calls
FbleFramePush/FbleFramePop when, particularly when dealing with native
functions? I think if I call some function FbleFoo(...) that returns an
FbleValue*, it should be the job of FbleFoo to do FbleFramePush and
FbleFramePop internally. Yeah. That's consistent and reasonable.

How to handle tail call then?

======= caller
======= callee

Seems like we want a way to traverse/compact in place. If executable->run
returns tail call, it returns the function and args to tail call still on the
callee frame (we could pass them via gTailCallData still if we want to).

What I want, then, is a function like:

void FbleCompactFrame(FbleValueHeap* heap, size_t count, FbleValue** save);

It frees everything in the frame except things reachable from those values
listed in save. It updates the pointers listed in save. If we had this, then
the implementation is straight forward I think. Do just like we have now,
except run FbleCompactFrame just before doing the tail call.

How could I implement this? We can't assume the frame is going to be small.
Some of the args could have been allocated on the frame and be really big.

Brute force way would be to double compact. Compact from A to B. Compact back
from B to A. That's two traversals instead of one. It's probably doable to
start, but not much fun.

For example, push a new frame, 'return' from the caller frame to the new
frame, then push a new caller frame, return from the previous new frame to the
new new frame. We don't have to write any new code for this, aside from the
wrapper to update save values.

Could we instead flip the polarity of the two stacks?

Say our stacks are A and B.

Say X calls Y and Y tail calls Z.

A: X
B: Y

When Y tail calls into Z, we 'pop' from B to a new frame in A.

A: X, Z
B: 

Now two things can happen.
1. We do another tail call. Z to W say.

A: X,
B: W

We end up exactly where we want to be.

2. We do a return from Z. Now we are in trouble again, because we can't return
in place.

What if we had 3 stacks, A, B, C?

X calls Y

A: X
B: Y
C:

Y tail calls Z

A: X
B: 
C: Z

Z tail calls W

A: X
B: W
C: 

Z returns to X

A: X
B:
C:

I think that works. We just need to keep track of which secondary stack is
active. Tail call swaps the active secondary stack.

We'll need to record where to return to. I'm sure it's doable. Just need to
work out the details.

Okay, easy.

Heap is a Frame* current.

Frame stores a Frame* caller and a Frame* alternative.

PushFrame:
  Pick either current->caller or current->alternative for the new current.
  The old current becomes caller, the unchosen becomes alternate.
  (In the future we could maybe pick based on free space for better memory use?)

PopFrame:
  current becomes current->caller

CompactFrame:
  current becomes current->alternate. It's caller is old current->caller and
  alternate is old current.
  
Sounds like a plan. How will we make use of this for tail calls now?

FbleTailCall does exactly what it does today. Copy func and args to
gTailCallData and return TailCallSentinel.

When we see TailCallSentinel, we add unused to gTailCallData like today. Then
we call CompactHeap, passing a pointer to gTailCallData.

The only thing is, I think we should store func and args next to each other on
gTailCallData and avoid storing the FbleFunction* pointer so that we can call
CompactFrame and have it work.

Okay? Let's try it.

---

More guidance on when to use FbleFramePush: Any native function that allocates
new FbleValues that are not returned directly to the caller should probably
call FbleFramePush/FbleFramePop to avoid leaking those allocations.

---

Code is drafted. Let's see if it's in a working state.

Some bugs:
* fble-mem-tests are hitting an assertion in FbleNativeValueData.
* StackSmash is clobbering a func value.

Yeah, so seems like we are clobbering memory somewhere. Let's see. Start with
the first failure.

---

Progress. We can run fble-md5 now. There are still bugs. fbld-md5 is clearly
leaking memory. That said, we have some numbers:

User time (seconds): 47.01
Maximum resident set size (kbytes): 30500

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 47.01
  Maximum resident set size (kbytes): 2516 => 30500

This is hopefully just the starting point to improve performance, because we
haven't optimized out calls to Retain/Release yet and we still have the memory
leak. So, decent start?

We need to track down the memory leak for sure.

Let's see if I can reproduce the memory leak in a simpler test case.

Yes. This leaks a lot:

  @ Unit@ = *();
  Unit@ Unit = Unit@();

  @ Bool@ = +(Unit@ true, Unit@ false);
  Bool@ True = Bool@(true: Unit);
  Bool@ False = Bool@(true: Unit);

  (Bool@) { Unit@; } Toggle = (Bool@ x) {
    Bool@ y = x.?(true: False, false: True);
    Toggle(y);
  };

  Toggle(True);

I've messed up management of the three stacks. In FbleCompactFrame, we somehow
need to reset the popped frame.

---

Problem fixed now.

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 47.01 ==> 47.22
  Maximum resident set size (kbytes): 2516 => 30500 ==> 2176

Let's see where else we are at.

---

* fbld is really slow. That's not surprising.
* memory growth tests fail. That's not surprising.
* fbld-tests gets an assert failure. That's surprising. Any other assert
  failures?

All the other spec tests seem to pass. Hmm... It's probably worth checking.

Next steps:
* Try to clean up / disable known failing tests so its easier to see if we
  break something.
* Try cleaning up Release values calls, see what kind of performance boost we
  get in md5.

---

Initial linux perf review of md5.

Before:
 4.19     7001  FbleReleaseValues[0015X]
 2.43     4057  _int_free[001fX]
 2.31     3857  IncrGc[001eX]
 2.21     3694  FbleReleaseHeapObject[0016X]
 1.71     2855  malloc[0029X]
 1.28     2138  FbleRetainValue[001cX]
 1.12     1864  FbleHeapObjectAddRef[0031X]
 1.01     1683  cfree@GLIBC_2.17[002fX]
 0.98     1641  Refs[002eX]
 0.93     1550  FbleRetainHeapObject[0018X]
 0.90     1504  FbleNewHeapObject[001dX]
 0.69     1155  FbleAllocRaw[0028X]

After:
 4.72     9262  Traverse.part.0[0026X]
 4.58     8988  FblePopFrame[0014X]
 3.54     6950  FblePushFrame[0017X]
 2.38     4668  FbleCompactFrame[0013X]
 1.41     2774  __memcpy_generic[001bX]
 0.31      601  FbleRetainValue[0034X]

Oh well. What did you expect?

---

Currently failing tests:
* ./pkgs/fbld/fbld-tests
* SpecTests/Test/MemoryGrowth.fble
* pkgs/sat/tests, compiled & interpreted in Sat.Dimacs test.
* fbld generated values.

My guess is just the two issues right now: the known issue with memory growth
test, and some issue causing problems for fbld-tests and sat tests.

Let's start with memory tests. Even before the stack allocation change
tracking number of FbleAllocs wasn't enough to catch leaks due to stack
growth. I want to try max RSS instead. It may be flaky, depending on how much
memory pressure the system is under. But assuming not too much memory
pressure, maybe it's good enough?

The first test is to see if memory-growth test works. Another good test would
be to see if the memory leak in the stack I fixed before gets detected.

The concern is whether we can observe the memory growth in the noise of
everything else involved during the memory test. Namely:

1. Compilation, initial evaluation of the function.
2. Allocation of the large value n.
3. Running the small test.
4. Running the large test.

Let's see some numbers for how it goes today. How do we get RSS/Max RSS for a
process?

getrusage has ru_maxrss and a few other memory related ones. Let's see what
those look like. Looks like just ru_maxrss actually.

MemoryGrowth sees 1704 RSS from start to finish. Do we need bigger n?

---

Through the power of git rebase, we now have ru_maxrss based memory tests.
Let's see if it's working now.

Yes, works now. It's just really slow because, well, O(n^2) traversal behavior
when allocating larger data structures.

---

Let's see what we can do about the sat test failure. I rather work with the
interpreter on this one I think.

I minimized the test a little. We are doing Reverse, ForEach in ParseLines. We
have a List@ whose 'cons' is showing up as a UnionValue instead of a
StructValue.

I should be able to step through the ForEach code to see how that list value
is constructed and trace when it gets corrupted.

First question:
* Is the list corrupted at the start of the call to Reverse?
Yes. It's already corrupted by then. That's going to make it harder to track
down what's wrong.

Maybe I can step through construction of the list and see where it goes bad.

---

Some logging shows an issue here:

pop
u 0x7fc7e4b348 -> 0x7fd7e47f20
s 2 0x7fc7e4b328 -> 0x7fd7e47f38
s 2 0x7fc7e4b308 -> 0x7fd7e47f58
n 0x7
n 0x7fd7e47f78
u 0x7fc7e4b2f0 -> 0x7fd7e47f78
n 0x7fd7e47e60

Look at 0x7fd7e47f78. The 'n' means we don't touch it because it's out of
range. But then right away we allocate it as a new value. That almost
certainly clobbers the original value, right? It shouldn't be possible to see
an address and have it be in the new allocation. The new allocation region has
to be empty.

Yeah. That explains the abort I'm getting. 0x7fd7e47f78 was the original list
pointer, but we over-allocated it as a new union value.

Next step is to figure out how we could ever see something in a region that is
supposedly all free. Add some more debug logs to print the frame regions when
we push/pop/compact. That should help to see what's going on. In particular,
when we reallocated this region that was apparently in use.

After modifying the algorithm slightly, now we have:

pop
u 0x7fc7e4b320 -> 0x7fd7e47ef8
s 2 0x7fc7e4b300 -> 0x7fd7e47f10
s 2 0x7fc7e4b2e0 -> 0x7fd7e47f30
n 0x7
n 0x7fd7e47f50
u 0x7fc7e4b2c8 -> 0x7fd7e47f50
n 0x7fd7e47e48

So question is, going into this pop, how could 0x7fd7e47f50 be both allocated
and free? Which one is correct?

Just before, we did:
pop 0x7fd7e47ef8 - 0x7fd7e47f88

That should have removed all references to 0x7fd7e47f50. How could we still
have a reference to it?

We compacted to 0x7fd7e47ef8 - 0x7fd7e47f88, then popped that range. The
popped value is 0x7fc7e4b178.

Hmm... I need to track allocations I think.

Here's a more complete sequence.

push 0x7fc7e4b2a8
push 0x7fd7e47ef8
push 0x7fe7e480d8
push 0x7fc7e4b2c8
pop 0x7fc7e4b2c8 - 0x7fc7e4b2e0
u 0x7fc7e4b2c8 -> 0x7fe7e480f8
n 0x7fd7e47e48
compact 0x7fe7e480d8 - 0x7fe7e48110
n 0x7fc7e4a360
u 0x7fe7e480f8 -> 0x7fc7e4b2c8
n 0x7fd7e47e48
s 2 0x7fe7e480d8 -> 0x7fc7e4b2e0
n 0x7
n 0x7fd7e47f50
 ==> 0x7fc7e4b2c8 - 0x7fc7e4b300
pop 0x7fc7e4b2c8 - 0x7fc7e4b338
u 0x7fc7e4b320 -> 0x7fd7e47ef8
s 2 0x7fc7e4b300 -> 0x7fd7e47f10
s 2 0x7fc7e4b2e0 -> 0x7fd7e47f30
n 0x7
n 0x7fd7e47f50
u 0x7fc7e4b2c8 -> 0x7fd7e47f50
n 0x7fd7e47e48

It looks to me like an issue here:

alloc 0x7fd7e47f50
u 0x7fc7e4b398 -> 0x7fd7e47f50
alloc 0x7fd7e47f68
s 2 0x7fc7e4b3b0 -> 0x7fd7e47f68
n 0x6316eecc88bb33
n 0x7
 ==> 0x7fd7e47ef8 - 0x7fd7e47f88
pop 0x7fd7e47ef8 - 0x7fd7e47f88
n 0x7fc7e4b178

I'm pretty sure we should be returning 0x7fd7e47f50 as part of this pop, but
we don't see it in the traversal. Later on we get a pointer to it somehow, but
because it was popped, we allocate on top of it.

Oh, look at that. 0x7fc7e4b178 is a ref value. Yeah. That's a problem. We
traverse the ref value, but fail to see through it to the value allocated on
the stack.

alloc 0x7fc7e4b178
r 0x7fd7e4da88 -> 0x7fc7e4b178

I was kind of assuming we would assign ref values in the same stack frame as
the value we assign to. That way we would traverse into their value? I'm not
sure. Think about it some more.

---

Question: How could a ref value in 0x7fc7e4b... be assigned a value allocated
higher up on the stack?

Okay, I'm close to decoding this.

Do tail calls 
  f(r.parsed.result)(r.parsed.state);

f is  /Sat/Dimacs%.ParseClauses!.:!!, with static ParseClauses
result is Unit.
state is our state with formula as a list of clauses.

We call the ParseClauses!.:!! function, which returns ParseClauses. We have
not yet applied the state argument, but we pop the frame anyway. When popping
the frame, we destroy the state, but it's saved as an unused argument. We
later go to apply the unused argument, but it's garbage. I think that's the
problem.

Here it is:

    } else if (num_unused > 0) {
      FbleValue* new_func = FblePopFrame(heap, result);
      return FbleCall(heap, profile, new_func, num_unused, unused);

Note the call to FblePopFrame too soon.

---

I tried writing a regression test for this, but I can't figure out how to
reproduce it in a small test.

---

With the fix, all fble tests pass now. It takes 4 hours to build everything,
but it all passes.

Next phase:
* Clean up retain/release calls, which are no longer needed.

After removing calls to FbleRetain/FbleRelease:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 47.22 ==> 45.03
  Maximum resident set size (kbytes): 2516 => 2176 ==> 2108

Cool. There's still more cleanup to do. Then it's time to work on this
pathological case. I have ideas.

---

All the cleanup is done now.

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 47.22 ==> 45.03 ==> 44.81
  Maximum resident set size (kbytes): 2516 => 2176 ==> 2108 ==> 2116

Now on to performance.

There are a couple high level ideas to improve performance. I'm sure it's
dominated by Traverse. We should be able to verify very quickly running fbld
with linux perf.

Yeah:

Flat Profile by Self Time
-------------------------
    %     self  block
99.97    17963  Traverse.part.0[0001X]
 0.03        5  0xffffffd436e103cc[000aX]
 
How can we avoid so much traversal?

1. Merge stack frames.
For example, if we call a => b => c => d, instead of traversing when we go d
to c, c to b, and b to a, merge the stack frames for a,b and c,d. Traverse
only when we go c to b.

Variations: Merge frames until a certain number of allocations have been done.
We could limit overhead of potential garbage this way.

2. Make use of GC for some objects.
The extreme case is implementing the current API with every object GC
allocated. I feel like we should be able to get performance equivalent to
before stack alloc, but with all the nice cleanup of the new API.

The less extreme case is somehow identifying objects appropriate to allocate
on the heap and track them separately.

3. Allocate directly to caller.
Is there some way we could know where returned values should be allocated to
start? For example, in a function, I know if I'm allocating something I'm
going to return. Have the caller pass the frame where I should allocate to
directly.

4. ???

---

There are too many different ideas to try out. Let's pick one and run with it.
Specifically, let's go back to a GC based approach with the new API. I'm
hopeful that will be fast enough to let us merge the recent changes with the
main line and continue iterating from there.

How does a GC based approach work?

At a high level:
* Objects are 'owned' by their stack frame.
* When you return from a frame, you transfer ownership of returned objects to
  the new frame.

For performance to work out, we need to avoid doing allocation for new frames.
Ideally dropping a frame is constant time, regardless of how many objects were
allocated on it (not including time in incremental GC).

This means:
* We don't have to explicitly release each object on a frame when dropping the
  frame.
* We have immediate access to the next frame when dropping a frame.

We will need to drop references to objects at some point. Better to do it
sooner because it's more likely to be in the cache sooner than later.

Straw 1:
* Objects form a linked list. Each new object on a frame keeps the next one on
  the frame alive and so on.
* The 'top' object on a frame also points to the next frame.

This adds 16 bytes overhead to objects, in addition to the GC overhead.

Straw 2:
* For each frame we track two representative objects.
  1. An object whose 'next' is the next frame, forming a linked list of
     frames.
  2. An object whose 'next' is the next object in the frame, forming a linked
     list of objects in the frame.

Object (1) does not participate in the linked list of object (2).

Now we only need 8 bytes overhead per object, which sounds reasonable to me.

But where do we keep track of these two objects? That's the trouble. We don't
have a place for that.

Another challenge with this approach is I'm not sure if it's safe to remove a
reference from an object the way the current GC is implemented. That may not
trigger a collection event.

We could store the frame data structure separately, outside of the heap. Maybe
batch things together, so we allocate once every 1000 frames or so. Or have a
vector of objects. That sounds reasonable to start anyway.

Let's start simple. Each object has two extra fields: next in frame, and next
frame. FbleValueHeap keeps track of the top frame, and maybe an integer saying
how many frames down on the stack that is.

To allocate an object:
* next points to top frame,
* tail points to top frame's tail.
* drop reference to previous top frame.
* set new top frame

To allocate a new frame:
* increment count on FbleValueHeap.

To pop a frame:
* release the top frame, decrement the pop count? No, that doesn't quite work.

How can we have frames with no allocations? Where to keep info about that?

Again, start simple. Each object has one extra field: next in frame. We have a
vector of Frame entries that we update for each frame we enter (regardless of
whether we allocate there or not). The frame stores the top object on the
frame which we have retained. It could be NULL.

To push: update top object on frame.
To pop: release top object on frame, pop frame.

Yeah. That's it. Good and simple.

Trouble: I need a way to know if the value being returned was allocated on the
frame being popped or not.

The cases are:
A. value being popped was allocated down on the stack. No need to
   Retain/Release.
B. value being popped was allocated on this frame.
   Don't release. Push onto child frame.
C. value being popped is referenced from a value allocated on this frame.
   Retain and push onto the child frame.

How do I distinguish between these three cases?

We need to allow the same value to show up on multiple frames.

That solves it. So in the end we basically move our stack from managed by the
compiler to managed by the heap.

Performance:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 44.81 ==> 56.61
  Maximum resident set size (kbytes): 2516 ==> 2116 ==> 2364

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 98.44 ==> 120.47
  Maximum resident set size (kbytes): 99592 ==> 101404

We'll need to improve performance before we can take it. But at least fbld is
in the right ball part now.

---

Avoiding vector shrink resize for the stack:

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 98.44 ==> 120.47 ==> 109.03
  Maximum resident set size (kbytes): 99592 ==> 101404 ==> 100472

---

Idea for tracking these cases:
A. value being popped was allocated down on the stack. No need to
   Retain/Release.
B. value being popped was allocated on this frame.
   Don't release. Push onto child frame.
C. value being popped is referenced from a value allocated on this frame.
   Retain and push onto the child frame.

Have a frame counter. Every time we push a new frame, increment the counter by
1. When we allocate a new object, store the current frame counter with the
object.

Now, when we return an object, there are three cases:
1. The object's allocation frame is less than the frame being popped. No need
to retain/release.
2. The object's allocation frame is the frame being popped. Don't release, move
to child frame. Update the allocation frame value to the child frame.
3. The object's allocation frame is the greater than the frame being popped.
The value being popped is referenced from a value allocated on this frame.
Retain and move onto the child frame, updating its allocation frame.

It's exactly the info we wanted. The benefit of this is we don't have to walk
through all the objects on a frame when we pop or compact. We can let GC
traversal take care of that. We just need to retain one object per frame which
can reach all the other objects on the frame.

Actually, in this case, no need to distinguish between (2) and (3)? The main
thing being that we aren't allowed to disrupt the reference chain down the
stack. But we can mess it up for any other objects?

Straw:
* For each frame we retain a single object which can reach any other object on
  the frame.
* Each object says which frame push it was allocated on.
* When you pop a frame,
  - if the returned object was allocated downstack, we know it's retained
    downframe, so just return it.
  - if the returned object was allocated since, make it the new retaining
    object for down stack (i.e. retain it) and return it.

Easy, no? Compact works similarly, except we add saved values to a new
frame instead of going downstack.

Each value has:
* 'next' field in singly linked list.
* 'generation' field for frame it was allocated at.

And we have a growing vector of values which are the retainers for each frame
on the stack.

The heap knows its current generation count.

Let's try it.

Do we have to increment generation count for compact frame? No, because
'downstack' hasn't changed?

It's leaking memory. I fear the current GC assumes we never change pointer
values of an object except to go from NULL to non-NULL.

Can I make a custom GC for this? It feels like we should be able to do better
that way.

---

Proposal for a custom GC:

Objects can be part of a doubly linked list of objects.

Each frame stores three lists of objects:
1. Potentially garbage objects allocated on callee frames.
2. Non-garbage objects returned from callee frames.
3. Objects allocated on this frame.

New objects go to the list of objects allocated on the latest frame. When we
pop from a frame, we add all objects allocated there to (1) of the caller,
then move the returned objects to (2) of the caller.

To perform GC: find the oldest frame with garbage (we can track this
incrementally so it's constant time to find), traverse objects on (2) as
roots, any objects in (1) not traversed after GC are freed.

During the traversal, we only traverse objects in (1) and (2). Any other
objects we don't have to worry about.

Keep track of the frame/generation an object belongs to so it's easy to tell
if we need to traverse an object or not. When traversing, update the traversed
objects generation to the current frame.

The reason to GC the oldest frame with garbage is because newer frames will be
popped first, and if we pop a newer frame before we GC that frame, we can
avoid traversing some objects entirely.

---

A couple thoughts:
* If we pop a frame while doing GC on it, we could lose track of where some
  objects belong. Best to not allow popping a frame while doing GC on it.
  Instead, when popping the frame, abandon the GC in progress entirely and
  return the garbage to the caller like we normally would when popping a frame.

  In practice, implement this by doing GC directly the data structure for the
  frame rather than moving potential garbage to a central area.

* If we are storing pointers to lists on frames, we have to be careful about
  reallocating and moving frames. To be on the safe side, use a data structure
  that doesn't move frames around in memory.

* How do we prevent GC from falling behind when GCing a frame that we keep
  compacting? If GC can't finish by the next time around and we keep adding
  more values to GC, we'll never finish, right?

  So, we should store current GC objects separately from the frame. We just
  need to make sure we pull them back into the frame if we pop the frame
  before GC finishes.

---

Draft is done. Things to debug:

SpecTests:
* StackSmash: fble-test.cov: ../lib/value.c|855| FbleFuncValueFunction:
  Assertion `func->_base.tag == FUNC_VALUE' failed.
* MemoryGrowth test: memory constant: M(10000) = 0, M(20000) = 0

Let's debug StackSmash first.

We allocate a ref value. Then a function value. We assign the function to the
ref. We free the ref value. We allocate a union value over the same space,
then try to access the ref value as a function.

Question is: how did we have a reference to the ref value but somehow managed
to GC the ref value? Guess I'll have to debug more.

We compact the heap, saving the function that the ref value points to. We
don't see that function holding on to anything, so we free the ref value.

Ah. I think I see. We are compacting the heap. We need to track generations
properly in this case to know what we should or should not traverse. Right now
we are saying don't traverse from func back to ref because ref has the
generation of the gc frame. That's the logic I had in mind, but it's wrong
because we want to traverse ref in this case because of compacting the heap.

---

If we didn't have compact frame:

Objects popped and saved to a frame with gen x:
* All have gen greater than x.
* None reference any object with gen greater than x outside of the set
  popped+saved.

That way we can safely traverse, following all objects with gen greater than
x. We'll cover all objects in the set and never go outside. As we go, we set
objects to gen x to remove them from the set of things to traverse.

That works great.

If we only had compact frame:

Objects popped and saved to a frame with gen x:
* All have gen greater than or equal to x.
* None reference any object with gen greater than or equal to x outside of the
  set popped+saved.

We could traverse by following all objects with gen greater than or equal to
x. We'll cover all objects in the set and never go outside. But we have no
obvious gen to set the new objects to remove them from the set of things to
traverse.

In general we have a mix of the two cases. Say we compact a frame, then push,
then pop back to it, all while GC is working elsewhere.

In the case of normal popping, we only traverse an object once, to move it to
the caller frame. For compacting, we could repeatedly traverse it on the same
frame as we repeatedly compact that frame.

In terms of generations when compacting, how about:
* Original generation of the frame is X.
* On compact, saved+popped is generation [X,Y].
* On compact, set new generation for frame to Z.

Now when we gc, we say traverse anything in range [X,Y], move to Z. Can we
make this work for both pop and compact cases?

Say we stored the range explicitly for saved+popped. If we had that, it's easy
to traverse it. And say we promise the generation for the target frame is not
in that range. Then it's easy to traverse. Can we maintain that?

At any time we might pop the frame before finishing GC. We would know the
range of all objects: range of saved+popped plus frame->gen plus anything
between here and current heap->gen.

Three operations to handle: Push, Pop, Compact.
* Push: Increment heap->gen, range starts empty (?).
* Pop: Add to range [callee->gen, heap->gen]. Note: caller->gen < callee->gen.
  Let's say callee->gen is the lowest gen object allocated on the frame always.
* Compact: Add to range [caller->gen, heap->gen].

New objects can always be allocated to a frame using heap->gen. They aren't
allocated using frame->gen. Is it safe to use heap->gen as the destination for
traversed objects?

I don't think so. Say we have callees allocating objects. Some will be gen
less than heap->gen, some will be gen greater than heap->gen, some will refer
to these objects we are saving in the caller. When the callee pops, we won't
be able to isolate the objects to traverse to saved+popped.

Compact is a special case. When we compact, we can allocate a new target id. A
new current generation for that frame.

What if we can process frame->gen is outside of saved+popped? GC needs to know
the lower and upper bound for traversal still.

Can we use (frame-1)->gen as a lower bound? Not if frame->gen is the lower
bound.

How about we record if we have compacted or not? compacted means the range of
saved+popped is ((frame-1)->gen, frame->gen), otherwise saved+popped is
(frame->gen, heap->gen]. Excepted we can't use (frame-1)->gen, because it's
lower than the actual lower bound.

Say we have two generations per frame: min, gen. They start the same. When we
compact, min stays the same, we bump gen. You can always use gen as the
destination for gc. Record if you've compacted or not.

If you've compacted, range is [min, gen). Otherwise range is (min, heap->gen).

Except, the problem is if you both compacted and popped. We can no longer use
frame->gen as an upper bound.

Needs more thought.

---

compact + compact => compact
popped + popped => popped
popped + compact => compact

The problem is: compact + popped. We want to compact forward to the
post-compact generation and pop back to the post-compact generation. That
means the objects span before and after.

A solution is to keep them separate. Each frame can have compacted objects and
separately popped objects.

Conceptually the solution is easy. A clean implementation is harder to come
by. Now each frame can have two parts of GC.

---

Bugs to work out. All the spec tests are passing, but as we know, that doesn't
mean much when working on GC these days.

Core test fails. Need to minimize.

It looks like we were compacting a frame while gcing the frame, which caused
us to somehow try to free a non-value alloced list node. The workaround was to
abandon gc in this case.

But now I'm afraid we could fall behind and never finish GC in a tight tail
recursive loop. In theory we should be able to keep subsequent compactions
separate from the current gc. The challenge is making sure we move marked
objects to compacted_saved instead of alloced after that. That means knowing a
bit more about what we are currently running GC on.

I should maybe write a test case for this? The idea would be: allocate a bunch
on the frame. Then compact and go into a tight tail recursive loop that
doesn't allocate much each iteration. The claim is we leak memory when we
shouldn't be leaking memory. We would get memory growth instead of memory
constant.

Initial performance numbers:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 52.21
  Maximum resident set size (kbytes): 2516 ==> 2260

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 98.44 ==> 86.73
  Maximum resident set size (kbytes): 99592 ==> 83044

Oh hey, that's actually pretty good for fbld. Maybe good enough to
mainline?

Before we mainline, I want to figure out a way to avoid hard coding a limit on
number of stack frames and preallocating stack frames.

Maybe allocate new frames on demand, put them in a linked list, but save them
for reuse? The important point is that we don't try to move them via realloc,
otherwise the pointers to lists get messed up.

Operations on frames:
* quickly compare stack depth of two frames.
* quickly access frame + 1 for next_gc.
* iterate over all frames for final cleanup.
* Add frame to the back.
* Pop frame from the back.

Seems like a linear doubly linked list of frames where we append to the end of
the list as needed and keep a pointer into the top of the list could work
well?

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.82 ==> 52.21 ==> 51.05
  Maximum resident set size (kbytes): 2516 ==> 2260 ==> 2344

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 98.44 ==> 86.73 ==> 86.85
  Maximum resident set size (kbytes): 99592 ==> 83044 ==> 86060

Cool. I think we're ready to mainline this now. I don't mind a 1 second
regression in md5 given the 11 second improvement in fbld.

---

What are the next steps here?

Do another round of performance analysis. Maybe there are some improvements we
can make to the current GC approach based on that. After that, consider again
if we can avoid allocating on the heap in some case.

md5:

 7.69    33193  FblePopFrame[0012X]
 3.67    15822  FblePushFrame[0016X]
 2.16     9304  FbleCompactFrame[0014X]

**      950    38890    14550  NewValueRaw[0019X] **
        362    10369           FbleAllocRaw[0028X]   
        124     9023           _int_free[001eX]   
        107     3674           cfree@GLIBC_2.17[002cX]   
         85      828           FbleFree[0041X]   
         70      355           free@plt[004aX]   
         39       90           malloc[0029X]   

We could maybe improve PopFrame and CompactFrame if we had fewer object lists
to manage on each frame. Perhaps most of the time these lists are empty. Maybe
we could use a separate data structure to store GC possibilities separate from
the main stack? That way we don't need to deal with any lists if they are
otherwise empty.

PushFrame appears to mostly be overheads of the function call. I wonder if we
could inline it, since it is mostly called in one place, and if that wouldn't
improve performance notably.

Today we don't move any objects. We could simplify the code slightly under
that assumption. Not sure if I want to make that assumption yet though.

If we can do anything to improve GC, I think it would be to keep potential
garbage on a separate stack. Push/Pop frame should be faster, especially for
frames that don't do any allocation.

Otherwise, for md5, can we come up with some way to avoid IncrGc and malloc
for the short lived struct object allocations in Addx and Shlx?

fbld:
 8.94     2375  FbleCompactFrame[0007X]
 6.87     1826  NewValueRaw[000aX]
 6.54     1738  FblePopFrame[000eX]
 2.18      579  FblePushFrame[0050X]

**     3398     5337     1826  NewValueRaw[000aX] **
        951     1812           FbleAllocRaw[000bX]   
        754     1065           _int_free[0044X]   
        406      503           cfree@GLIBC_2.17[0059X]   
         84       90           FbleFree[0085X]   

fbld looks similar to md5.

Note that Push/Pop/Compact are orthogonal from NewValueRaw. NewValueRaw only
happens with allocations. Push/Pop/Compact happens all the time.

That means there are two things to work on next:
1. Optimize Push/Pop/Compact. By coming up with better data structures.
2. Optimize NewValueRaw. By figuring out how to avoid calling it for short
lived objects.

---

For (1), I think we can group compacted and popped together. The key is that
we don't traverse if the object has the same gen as the gc frame's gen. That
covers both cases of compacted and popped.

That should be a good start to performance improvement. There may be room for
more after that.

I haven't thought of anything for (2) worth trying yet.

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 51.05 ==> 50.59
  Maximum resident set size (kbytes): 2344 ==> 2268

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 86.85 ==> 84.46
  Maximum resident set size (kbytes): 86060 ==> 83784

---

Idea: We don't have to call FblePushFrame/FblePopFrame at every function call.
We only need to call it enough to avoid excessive garbage. In particular:
* Only call FblePushFrame/FblePopFrame for recursive functions (when it's a
  RefValue?). Anything else will do constant allocation, which we assume is
  small enough we don't need to worry about.
* Only call FblePushFrame/FblePopFrame for one out of every X function calls.
  Again, each individual call is a constant amount of allocation. We can take
  a constant overhead of garbage easily enough.
* To skip FbleCompactFrame, just don't compact the frame. If we skipped the
  push for the caller and then find ourselves in a tight FbleCompactFrame
  loop, we can push a new frame instead of compacting and then compact in
  place the next time around on that frame.

This could get us up to 10-15% improvement based on the profiles for md5 and
fbld.

To start, I think we should move the code from function.c into value.c. It was
never clear which code should go where. That will let the compiler do inlining
in performance sensitive code and let us play around with things like only
calling PushFrame/PopFrame at recursive function boundaries without having to
deal with crossing a level of abstraction.

Moving the function.c code into value.c:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 50.59 ==> 49.05
  Maximum resident set size (kbytes): 2268 ==> 2340

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 84.46 ==> 80.68
  Maximum resident set size (kbytes): 83784 ==> 84732

---

To give a sense, here's a sample of function calls from md5 based on whether
they are normal, recursive, tail normal, or tail recursive. Note that
relatively few calls are recursive. So there's a potential to save a lot
here by skipping Push/Pop/Compact frame in that case.

1144988 n
  12907 r
 192804 tn
   7205 tr

How about this: add an option 'recursive' to push/pop/compact. Add to each
frame a count of non-recursive pushes and pops.

Push:
 * If recursive, push a new frame.
 * If not recursive, increment the non-recursive push count.

Pop:
 * Decrement the push count. If it's zero, pop the frame.

Compact:
 * If not recursive, do nothing.
 * If recursive and push count is 0, do like we do today.
 * If recursive and push count is greater than 0,
   Decrement the push count on the frame and push a new frame with push count
   0.

Easy? Worth a try?

Double check this is okay to do though. For example, any frame could end up
pointing to a really big object. With this approach, we end up essentially
holding on to some frames longer than they would otherwise. Are there bad
cases where that could impact memory complexity?

It's always a constant factor, right? 

Say we merge 5 frames, and each frame holds on to a big list. We end up with 5
big lists. That's not breaking anything. It's as if we had a single big list
which took 5x more memory to store. Constant factor should be fine.

But is it merging 5 frames, or is it potentially merging more? It's really
merging a tree of frames, right? If we assume at most K calls per frame,
that's potentially K^5 frames worth of memory all being merged into one. Still
constant, but a bit larger of a constant.

I still think we are okay. Shall we try it?

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 80.68 ==> 77.85

So, a little bit of improvement.

Official numbers:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.05 ==> 48.07
  Maximum resident set size (kbytes): 2340 ==> 3668

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 80.68 ==> 78.77
  Maximum resident set size (kbytes): 84732 ==> 103956

Memory usage is definitely up. Is that expected and okay? Or are we
essentially making things go fast by leaking a bit of memory?

Anyway, I have an idea for how to pull everything together based on this.
Allocate all new values on the stack frame. When we PopFrame or CompactFrame,
traverse the stack frame values converting to GC allocated values there.

My claim is this approach is simple, fast, lets us allocate most short lived
objects directly on the stack without malloc/free, but also avoid overheads of
traversal. The best of all worlds hopefully. If we find short lived objects do
end up allocating on the heap, we can consider merge more of the recursive
calls, say 4 of every 5. We end up with a decent sized pool of stack allocated
objects that avoid the need for malloc/free.

---

It's fine for stack allocations to refer to GC allocations. When we pop the
frame we'll traverse the stack allocations to see which GC allocations are
referenced.

It's not okay for GC allocations to refer to stack allocations, because we
don't want to have to traverse a long chain of GC objects every time we pop.
Proposed solution: turn all stack allocations into GC allocations as part of
traversing the popped frame. This includes stack allocations farther down the
stack.

To avoid creating duplicate GC allocations for the same stack allocation,
create a GcValue kind on the stack to replace the original stack allocation.
That GcValue acts like a RefValue and points to the Gc version of the
allocation.

For stack allocations, we want a stack data structure. Maybe start with like
what I did before: malloc a big array at the start for all the stack
allocations and hope it's big enough. Once we prove out the approach, we can
think of better ways to (dynamically) size the stack. We can use that stack
space for storing frames as well. Only the GC allocated objects need to be
allocated via malloc.

Pieces to pull together:
* Use malloc allocated stack space for frames.
* Implement a GcAlloc function that converts a stack allocated value to a gc
  allocated value.

To start, we can allocate values on the stack with space for list and gen
fields. It's a little wasteful, but will simplify the programming. We could
reuse a list field to track gc allocated counterparts to the value, for
example. We could optimize space usage later if desired.

High level approach is easy then:
* Initially allocate by bumping the stack pointer.
* Pop and Compact call GcAlloc to convert to a GC object when
  popping/compacting.

Cool. Let's give it a try.

---

* Let's allocate all native values on the heap instead of the stack, to
  simplify tracking of the destructors.

---

Okay, draft is done. Looks like we have a memory leak somewhere. How to track
it down? Found it.

Some early results:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 48.07 ==> 43.30
  Maximum resident set size (kbytes): 3668 ==> 3180

That's pretty nice.

Looks like we have some bug with ref values though, because fbld fails.

Let me allocate ref values on gc all the time. Yeah, that seems to fix it. I'm
not sure exactly why it's problematic, but it wouldn't surprise me if it is.
It shouldn't be bad to always Gc alloc.

The performance numbers look good:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 48.07 ==> 43.72
  Maximum resident set size (kbytes): 3668 ==> 3168

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 78.77 ==> 64.85
  Maximum resident set size (kbytes): 103956 ==> 107168

Can we mainline this?

Steps to do first:
* Make sure all the tests pass.
* Review linux perf results. See if anything interesting shows up.
* Figure out how to properly pick the stack size. Can we dynamically allocate
  as needed?

The performance is good enough. Good enough to declare the performance goals
of release fble-0.3 met I would say.

Tests pass. Let's mainline this and worry about a better approach to picking
stack size next.

It's hard to see much from the md5 perf profile. Who knows how much is inlined
where. The time spent in NewGcValueRaw is 2.5%. It's looking pretty good.

Similar in fbld perf profile. Hard to see based on what's inlined where.

I'd say performance wise we've achieved our object with stack allocation.

Now, how do we pick a better size for the stack?

Options:
1. Pick a better absolute size to start with.
2. Dynamically allocate more as needed in big chunks.

I like (2) better practically because we don't have to worry about silently
overrunning buffers or trying to gauge how much memory the user has, or
worrying about overcommit policy. The downside of (2) is we don't have a
single continuous range.

Let's start with (2), see how we can handle it.

A few issues to handle:
* How to check order of frames. Right now we use min_gen. Let's continue to
  use that for the time being. Easy.
* How to test if an object is gc allocated or not?
  - Store something in the value itself next to tag. Easy to do. Too costly?
  - Store something in the pointer value itself. Trickier. Maybe okay?
* Tracking bounds for new allocation.
  - Add a check every time we allocate. Easy. Not sure how costly.

Sounds like (2) is entirely feasible. Not sure how big a cost it is, but
honestly probably not very costly in practice. The idea is to allocate the
stack dynamically in large chunks of say, 1MB each. When we run out of space,
allocate a new chunk. We can keep linked list of allocated chunks if needed.

Say a Frame* lives in it's caller's space. The frame itself can allocate extra
if it needs to for new values or the callee. When we pop a frame, we put aside
any extra allocations it made for later use. Shouldn't be too bad. It's nice
to align the cleanup with frame pop. I assume/hope that the majority of frames
don't have any extra space allocated, so it's pretty cheap to deal with.

Add two fields to frame:
* A singly linked linked list of extra allocations.
* An intptr_t which is the max of the allocations.

Add one field to heap:
* A singly linked list of extra allocations.

Add a helper function to StackAlloc a certain size. Use that for allocating
values and frames. It checks for sufficient space, advances top pointer. If
necessary it uses or allocates a new extra allocation. When we pop the frame,
move all the extra allocations to the heap - O(N) is fine here, because there
should be very few extra allocations. It's very unlikely that a heap needs
more than 1.

Sounds like a plan. Let's implement it.

---

Cost of using a field in the value to store gc versus stack alloc instead of
pointer comparison:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 43.72 ==> 43.25
  Maximum resident set size (kbytes): 3168 ==> 3280

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 64.85 ==> 64.59
  Maximum resident set size (kbytes): 107168 ==> 106024

Cool, it's a performance improvement.

---

Cost of allocating the stack in 1MB chunks instead of all up front:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 43.25 ==> 44.19
  Maximum resident set size (kbytes): 3280 ==> 3276

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 64.59 ==> 66.65
  Maximum resident set size (kbytes): 106024 ==> 108952

Oh well. There was bound to be some cost.
