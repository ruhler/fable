Fble Compilation Challenge
--------------------------
Should we compile fble to machine code before evaluating it, or should we
continue to use an interpreter?

Advantage of compilation:
* Potential for substantial performance improvements.

Disadvantage of compilation:
* It's more tedious to run a program if you have to invoke a c compiler, for
  example.

What about AOT vs JIT vs ...?

That's more a question of what representation a program should be distributed
in. The tradeoff is: too high level a representation requires compilation /
interpretation to be done somewhere. too low level a representation takes up
more space and is limited in where it can be run.

I guess today the representation is .fble, which is relatively compact, but
takes some time to compile and is slow to interpret. If we compile, the
representation would be .o, which may or may not be as compact, but is much
faster to run.

Regardless, we need to go to well specified formats (.fble, .c, .o, etc.), and
in general it's good for the framework to support separate pre-processing
passes, so I think this is worth introducing.

This is an end user API change. Instead of loading a .fble program, you want
to load a .o file. Or maybe you want to link against a compiled .o program.

Proposal: given a .fble program, compile to an object file that exposes the
following API:

FbleValue* Eval(FbleValueHeap* heap);

And in general, once you have an FbleValue, you can run it efficiently with
the existing FbleApply and FbleExec APIs, it just that code will be
represented as pointers to machine code instead of pointers to instruction
blocks.

---------

It seems worth doing some more detailed research and up front design for this.
Consider the following options to start:
* Generate c code.
* Use llvm.
* Use c--.
* Generate a custom, compact, efficient bytecode.

C-- looks obsolete now, and I'm not inspired by Haskell's performance.

llvm looks like a very complex entity I'll have to learn a lot more about if I
want to use it.

C code will be pretty clunky.

My vote is to experiment with llvm to start, and maybe spend some time
thinking about a custom bytecode.

Things to figure out how to do in particular in llvm:
* call into an fble runtime library
* tail call behavior - calling into a function as if by goto instead of
  pushing something on the stack
* storing pointers to code

Perhaps a nice way to avoid a direct dependency on llvm would be to generate
llvm's text format assembly language. Not sure. We'll have to see.

---

It seems fine to use llvm as the bytecode instead of a custom bytecode. It's
presumably just as compressible as whatever I come up with, but has the
advantage that there is existing technology to convert it to machine code.

We still want to have control over threading, however. Which means llvm isn't
at the bottom of the execution stack, it's in the middle, between the thread
scheduler and the actual computation.

Let me start calling llvm native code. The value of using native code over an
interpreted bytecode is you get hardware support for incrementing the pc,
fetching the next instruction, predicting branches. That could easily give you
10x improvement over interpreting instructions.

The concern is that the thread scheduler, or interaction with it, may start to
dominate performance overhead. I expect the native code would be a function
you could call that would do some finite amount of work on a thread and then
return a continuation. This means we can't use the native stack, unless the OS
supports some form of voluntary context switching we could leverage. And it
means we'll end up with a scheduling loop on top things that doesn't take
advantage of hardware support for fast pc incrementing, etc. If the amount of
work that gets run on a thread is small, that would be bad. Which suggests we
want to inline as much as we can to get decent size chunks of uninterrupted
native code.

In summary, some things to think about:
* Inlining to get decent size chunks of interrupted thread execution.
* See if llvm has any support for cooperative multithreading, or manipulation
  of the thread.

---

In terms of a custom bytecode. If we wanted all instructions to have the same
size, we could do it with:

FbleInstrTag tag;
bool exit;
FbleLoc loc;
size_t tag; // or blockid, or pc.
FbleFrameIndexV args;
FbleLocalIndexV dests;
FbleInstrBlock* code;

Notes
* exit could be incorporated into tag
* args is a vector for struct and scope, otherwise it could be 2.
* dests is a vector for fork, otherwise it could be 1.
* code could be a pc in theory, if all the instructions were saved into one
  big array. Though I guess we would need some other way to initialize the
  locals memory.

---
It looks like llvm has a tailcc calling convention that can be used to support
tail calls. Good.

It doesn't look like llvm has any general support for multithreading.

Let's see if we can implement our own multithreading mechanism. The idea is to
separate execution into three parts:

1. thread scheduling. Decides which thread to execute next.
2. cooperative multithreading. A way to run a computation for a fixed amount
of time (in terms of instructions executed probably), then snapshot the thread
state in a way that can be later restored.
3. running computation for a single thread.

(1) is easy enough, assuming we have a way to represent thread state.
(3) is easy enough. Just compile to native machine code. Take full advantage
of hardware support for pc, instruction fetch, stack manipulation.

(2) may be doable. Here's what I envision: at the start of each block, we add
a 'yield' instruction. The yield instruction says how many instructions are in
the block. It checks to see if the current time slice is up, and if so,
magically calls into the thread scheduler. For example, maybe it causes
registers to be saved to the stack, records the stack pointer and pc
somewhere, then switch out the stack pointer and pc for another thread to
resume.

I expect thread scheduling to be low overhead, because switching threads
happens so rarely. The big cost here is having to keep track of how many
instructions we have executed and whether we need to yield or not. Because
that needs to be run very often.

I wonder if there's any value to yielding randomly instead of strictly by
count. That way we don't have to pass the count around everywhere. We would
still need to compute a random number, but we could use a global variable for
that? We could use the same random number as we do for profile sampling? As
long as it is pseudo-random, it will be deterministic. Is that meaningful? At
least is is runtime environment agnostic, if not agnostic to the generated
instructions.

---

gprof suggests there isn't that much to gain from compiling. Because the
execution time is dominated by allocations and memory management work.

---

Threading revisited: we can use pthreads if we want, or pth, or getcontext.
For pthreads, we just need a mutex that a thread has to acquire before it can
do any work. That way only one thread runs at a time.

Problems with pthreads:
* non-deterministic behavior is, right out of the box, causing lots of
  problems. Bugs go away under the debugger. Different bugs from different
  runs. That's going to be a debugging nightmare as long as we use pthreads.
* limits on number of threads. Like, 32K number on linux? That's not really
  acceptable. Max user processes of 7314 by ulimit. Definitely not acceptable.
* only the main thread has a dynamically sized stack. Though I'm not sure how
  we could ever get around that limitation. gcc's -fsplit-stack is not
  supported by the gcc preinstalled on my computer.

---

Idea is to compile blocks of instructions together that we can execute
atomically, while still using the interpreter to drive execution. Hopefully
this cuts down noticeably on the cost of instruction dispatch, while still
supporting full control over thread execution.

We can start by converting the existing instructions to abstract functions
that the interpreter calls. This will make the interpreter compatible with
abstract functions operating on thread state. The compiler can then generate
some new abstract functions and that works fine with the interpreter.

The concern is there will be too much overhead to calling these abstract
functions. Let's do some experiments.

The most commonly called instruction at the moment in fble-bench, by far, is
FBLE_RELEASE_INSTR. It accounts for over 40% of instructions executed.

One nice benefit of putting instructions into functions is then profiling can
tell us which instructions take a lot of time, and how much time is spent on
instruction dispatch.

Baseline fble bench: 2m9.762s

Experiment:
  Access thread state through thread pointer instead of local variables.

  fble bench: 2m10.890s

Experiment: 
  Pull ReleaseInstr into its own function.

  fble bench: 2m12.889s
  
If we assume all instructions regress the same amount, then total I would
expect 7.5s overhead changing all instructions to functions. But then we can
get rid of the binary search from the switch statement, which I expect would
get us all of that back. We also get the advantage of per-instruction
profiling breakdowns and the opportunity for compilation. I think it's worth
trying. Let's do it.

After converting a bunch more instructions over:
  fble bench: 2m12.251s

After converting all the rest of the instructions and getting rid of the
switch statement entirely:
  fble bench: 2m16.704s

That's too bad. Total regression of 7s, even after removing the case
expression.

Well, if it's any consolation, we get 1m12.644s with -O3 turned on and no
profiling arcs.

---

Starting up again to try and get an fble compiler.

Let's start as simple and basic as we can. Generate C code which the user can
compile using a C compiler. No need to bother with llvm at this point. I can
make my own abstractions for representing C programs if I don't want to just
output strings everywhere.

Primary goal: replace the interpreter while loop with straight line code that
calls the same exact exec functions the current interpreter does.

Today we have two kinds of functions:
* FbleFuncValueTc,
* FbleCompiledFuncValueTc

The first represents a function value as an abstract syntax tree of values
after type check. The second represents a function value as an fble
instruction block. Maybe we can add a third type that represents a function
value as pointer to a C function to call?

It's starting to come back to me now.

Goal is to separate thread scheduling from function execution. We currently
track thread state in thunk values. It would be nice if we could have two
kinds of thunk values: one tracking func, pc, and locals for interpreted code,
and one with a function pointer and locals for compiled code?

Maybe what we want is an abstract data type representing thing that can be
partially computed.

 Inputs: 'this', heap, thread, io_activity
 Results: updated 'this' or finished or aborted, etc.

With a single method to 'run some finite amount of time'.

Or, change instructions to hold the actual C function pointer for executing a
function. Then the compiler could merge adjacent instructions that can be
compiled together into new C functions and insert them into the instruction
stream?

I don't know. I need to think on it a bunch more.

---

Two big challenges for compiling fble that I keep getting stuck on:
1. Fine grained multithreading. Which is painful, because I feel like I could
get away without it fble in practice, but that we really want it in theory.

2. General support for tail calls not smashing the stack. I double checked.
gcc and llvm can be made to support a special kind of tail call where the
function being called has the same prototype as the caller. Is this good
enough? Maybe, depending on how we abstract away function calls.

But I can't get a simple example to work for gcc using -O2 and
-foptimize-sibling-calls if the calls span across multiple .o files. Seems
frighteningly restrictive.

---

Here's my initial proposal. We support multithreading and tail recursion to
start. There will be, perhaps, modest performance gains initially, but
hopefully there's lots of opportunity to improve from there.

Well generate a C function for each InstrBlock. The C function is the compiled
form of the vector of FbleInstrs of the InstrBlock.

The C function has the following API:

Status F(FbleValueHeap* heap, Thread* thread, bool* io_activity);

The function will read the value of the pc from the top of the stack to
determine where to start executing from. Given instructions a, b, c, ...,
we'll use a case statement to allow the function to jump directly to the
current pc. For example:
  
  switch(thread->stack->pc) {
    case 0: a;
    case 1: b;
    case 2: c;
    ...
  }

This will incur an overhead when resuming a function after a context switch.
Hopefully the overhead is small in the default case when we start from pc 0,
or we could always add an explicit check at the start of the function to try
and optimize that case. Once the function finds where to start, I assume
fall through to the next instruction will be as if we didn't have the switch
statement at all.

Initially we should add logic around every instruction to check for profile
sample and time slice yield. We should emit profiling op instructions directly
too to support profiling.

If we need to yield or block, it should be as easy as returning the status
code, because we'll keep the thread state up to date in the Thread* pointer at
every instruction.

To do a normal function call, we can update the thread state for the new
function and then directly call that function's compiled C function. To do a
tail call, we can update the thread state for the new function and then return
'RUNNING' to the caller. Then it's the callers job to call back in to whatever
is on the top of the stack? It's certainly doable, but some details need to be
worked out. Like, how does it know if the function that it called finished or
has a continuation? And we would need special logic every time you call a
function to wait for the continuation to be completed.

To ease the transition, maybe we can add another field to FbleThunkValueTc
which is the native C function to call to execute it. Initially they all share
the same native function: the equivalent of RunThread, but that only executes
code from the one function, not any functions being called. That way we get
the API in place, and deal with having to call functions recursively and
handling tail calls, but without having to worry about compilation yet.

Once we have compilation, we can switch to the native function and ignore the
FbleInstr** pc. Hmm... Maybe FbleInstr** pc should change to an int pc that we
read from func->code if needed. Maybe better to add the native function to
FbleCompiledFuncValueTc. And once we switch to native, we can ignore the
'code' field or set it to NULL. Yeah. That sounds good.

We can do initial work in the interpreter to prepare the way so the switch
compilation is easy:

Add function pointer to FbleCompiledFuncValueTc that should be called to
execute the body of the function instead of RunThread (or in collaboration
with RunThread). Initially everything FbleCompiledFuncValueTc should use the
same generic C function. We'll have to figure out how to manage function
calls, tail calls, continuations at this stage. It's probably a good time,
too, to factor out the code for running the body of a function from the thread
scheduler.

---

Well, we have a problem. I tried compiling Fble/Tests.fble to a C function
that allocates the corresponding FbleCompiledFuncValueTc. It's over a million
lines of C code, and gcc isn't too happy about that (I gave up after 2.5 hours
compiling and 64% of my computer's total memory).

Perhaps we loose some sharing by compiling. I don't think for InstrBlocks or
Instrs. Maybe Locs? Or at least source file names. But I don't feel like that
would account for a million lines of code. More like we just have a lot of
instructions. Think about a literal string for example. Lots of instructions
there. I should disassemble Fble/Tests.fble to see how many instructions.
Maybe multiply by 5 or 10 or so to get expected number of lines of C code?

I'm thinking we're going to need some form of modular compilation. Would it be
enough to put each FbleFuncValueTc* into its own C function all in the same
file? It will still be over a millions lines of source code though. That seems
unrealistic.

Can we do per-fble module compilation? That sounds complicated, but is almost
certainly what we'll want eventually.

---

Disassembly of Fble/Tests.fble shows:
* 1073 instruction blocks
* 14204 profiling ops
* 115267 lines.

More worrying is that the main instruction block is over 60% of the entire
thing. It has 65K instructions. A lot from the sat solver test and invaders
graphics. That means even if we output each instruction block in a separate
file, we're not going to be able to compile the result.

Two ideas come to mind:
1. Run the main function in the interpreter, compile the resulting function to
C code. That should work around needing to compile the main function.
Hopefully the rest is small things and pre-computed values. Those precomputed
values could be pretty expensive to serialize though, and we would definitely
have to preserve sharing in the generated code.

2. Compile each module's code into a separate C program. In other words,
figure out how to support modular compilation.

---

Modular Compilation

Here is what I'm thinking.

Module Interfaces
-----------------
The interface to a module is its type. When you reference a module, we really
only need to read its interface, not its entire value. But we can derive the
interface from the value if needed. I propose for a module path /Foo/Bar%, we
can track that to two possible .fble files, one containing the implementation
(Foo/Bar.fble), and one containing the type (Foo/Bar.@.fble?).

If the type is present, we use that without reading the implementation. This
is where you get the savings from modular compilation. If the type is not
present, we read the implementation and get the type from there. When we
compile a module we check its type against its type file (if any). The
compiler has an option to output a type file for a module automatically from
its implementation if you don't want to do one by hand.

Ideally the type file is a normal .fble file. It describes a module whose
value is a type that you could refer to normally in the implementation file if
you wanted to. That leads to the tricky question: given a module /Foo/Bar%,
what should be the name of its type module? Maybe /Foo/Bar@%?

Alternatively, it seems like you would never need the type of a type file,
because it contains the same content as the original type file. Then maybe
it's okay to, for example, use '.fble' for implementation and '.fbli' for
interface. Perhaps if you try to reference the '.fbli' from the '.fble' it
automatically figures out the right thing?

TODO:
* Try writing some of these interface files by hand. See how it looks. Would
  we ever want to write them by hand? Would we always want to write them by
  hand?
* Experiment with different mappings from implementation module path to type
  module path.

Generated Module
----------------
We need to compile fble modules to C modules. What does such a C module look
like? I propose we generate a C module with 3 functions:

1. 'Load' - for internal use. A function to lazily allocates and caches an
FbleValue* corresponding to the fble module, given a FbleValueHeap* pointer. I
guess we cache per heap, unless we want an implicit global FbleValueHeap.

This loads dependent modules as a side effect. Multiple calls to Load
increment a reference count.

2. 'Unload' - for internal use. Decrements the Load reference count. If that
reference count drops to 0, frees the cached FbleValue* so that GC can reclaim
it. Unloads any dependent modules.

3. 'Value' - for external use. Calls Load, grabs a reference to the value,
then calls 'Unload'.

This way we don't have to explicitly provide values for dependencies. It's all
done implicitly, with internal sharing as appropriate. We Load, Unload, and
Value functions are named based on the module path. We can compile .fble to
.c, compile .c to .o, link all the .o together, and everything just magically
works if you call 'Value' for the top level module that you want.

TODO:
* Figure out how to map module path to C function name for 'Load', 'Unload',
  and 'Value'.

And that's it. Modular compilation solved.

It's unclear to me if we want to maintain the interpreter separately from
modular compilation or not. Extra burden. Extra testing. But the interpreter
is much more convenient to use than compilation, no? Let's see if I can't
share as much code between them as I can as long as I can before giving up on
the interpreter.

---

It's not too bad to define a type interface for a module, except:
* You kind of have to redefine any types in both the interface and definition.
  You need the type in the interface to get the type of it. You need it again
  in the implementation to return it, but you can't reuse what you define in
  the header for the implementation.

* It's annoying to have to worry about what order things are defined in.

I'm not convinced this is something that should be forced on the programmer.
It sounds like you would want three things
 1. type declarations.
 2. interface (depends on types)
 3. implementation (depends on types)

Or, in other words, types are always defined separately. The interface and
implementation only deal with collections of values.

Or we could make the interface polymorphic in types, but then it doesn't match
the type of the implementation.

This suggests to me that, perhaps, we ought to always generate the interface
during compilation, by the compiler, and for the compiler. That way we have
some more confidence it matches the implementation. The user can always
separate interface from implementation manually if they want to do it for
other reasons. No reason to tie that together with module compilation.

So maybe compiling /Foo/Bar% gives generates an interface file Foo/Bar.fble.@
that the compiler can use, that just happens to be a normal fble file? I'm not
sure. It probably wants all the same information as a type declaration, but it
could pack that some special way. Keeping it a straight fble file would just
be an implementation detail.

For list, I would expect...

@ Unit@ = *();
<@>@ List@ = <@ T@> { +(*(T@ head, List@<T@> tail) cons, Unit@ nil); };
*(
  @<List@> List@,
  <@ T@>(List@<T@>) { List@<T@>; } List,
  <@ T@>(T@, List@<T@>) { List@<T@>; } Cons,
  List@ Nil,
  <@ T@>(List@<T@>, List@<T@>) { List@<T@>; } Append,
  <@ T@>(List@<T@>) { List@<T@>; } Init,
  <@ T@>(List@<List@<T@>>) { List@<T@>; } Concat,
  <@ A@, @ B@>(List@<A@>, (A@) { B@; }) { List@<B@>; } Map,
  <@ A@, @ B@>(List@<A@>, B@, (A@, B@) { B@; }) { B@; } ForEach,
  <@ A@, @ B@>(List@<A@>, B@, (A@, B@) { B@!; }) { B@!; } ProcessEach
);

So we still have names and lets, but it's all inlined. That's pretty cool to
see. And then we save more time by not reading a whole bunch of dependencies
just to get type declarations. I think that's good. No change needed for the
user.

For the first version of modular compilation, we can always read the
implementation file to get its type. Worry about generating these .fble.@
files later as an optimization.
