Fble 0.2 Performance
====================
Working on optimizing the sat solver, it's showing 50% time in Lt/Ord
operation on Int@. That .fble code looks about as straight forward as it could
get. Anything worthwhile we can do in the fble implementation to speed things
up here?

A few ideas come to mind:
* Remove profiling logic from generated code?
 - Maybe put it behind a compile time flag?
 - Because it seems silly to slow things down all the time for a rarely used
   use case.
 - It's probably worth at least measuring how significant this is.
* Remove explicit error checking from generated code?
 - Maybe we just segfault and ask users to run a debugger to see the issue?
 - Maybe we can use signals somehow to trap and handle these cases implicitly?
 - Probably worth at least measuring how significant this is for performance.
* Improve value packing algorithm.
 - Though honestly I don't think that will help at all for Lt/Ord, because I
   don't expect any allocations there.
* Something about function call/tail call optimization?

---

Reviewing perf profile for the sat problem in question:

It shows only 25% time spent in Ord, and about as much time in
FbleNewHeapObject. It could be hard to interpret the profile, given how tight
a recursion is going on here.

Self time shows 8.5% in the Ord function. Everything else is runtime code:

    %     self  block
 8.49     4110  _Run.0x22673a48._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_[0029]
 6.50     3145  MarkRef[0043]
 5.86     2838  FbleRetainHeapObject[0010]
 5.70     2760  FbleThreadTailCall[002e]
 4.86     2350  FbleStrictValue[002d]
 4.64     2248  FbleUnionValueTag[002c]
 4.54     2196  FbleThreadCall[0001]
 3.80     1840  FbleThreadSample[003e]
 3.70     1790  FbleReleaseHeapObject[003f]
 3.48     1682  IncrGc.part.0[001a]
 3.39     1639  Refs[004c]

Reading through the assembly, I feel like we could inline parts of things like
FbleReleaseValue, FbleRetainValue, FbleStrictValue, etc. Avoid the function
call overhead and having to save/restore regs.

My vote is we do some exploration. Don't worry about breaking things or
features. Use a separate branch. Remove profiling and error checking code.
Maybe try inlining things. Focus on aarch64 compiler target. See if we can
flesh out what we can do in terms of generating optimized code and how big a
different it has a potential to make.

After the exploration, we can consider if we want to change anything for
production.

---

First thing, establish a benchmark.

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m37.040s
user    1m36.508s
sys     0m0.469s

Next: Let's strip out all the profiling code. Anything that would be in the
critical path for runtime, including stuff in execute.c and generated code.

Commenting out profiling related code, all except tracking profile block
offset stuff for functions:

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m31.467s
user    1m30.843s
sys     0m0.537s

Attempted again:

So, Maybe 7% performance overhead from having the profiling code in when
profiling is not enabled.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m33.136s
user    1m32.554s
sys     0m0.508s

Next: Let's strip out all the generated error checking code.

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m22.338s
user    1m21.828s
sys     0m0.476s

Next: let's look at the generated assembly code for IntP@ Ord, see how it
looks.

* The jump table looks potentially more expensive than need be?
.L._Run_0x31b06a48.0.pcs:
  .xword .L._Run_0x31b06a48.pc.1
  .xword .L._Run_0x31b06a48.pc.5
  .xword .L._Run_0x31b06a48.pc.15
  ...
  
  bl FbleUnionValueTag
  lsl x0, x0, #3
  adrp x1, .L._Run_0x31b06a48.0.pcs
  add x1, x1, :lo12:.L._Run_0x31b06a48.0.pcs
  add x0, x0, x1
  ldr x0, [x0]
  br x0

  This is a load followed by a branch. Difficult for branch prediction to
  predict anything from I would guess. Don't you think it would be much faster
  to do a binary search based on the tag?

  For example, consider a linear search on the 3 branches in this case:

  bl FbleUnionValueTag
  b.eq .L._Run_0x31b06a48.pc.1 x0 0
  b.eq .L._Run_0x31b06a48.pc.5 x0 1
  b .L._Run_0x31b06a48.pc.15

  No memory load required. We can take full advantage of branch prediction.

  And, if we use this approach, or a binary search, we can take advantage of
  the default option the user provides. So, character equality, for example,
  instead of being a 64*64 split branch, would be a 64 * 2 branch split. That
  sounds like a pretty big deal.
 
* Lots of calls to FbleRetainValue, inlining could help I think. In most cases
  this is either skipped entirely for packed values, or just increment, right?
  
Let's explore the generate of union select first. The right way to do it, I
think, is pass info about default branches down to the bytecode, and use
binary search. That's tricky to do and get right. For a first pass, just to
gauge how sensitive performance is to this, let me switch to a linear approach
like above. It should work fine for small branch counts. Won't work great for
this like character conversion.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m21.529s
user    1m21.114s
sys     0m0.398s

---

Another opportunity to improve performance I just saw: for union access, don't
check the tag value. Assume it's the right value. Hard to productize this,
but would at least be nice to gauge the cost.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m19.422s
user    1m18.999s
sys     0m0.381s

---

Trouble: my numbers aren't very reliable. I see swings +- 2s in some cases
when running the same thing, and sometimes I forgot to build in the change I
intended.

Looks like error checking code is more costly than I first thought. That
changes things.

I kind of wish I had a more extensive set of benchmarks than just the sat
solver. Can we add md5? Is there an invaders benchmark?

Anyway, I couple ideas I think are worth exploring:
* Changing how we generate union select statements. Even if the numbers above
  don't suggest much difference, it feels like the right thing to do.
* Finding some alternative to error checking.
  For the most part it's NULL checking. We could use memory protection and
  signals to handle that?

---

Examples of what gcc generates for switch statements:

  switch (x) {
    case 0: printf("0\n"); break;
    case 4: printf("4\n"); break;
    case 5: printf("5\n"); break;
    case 6: printf("6\n"); break;
    case 7: printf("7\n"); break;
    case 12: printf("12\n"); break;
    default: printf("?\n");
  }

Turns into the equivalent of:
  if (x == 5) {
    printf("5\n"); break;
  }

  if (x > 5) {
    if (x == 7) {
      printf("7\n"); break;
    }
    if (x < 7) {
      printf("6\n"); break;
    }
    if (x == 12) {
      printf("12\n"); break;
    }
    printf("?\n"); break;
  }

  if (x == 0) {
    printf("0\n"); break;
  }
  if (x == 4) {
    printf("4\n"); break;
  }
  printf("?\n"); break;

It uses 'beq', 'bhi', and 'bcc' for the conditional branches. That is binary
search. 'CC' stands for 'Carry clear', same as less than. 'HI' stands for
'unsigned higher', same as greater than.

---

Migrating to new union select.

Benchmarks for awareness:

time fble-sat < rope_0002.shuffled.cnf: 1m39s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m42s
time yes | head -n 40000 | fble-md5: 0m59s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m41.013s

Initial draft status:
* Need to update union_select_tc everywhere.
* Need to update compilation from union select tc to instr.
* Want to update aarch64 code gen to do binary instead of linear search.

---

First draft done. It uses binary search for interpreter, but linear search for
aarch64 code.

time fble-sat < rope_0002.shuffled.cnf: 1m39s ==> 1m36s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m42s ==> 41s
time yes | head -n 40000 | fble-md5: 0m59s ==> 53s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m41s ==> 0m39s

So, modest improvement in performance, which is nice. Note: these benchmarks
don't capture the pathological case of large number of tags. Probably should
have wrote something up for that. Oh well.

Next step: binary search aarch64.

---

I made an fble-cat program. I think this should be good to check the
pathological case of large tags, because it converts characters to and from
ascii.

Let's see, for example:

time yes '!!!!!!!!!' | head -n 10000 | fble-cat > /dev/null: 6s
time yes '~~~~~~~~~' | head -n 10000 | fble-cat > /dev/null: 8s

Hmm... It's hard to know how much of this is union select as opposed to what
characters happen to be where in the tree.

---

How to implement codegen aarch64 for binary search based union select. There
are a number of puzzle pieces to put together:

1. We want to know the max tag value possible.
Imagine you have

  goto x.?(1: a, 2: b, 3: c, : d);

You know min tag value is 0. You don't know if tag 4 is possible or not.
Binary search starts checking on '2', if greater than '2', there are two
possibilities:
 i. 3 is the max tag: jump direct to c
 ii. 4 is the max tag: need to compare tag to 3.

Thus we want to know max tag. Add that as a field to select_instr.

2. Determine if a tag interval range has one or more targets.
Inputs:
* list of targets with tags in interval range. e.g. [1: a, 2: b, 3: c]
* possible tag values. e.g. [0, 4]
* default target. e.g. d

Outputs:
  Either the single possible branch target, or note that there is no single
  possible branch target.

There are two cases when there is a single possible branch target:
 i. The list of targets with tags is empty. Default is the single possible target.
 ii. There is a single element int targets, and that tag is the only tag in
     the list of possible tag values.

3. The top level list of explicit targets may be empty.

For example:
  @ T@ = +(Unit@ t); 
  T@ x = ...
  x.?(t: ...);

Or:
  Char@ c = ...
  c.?(: ...);   

I'm not sure if the language allows the second example, but it certainly
should allow the first. We should make sure there are spec tests for these two
interesting cases.

4. You must explicitly branch to all targets. You can't assume the next target
is the next pc.

Who knows what changes we'll make to compile.c. From code.h point of view, the
targets could be unrelated to the current pc. So, in the case of (3) for
example, we require an explicit branch instructions to get to the target. We
can't assume the target is the next pc.
 
5. When generated code, you'll find yourself in one of two situations:
 i. You have space for only one instruction here, any more has to be
elsewhere.
 ii. You have space for as many instructions as you need here.


Putting all these things together, there are really four cases depending on
the case for (2) and the case for (5):

  * Space for one instr, need one instr: just write the instruction. Done.
  * Space for one instr, need multiple: write a branch to some target, add
    todo item when you have space available to do the multiple.
  * Space for multiple instrs, need one: just write the instruction. Done.
  * Space for multiple instrs, need multiple: write the multiple.

Pseudocode starting at the top level:
  Add info to todo.
  While more todo:
    F(MULTI, next todo)

Where F(space, info):
  if info needs only one instr:
    Write the instruction and return.
  else if space is single:
    Alloc a new unique id.
    Write branch to new unique id.
    Add todo for multi.
    Return.
  else 
    cmp x0 mid_tag
    b.eq mid_target
    b.lt F(single, low info)
    F(multi, high info)

Which brings out one more piece of the puzzle: for the 'single' case, you have
to provide the label, the branch instruction mnemonic comes from the parent.
    
Maybe split into two separate functions:
  SINGLE - outputs a label, optionally adds to todo.
  MULTI - outputs instructions.
    
In terms of todo list, we need to keep track of a unique id to use for
internal jumps.

It's a lot of info to manage all at once. We're talking about having a queue
and doing recursive function calls. Any way to do only as a queue?

Queue entry has:
 uid, info

Top level:
 Push (NONE, info)

To process:
 output label
 if single target info
   b.eq target; continue
 cmp x0 mid_tag
 b.eq mid_target
 if single target low info
   b.lt target
 else
   alloc id, push low info.
   b.lt id
   push (NONE, high info)

Make it a stack. This should work. All we need is the helper function to find
the single possible target, if any.
   
Or... instead of needing an explicit stack, use function stack?

Top level:
 F(NONE, info)

F(label, info):
 output label
 if single target info
   b.eq target; return;
 cmp x0 mid_tag
 b.eq mid_target
 if single target low info
   b.lt target
   F (NONE, high info)
 else
   b.lt id
   F (NONE, high info)
   F (id, low info)
 
Yeah. Let's go with that. Just need to pass some shared state about intervals,
targets, etc.
 
---

The results:

time fble-sat < rope_0002.shuffled.cnf: 1m36s ==> 1m36s
time yes | head -n 40000 | fble-md5: 53s ==> 54s

Interesting to note that md5 actually does have a pretty big switch statement
in it, of 25 tags at least. Anyway, no major regression, so I'm happy.

---

Actually, the code generated for those 24 tags doesn't look right to me. Too
many references to the default target. May be worth walking through it to
double check.

Yeah, I'm pretty sure something is wrong. Case should be
 [0, 20], [18:, 19:], :

I expect:
 cmp x0, 18, but getting cmp x0, 19.

Found the issue. Off by one due to high interval being exclusive instead of
inclusive. Should be fixed now. Re-running fble-md5:

time yes | head -n 40000 | fble-md5: 53s ==> 54s ==> 53s.

Good.

---

For the fun of it, let me try another perf deep dive. I'm thinking next thing
to try will be moving profiling and error code out of line of the main code,
in the hopes it avoids spoiling the instruction cache and is easier to
branch predict. But first, a deep dive...

Flat Profile by Self Time
-------------------------
    %     self  block
 7.62     3714 _Run.0x28c3e078._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_[00
 6.44     3138  MarkRef[002e]
 6.04     2944  FbleRetainHeapObject[0022]
 6.00     2923  FbleThreadTailCall[003d]
 4.76     2321  FbleStrictValue[0039]
 4.58     2232  FbleThreadCall[0001]
 4.53     2206  FbleUnionValueTag[002f]
 3.93     1914  FbleReleaseHeapObject[0032]
 3.81     1857  FbleThreadSample[0048]
 3.46     1688  IncrGc.part.0[001b]
 3.39     1654  Refs[0043]

Let's see where that 7.62% is going to.s

perf annotate -n _Run.0x28c3e078._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_

3714 samples total.

* 398 of that in function preamble.
* 344 of that in function post-amble.
* bl to FbleThreadSample, loading from args array, checking error condition.

Hey, you know what's crazy? Perf annotate is showing .fble code. That's
awesome. Anyway...

It feels like the biggest hits are those that come after long jumps. And load
instructions. Maybe not surprising. Presumably it's all related to memory
operations and what's in the cache or not.

Is there a way to perf sample by cache miss event instead of frequency
sampling? I wonder if I can get an annotation for that and align it.

There are lots of possible events, maybe:
  cache-misses
  major-faults
  minor-faults
  page-faults

Let me try page-faults to start?

perf record -e page-faults -d ...

Hmm... It wants a frequency. I wonder what it's counting then?

Interesting. Looks like it works like I think. Instead of sampling on
frequency, it counts events.

All the page-faults are happening in malloc. That's not surprising. Let's try
cache-misses instead.

Most of those are in RetainValue and MarkRef. Not surprisingly I guess.

I'm not sure what to make of that though.

The frequency must be in terms of events, not time. So frequency of 500
means sample once every, uh... ??

Anyway, biggest thing in Ord is just that it's a function that's being called
a whole lot, and there is overhead to the function call.

So, either figure out a way to reduce that overhead, or call the function
less? Like, seems like function inlining could help a bunch here? Or a calling
convention to avoid having to save so many values to the stack?

Registers we use that we need to therefore save before using:
  heap, thread, locals, args, statics, profile_base_id, scratch_0, scratch_1

Hmm... What if I turned tail recursion into a while loop? No need to save and
restore the registers every iteration.

(Ordering@) { Ordering@; } RoundLt = (Ordering@ x) {
  x.?(eq: Lt, : x);
};

(Ordering@) { Ordering@; } RoundGt = (Ordering@ x) {
  x.?(eq: Gt, : x);
};

Ord@<IntP@> Ord = (IntP@ a, IntP@ b) {
  a.?(1: b.?(1: Eq, 2p0: Lt, 2p1: Lt),
      2p0: b.?(1: Gt, 2p0: Ord(a.2p0, b.2p0), 2p1: RoundLt(Ord(a.2p0, b.2p1))),
      2p1: b.?(1: Gt, 2p0: RoundGt(Ord(a.2p1, b.2p0)), 2p1: Ord(a.2p1, b.2p1)));
};

It's not all tail recursion. But could we write by removing function calls?
Like, if you were to write it in C?

Let's say we didn't have RoundLt or RoundGt. Then it's straight tail
recursion.

  while (true) {
    if a == 1:
      return b.?(1: Eq, 2p0: Lt, 2p1: Lt);
    if b == 1:
      return a.?(1: Eq, 2p0: Gt, 2p1: Gt);
    a = a.2px;
    b = b.2px;
  }

To add the rounding bit? Keep track of a function, and how things are applied.

By default: Id.  RoundGt turns eq to gt. Round lt turns eq to lt. So
composition looks like:

Id . RoundGt = RoundGt
RoundGt . Id = RoundGt
Id . RoundLt = RoundLt
RoundLt . Id = RoundLt

RoundGt . RoundLt = RoundLt
RoundLt . RoundGt = RoundGt

So, we can keep track like that, collapse things down. In other words, yes, I
could totally write this is a single function call that is a while loop.

That's much better than function inlining, because we avoid code duplication
and we get 'full' inlining in the sense of never having to call a function
once we are inside.

A couple bits here:
1. Transforming it to pure tail calls via clever manipulation of RoundGt,
RoundLt.
2. Given a pure tail call, that is, a tail call to the same function we are
in, you could jump past the pre-amble and skip the overhead of pre and post
ambles.

---

Some things to think about more generally:
* Replace per-instruction sampling with sampling on enter. Then bypass all of
  profiling with a branch instruction based on value of profile pointer.
* Move profiling and error code out of the main path.
* Can we have a special calling convention for run functions, given
  FbleThreadCall is the only place we ever invoke them?
  - With the idea that we change to all registers caller saved, and avoid any
    caller save registers in the run functions.

Let me read about calling conventions. Does gcc have a way to specify an
alternative calling convention to use? What are the options? Any thing
equivalent to "caller save all"?

Looks like intel has some different options, documented under "Function
Attributes": fastcall, thiscall. But I don't see anything for aarch64.

Oh well. Probably a bad idea to be that hacky anyway.

---

Moving 'DoAbort' code out of mainline path:

time fble-sat < rope_0002.shuffled.cnf: 1m36s ==> 1m35s
time yes | head -n 40000 | fble-md5: 53s ==> 51s

Very modest improvement, but I'll take it.

---

Turns out we weren't using R_SCRATCH_1 for anything. Let's see what we get
from removing that.

time fble-sat < rope_0002.shuffled.cnf: 1m35s ==> 1m33s
time yes | head -n 40000 | fble-md5: 51s ==> 51s

---

Anyway, clearly the next thing I want to do is consolidate and outline the
profiling calls. We're spending a lot of instructions on that, even when
profiling is disabled.

Questions:
* Can I move profiling calls out of FbleThreadCall?
* Can I do samples per block rather than per instruction?
 - Is there any case in which instructions belong to some specific profile
   block aren't always going to be executed once we enter that profile block?

Because if we have those, then we can move everything behind a flag. Use an
R_PROFILE to know if we should enable profiling or not, and if so, jump to a
separate profiling section of the code.

---

Let's not make this hard. First step: change profile sampling to be an
explicit profile op instead of done for each instruction. And because it's too
painful to me to add a sample profile op to each instruction, when I add
explicit sampling profile ops, coalesces them into as few as possible.

Let me review some code to see if I notice anything obvious about when it's
safe to group profiling things together.

* Instruction with profile enter op gets one sample.
* Straight line code can all be coalesced.
* We have a fair number of profile enter/exit, probably from let
  statements. We'll have to sample these separately.
* Note: some functions don't have any profile enter/exit, because those are
  taken care of by FbleThreadCall. 
* return statements shouldn't have profiling follow them.
* Might be nice to do sample as part of:
  - Explicit sample for function entrance.
  - Profile Enter.
  - Profile Replace.
* The 'goto' instruction following a union select branch after profile exit
  should be attributed to the block outside the branch. Assuming we care about
  counting that as part of the profile.

I think this is totally doable. We keep track of blocks. Any time we push a
block, we have an associated profiling instruction. Any an instruction,
increment the count of whatever the current block's profiling instruction is.

I'll need to restructure things a little bit to keep whatever pointers we want
as part of the stack, but otherwise, easy.

In summary:
* Add an FBLE_PROFILE_SAMPLE_OP.
* Add a count field to profile ops for SAMPLE, ENTER, REPLACE.
* Change interpretation of ENTER, REPLACE to always sample.
* Track profile op as part of block stack.
* Any time we append an instruction, increment the count of the top profile
  op.

Decide what I want to do in terms of sampling a block of instructions at once.
There is some math.

For example:

if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1

Is different from

if rand%1024 == 0 sample 5

Is different from
 
if rand%1024 < 5 sample 1

---

I claim we can associate all profiling ops with one of two places:
1. The beginning of a function.
2. The beginning of a union select branch body.

How about I switch to an explicit profile instruction, and emit one for each
of those places. Or... make it part of FbleCode and FbleUnionSelectInstr
directly?

It may be more general and easy to work with as an explicit profile
instruction that we emit.

---

In terms of 

if rand%1024 == 0 sample 1
if rand%1024 == 0 sample 1

versus

if rand%1024 == 0 sample 2

versus

if rand%1024 < 2 sample 1

The property we want ideally is that
  F(a) + F(b) = F(a + b).

We'll only get that with the first case.

But the expensive bit is the sampling, not the rand part. So I vote we do as
follows:

F(a):
  s = 0
  for (int i = 0; i < a; ++i)
    if rand%1024 == 0
      s++
  if s > 0
    sample s

Nice properties:
* Single sample per call of F.
* F is linear, so doesn't matter how we group things.

I guess the question is: can we come up with a decent appropriation of F that
is O(1) in the size of a instead of O(N) in the size of a?

---

Updated thoughts:
1. Make sampling explicit profile op.
 - So that compiler can optimize how profiling is done and no special cases
   are needed for that versus other profiling ops in the back ends.
2. Coalesce profiling ops where possible.
 - Start of function, start of branch, end of union select all we need?
 - Maintain order of ops. Don't reorder sample ops: doesn't seem worth the
   added effort even though sampling can be reordered in ways enter/exit
   can't.
3. Don't make a separate instruction for profile. There are some potential
   benefits to it: avoid the need for nop, avoid the need to check profiling
   at every instruction in the interpreter. But that feels too invasive to the
   program. Like, if you want to count number of instructions, that shouldn't
   change with profiling enabled/disabled.

Let me start with (1). I think auto-add a sample op every time we add an
instruction. That's the easiest way.

Interesting note: making sample ops explicit changes where the samples are
applied to, because the sample comes after other profile ops rather than
before. I think it's better this way.

---

For coalescing, we'll need to reset after function calls, because those have
profiling side effects.

---

Now that we coalesce profiling ops as much as I currently desire, let's take
advantage of that for code gen. In all three backends (interpreter, c,
aarch64), the idea is to jump over all profiling ops if profiling is disabled.

I would love if I could use R_PROFILE_BASE_ID to quickly detect if profiling
is enabled or not. We already pass that around everywhere. Normal value means
profiling is enabled. If profiling is disabled, we don't need that value at
all.

The question is, can I use 0 as the 'disabled' value? That would be super nice
for aarch64, because we could use cbz/cbnz to jump past, instead of needing
a cmp followed by a branch. Or, is there some magic instruction to test for -1
maybe?

Maybe TBNZ, TBZ, which test a bit of a register? Syntax is:

  tbz x20, #4, foo

To test bit 4 of reg x20 and jump to foo.

So, we could test for -1 by testing bit 63, right? Cool.

Now, what can profile_base_id be normally?

Uh, profile_block_offset is the new name?

I'm not sure this is okay. Because we need profile_block_offset to be non-zero
if we want to switch between profiling enabled/disabled with the same
allocated objects. Is that possible?

Actually, -1 may be safe to use. If profiling is disabled at link time... It's
subtle. Using -1 means any functions we allocate without profiling enabled
would be broken if you enable profiling. That sounds dangerous.

Can we just use thread->profile for the time being? Except that's internal
state. We don't have access to that.

How about: add an additional argument to the run function which is a bool to
say if profiling is enabled or not.

---

Now it's time to see what benefit we got. I expect performance improvements
across the board. I don't have all the baselines you could want, but the
sampling of what we have should be nice.

time fble-sat < rope_0002.shuffled.cnf: 1m33s ==> 1m26s
time yes | head -n 40000 | fble-md5: 51s ==> 45s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m39s ==> 0m35s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m41s ==> 0m38s
time fble-sat --profile -- < rope_0002.shuffled.cnf: 4m00 ==> 4m02

Reviewing ideal runtime without any profiling or error related code, that was
down to 1m22 seconds as opposed to 1m26s. Not bad.

