Fble DataFlow
=============
The biggest problem with the fble language today is that it is slow to run
programs. This is partially because we haven't done the work yet of all the
fancy optimizations, but it's also partially because of how the language is
defined.

In particular, in fble you don't distinguish between values and pointers. The
stack and heap are used implicitly as a globally contentious resources the
programmer has very little control over. It's not possible to describe or have
control over the flow of data, which is what really matters in practice for
super high performance computation.

One could argue this is why we think of C and C++ as fast, but Java, Haskell,
and fble as slow: C and C++ at least let you know about pointers, which give
you some control over data flow.

I still believe fble is a great language for describing information. It's
support for programming language abstractions is wonderful. The idea is, could
we add a data flow construct into the fble language? That way, you get the
benefit of all the abstractions of fble for describing a computation, and you
get the full control of a data flow graph for doing fast computation.

Advantages of data flow for performance:
* We restrict data to non-recursive unions and structs. This means we don't
  need a fancy garbage collector at runtime. We know the size of everything.
  We don't have to share anything. It's easy to pack things into bits.
* We don't have function calls at run time.
* We can effectively have mutable data types.
* No need for automatic memory management.
* No need for closures capturing things from local scope.

In other words, it's solves all the performance challenges of fble except for
taking advantage of primitives and light weight multi-threading. Both of
which, hopefully, could be solved in practice by having support for
compilation of some parts of the data flow graph to hardware.

What is a data flow graph?

We have types: non-recursive struct and union types just like in fble.
  Though we may want to distinguish between unit and other struct types.
We have edges/wires: each edge has a type. This represents data.
We have nodes/modules: with multiple input and output ports connected to wires.

The nodes/modules are primitive. No need for hierarchy in the data flow graph
because we can push all our abstraction to fble. Unless we want some form of
profiling or named hierarchy for debugging after the fact, which should be
easy enough to add.

I think it's useful to model wires as single element fifos. A module takes
values off of input fifos and puts values on output fifos. In general the
behavior may not be deterministic.

Primitive Modules:

Struct
  Takes n input ports, has 1 output port. Waits for something to be available
  on all input ports, then bundles them together into a struct and puts them
  onto the output.

Const
  A special case of Struct for the unit type. Has 1 input port and 1 output
  port. Waits for a value on the input port, then outputs Unit on the output
  port.

DeStruct
  Takes 1 input port, has n output ports. Waits for something to be available
  on the input port. Then splits it apart into its n fields.

Union
  Adds the given tag to a value to make a union value. 1 input, 1 output.

DeUnion
  1 input, N output. Given a union value, outputs the argument of the union to
  just its corresponding port.
  
Copy
  1 input, N outputs. Copies its input value to each of N output ports.

Merge
  N inputs, 1 output. Copies any of its inputs to the output.
  Nondeterministic.

Sink
  1 input, no outputs. Consumes its input.


Initially say the data flow graph starts with all wires empty. You can
initialize it with a reset if you like by sending an external 'reset' signal
to a bunch of Const modules inside the network.

For software, it's also useful to be able to terminate a network. This can be
done by augmenting each wire with a finish signal that propagates out to all
the modules when set.

---

Stepping back. The goal here is to describe a computation that we can make run
really fast, in both hardware and software.

I don't see how we can make dataflow fast in software. I think we lose
high level information about scheduling that is important for efficient
software.

I think better would be to describe the computation as close to fble as
possible, but with the key limitations needed to be fast. Namely: all memory
is statically allocated.

I assume if we can describe our computation that way, it will not be hard to
compile to efficient hardware.

The things we want to avoid at runtime:
* Heap allocations. Garbage collection.
  This implies we don't allow recursive data types. That way we have no
  cycles, no need for a complicated gc. I would go so far as to suggest we
  also don't need reference counting: data movement is expensive in reality,
  so copying a value should be expensive in the code.
 
  Ideally we pack data values as tightly as we can into bits and make copies
  as needed.

* No function calls.
  Which would require stack allocations.
  This implies we want to inline all function calls. There's no need to worry
  about the memory cost of inlining, because the entire point is to see what
  the total memory cost of a program is.

  This also implies we don't allow function or proc values at runtime, because
  there's no way we could make use of them.

  Tail recursive function calls are okay, because they don't need any
  additional memory.

I would expect a computation with these limitations to compile down to code
that operates on pure values, does no dynamic memory allocation, and consists
of some mix of straight line code, conditional statements, and while loops. It
should be very easy to generate high performance code for this: translate
types to structs in llvm, temporary variables become global variables, and we
translate the code directly. No GC. No runtime. Super fast.

It would be nice if I could try writing fble program's with these
restrictions, to see how hard it is to write meaningful programs. The compiler
should be able to help. For example, let's say we define a variant of the
function type that is the "high performance" function type. The body of the
function obeys the limitations above.

Syntax could be something like:

 %(Int@, Int@) { ... };
 %(Int@ a, Int@ b) { ... };

The compiler can check the arguments and the body.
You are allowed to use high performance functions in normal functions, but not
the other way around.

Stating the restrictions again:
* Types of args, variables, expressions, etc must be non recursive union and
  struct types.
* Function calls must be to high performance functions.
* Recursive function calls must be (self?) tail recursive.

Are procs allowed?

It seems like we should be able to get and put from external ports. But we
can't fork new threads dynamically. We would have to know all the threads at
the start.

I think we could have a "high performance" proc. There are two variants:
1. The same as a high performance function but with the ability to get and put
from given ports. No new links. No forking.
2. A top level process that allocates a bunch of links statically, then does a
single exec with a bunch of high performance processes. Actually, it might be
okay to do multiple execs, in a non recursive fashion.

In terms of implementation, we would want to know all links statically to turn
them into global variables. But now we need a thread scheduler, which sounds
scary. So we would really want to minimize how often we fork and join and how
many threads there are. My vote to start is to not allow procs.

But maybe it's fine to let high performance funcs be the core kernels of high
performance computations, and wrap everything else in normal computation. In
other words, it would be awesome if we automatically compiled high performance
functions to be high performance (with some overhead for data conversion). As
long as the normal performance stuff is a small enough part of the
computation, it's fine to make it slow.

---

Here's the plan. Add a new type, analogous to process type, which represents
an "inline" type. The operations we support on normal types also work on
inline types, except with the extra restrictions.

So, we have:
inline_struct_type - a non recursive struct type with inline data args.
  This is considered an inline data type.
  Proposed syntax: +`(...)
inline_struct_value_explicit_type
  Same concrete syntax as struct value.
  The type and args must be inline.
inline_struct_value_implicit_type
  Proposed syntax: @`(...)
inline_struct_access
  Same concrete syntax as struct_access.

inline_union_type - a non recursive union type with inline data args.
  This is considered an inline data type.
  Proposed syntax: +`(...)
inline_union_value 
  Same concrete syntax as union_value.
inline_union_access
  Same concrete syntax as union_access.
inline_union_select
  Same concrete syntax as union_select.
 
inline_func_type - a function type with inline data args and inline data
  return type.
  This is not considered an inline data type.
  Proposed syntax: (...)`{...}
inline_func_value
  Enforces that the body only uses legal inline operations. (Some more thought
  needed here about what exactly that means).
  Proposed syntax: (...)`{...}
inline_func_apply
  Same syntax as func apply.

inline_proc_type - a proc type with inline data return type.
  This is not considered in inline data type.
  Proposed syntax: ...`!
inline_proc_eval
  Proposed syntax: $`(...)
inline_link
  This is a normal proc that creates inline get and put ports.
  Proposed syntax: T@ ~` get, put; ...
inline_exec
  Same syntax as normal exec, except restricted to a single argument.

I'm not sure about how to handle polymorphism

run_inline
  Converts an inline object to a normal object.
  Proposed syntax: `(foo)

Uh, so, this is going to be awkward when we start mixing inline data types
with normal computation, like in union select.

I suppose a key question is if we want to think about this as a different kind
of computation, or if we can think of it as a property a computation.

Sigh. Needs more thought.

I guess, if we think of it as a property of computation, we just need a single
extra kind of operation:

  inline <expr>

Which fully inlines an expression, which must be fully inlinable. If it isn't
fully inlinable, hopefully you can get a decent error message about what part
of it isn't inlinable. inline preserves the type of an expression.

I worry that won't give you enough when it comes time to compose inlineable
things. Because what happens if there is some library call that's not
inlineable?

On the other hand, maybe it's better, because it lets you mix non-inlineable
things, as long as they can be computed down to inlinable data values.

In other words, you can inline any struct or union satisfying the inline data
type requirement. You only get into trouble if you try to inline a function or
proc that does unsupported operations.

I'm tempted to try the inline expr approach. I suspect long term it won't work
out, but it's easy to start, it makes inlining orthogonal to the rest of the
language instead of doubling the complexity of the language, it allows for
easy mixing of complex statically elaboratable things in inline computations.

Proposed syntax:  $(...), where we replace proc eval with the syntax !(...).

