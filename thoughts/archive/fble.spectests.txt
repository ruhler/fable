Fble Spec Tests
===============
Question: can we change how we specify fble language tests?

Currently they are specified using tcl with a funny mechanism for specifying
module hierarchy. Question is, could we write .fble files directly, like as a
separate fble library package with no other fble library dependencies? I think
that could be nicer.

We just need a way to indicate the metadata for each test:
 fble-test
 fble-test-compile-error <loc>
 fble-test-runtime-error <loc>
 fble-test-memory-constant
 fble-test-memory-growth

Advantages of what I'm envisioning:
* No need for special tcl syntax for modules.
* We know the name of the top level test module.
* We write .fble files directly, so we get syntax highlighting and direct line
  numbers for errors.
* We avoid an extra extraction step to get .fble files, which should simplify
  the build system notably.
* Hopefully more easily reused by hypothetical other fble implementations.
* We can compile Nat.fble just once and easily reuse it.

All I have to do is figure out a reasonable way to do the metadata.

Seems something like checktest from llvm, but I don't really like that, and I
certainly don't want a dependency on llvm.

Most natural thing to do is add a comment to the code. Some syntax tests will
be out of reach, like the ones involving comments, but that's such a basic
thing I don't think we need to test that using a separate test.

Note that the top level for the test case may be different from where the
error occurs. I suggest putting a metadata line only at the top level of the
test case?

So it's pretty straight forward, right? First line of the file for a test case
should be:

# fble-test-* <loc>

Or, maybe, because we aren't using tcl, we can use different names. Like:

# TEST
# TEST_COMPILE_ERROR <..>
# TEST_RUNTIME_ERROR <..>
# TEST_MEMORY_CONSTANT
# TEST_MEMORY_GROWTH

Or, like, have assertions? I dunno. I don't think it's hard, it's just
arbitrary.

We could do double comment:

## NO_ERROR
## COMPILE_ERROR 8:3
## RUNTIME_ERROR 8:3
## MEMORY_CONSTANT
## MEMORY_GROWTH

If it isn't annotated, it isn't as separate test case.

How will the test runner work? How will it integrate with the build system
where we want to compile?

We could run a single grep command over all modules in the directory. That
gives us a mapping from filename (hence module name) to test case. We can then
do a loop over those lines. Maybe we can generate that using a build rule and
have build.ninja depend on it and build.ninja.tcl read it? With a slight
bootstrapping process to generate it initially I suppose? That's unnecessary
though.

Then we can parse the entry easy enough in tcl.

So, I just need some metadata easy to locate with grep, and easy to parse from
tcl. Let's explore options.

For prefix, we want something that is a comment and is unlikely to occur by
accident. It may be nice if it indicates to readers of the code what it is,
instead of being some magic. If we have text, we don't need magic symbols
aside from the comment character. I like 'test' and 'fble' in the name.

# FBLE-TEST no-error 
# FBLE-TEST compile-error 8:3
# FBLE-TEST runtime-error 8:3
# FBLE-TEST memory-constant
# FBLE-TEST memory-growth

Say it has to be the first line in the file, because why not? Or is that too
picky and too easy to accidentally not be applied? No. I think anywhere should
be okay. Allow any amount of whitespace between tokens. How to prevent
accidental uses of 'FBLE-TEST' in a document? I think we want some special
symbols for it.

Okay, how about this, a line of the form:

# @@fble-test@@ <type> <msg>

<type> is one of:
  no-error
  compile-error
  runtime-error
  memory-constant
  memory-growth

<msg> is an arbitrary string expected to be included in the error message,
primarily used to identify the location of the error.

There should be at most one @@fble-test@@ line per file. I'm not sure who
enforces that though.

Clearly it has some special meaning. Simple to detect with grep. Clearly
related to fble and testing. Simple to parse. I say go with it. No need for a
name, the current module path should be fine?

Good. What do you think? Shall I try doing this cleanup?

---

What are the next steps here? I need to come up with a plan for how to
implement this.

So we have the .fble code already extracted.
We get a list of tests from build.ninja.tcl using glob and/or grep.
We parse the test type from within build.ninja.tcl and call the appropriate
spec-test-compile, fble-test, fble-test-compile-error, etc. functions.

I'll want to migrate slowly to the new scheme. So duplicate and modify in
build.ninja.tcl to start. Start with just a handful of tests, get that
working, and slowly migrate.

What directory structure do I want for the spec tests? Put them in spec
directory maybe? spec/SpecTests/... I would say the tests are part of the
spec. 'lang' versus 'spec'? I think 'spec' is more specific. Do we want an
'fble' prefix? Or is everything okay to be fble based now?

Let's try 'spec/SpecTests', where spec is the -I value and tests go under
/SpecTests/...%

Immediate next steps:
* Port the langs/fble/0... tests to new format.
* Set up build.ninja.tcl infra for running tests in the new format.

---

The easy way to build the new test format is:
* For every file under SpecTests directory, create a spec test rule that calls
  a script to run the file. The script reads the metadata, verifies it is a
  test (trivially passing otherwise), and executes the commands necessary to
  run the test.

That way we don't have to read the contents of any of the spec tests to
generate the build.ninja. The downside is we don't know at build time whether
the test depends on fble-test, fble-compile, fble-mem-test, etc, so we'll end
up adding a dependency on all of those, which could trigger unnecessary test
reruns in some (rare?) cases.

The slightly harder way is to parse the test file in build.tcl and generate
rules depending on the test. Let's start with the easy way. Once we have the
easy way, we can decide if we want to migrate to the hard way, which needs all
the same code, just a bit more and in different places.

First steps for implementing script to execute:
* Pass an include directory and a module path? Or include directory and .fble
  file name. Either way, want the info necessary for the .fble file name, the
  include directory, and the module path.
* Read the file, search for and parse the @@fble-test@@ metadata.

---

Next step, let's consider the case of no-error.
* No need to extract anything.
* We want to compile .fble -> .o all modules the test depends on.
 - How do we know which modules the test depends on?
* We want to compile the main .fble -> .o with --main.
* We want to generate a binary.
 - Can we call ld directly, or do we need gcc flags?
 - How does that interact with code coverage?
* We want to run fble-disassemble, for the fun of it.
* We want to run interpreter with --profile (can ignore result)
* We want to run compiled executable.
* We want to catch any errors and report the test file.

Questions:
* How to know which modules to compile?
  - Maybe run fble-deps to get that info?
  - Can we avoid recompiling, e.g. Nat.o multiple times?
  - Or should we separate compilable from non-compilable, and just compile all
    the compilable modules up front in one big go?
* Where do we put output files?
  - tempdir? out dir?
* How to link compiled executable / how does that work with coverage?

I should be able to answer the question about linking on the existing spectest
infrastructure. Maybe let's start there.

Looks like directly invoking 'ld' doesn't work great, because it's missing
some standard options and libraries that gcc adds when it calls 'ld'. For
example, _start as entry point and whatever library implements 'rand'.

Does that suggest we want to reuse the build options in build.ninja for
building these things? In which case, maybe I really want to parse the spec
test .fble files and generate build rules from those rather than doing a
one-off test run directly from the .fble?

---

Here's the issue:
* Unlike the old approach to spec tests, the new approach does not explicitly
  list the modules that a test depends on.
* Using fble-dep to get that information is feasible, but suggests we can't do
  it as part of generating the build.ninja file, because we need build.ninja
  to build fble-dep first.
* The fact that ld isn't trivial to call directly, and that we ideally would
  like to reuse compilation of Nat.fble, suggests we would prefer to have
  build.ninja in charge of building for tests.

So we are stuck in a knot. It's a question of how I want to break the knot.
Some possibilities:
* Come up with some way to make dependencies explicit in test cases. For
  example, a test /SpecTest/.../Foo.fble depends on exactly Foo.fble and
  everything in the directory Foo/, plus Nat.o for memory tests.
* Don't worry about recompiling Nat.fble or factoring out common code for
  linking fble-based executables.

As far as building with coverage goes, I think the current approach is the
best we can do:
* Run fble-test.cov, fble-compile.cov tools.
* Link tests against non-coverage libfble.a.

Because I assume gcc needs to generate special code to support coverage, and
when we are compiling fble code, we bypass gcc and whatever special code you
need. In other words, compiled fble code does not support code coverage.

I kind of wish there was an alternative to gcov we could use for code
coverage. Something perf based possibly? I feel like perf would require
sampling at every instruction, which is way overkill.

---

Solution for build.ninja dependency on fble-deps: see if I can set up a
subninja file that depends on fble-dep. I feel like ninja should be able to
read the initial build.ninja file, use it to build fble-deps, use that to
generate the subninja file, then reload the subninja file, start over again,
see fble-deps and subninja are up to date, and execute those rules.

Solution for knowing whether we should be able to compile a test .fble or not:
any .fble file that can't compile should be marked with @@fble-test@@
compile-error. Even if it's not the main test entry, for example, if we want
to test reference to a module that doesn't compile. That's a rare case, so
it's fine to have an extra @@fble-test@@ compile error on the referenced
module saying that compilation would fail from the top level module too.

So, I guess my vote is the following:
* Use fble-deps to figure out dependencies we need to compile and link
  together for a test. We want that regardless of whether we build via ninja
  or not.
* Start by using separate test script that duplicates compilations and gcc
  command lines. Generate intermediate files to out/<path>/ directory. Boot
  strap that way.
* If desired, switch to hack for generating subninja dynamically.

Next steps:
1. Add support for interpreted test cases via script.
2. Set up spec/build.ninja to run test cases.
3. Add support for compiled test cases.
  - Figure out how to know what to compile and link using fble-deps.
4. Migrate rest of test cases.
5. Optionally figure out hack for generating subninja dynamically.

---

Note: One thing nice about having the .fble be the source for the test is we
don't have to output a separate error message to the user to direct them to
the .tcl file.

Err, actually, that's not true. We still need to output an error message
location in case of unexpected success, or mem-test. Ideally if the test
fails, we output an error message of the @@fble-test@@ line.

---

Okay, we have new spec tests set up for non-compilation cases. I'm actually
fine not writing out artifacts of the test, just be silent in case of passed.
Probably saves some IO cost?

Next step is the tricky part: how to support compiled tests. Components:
* Generate a .d file with fble-deps. We'll actually want to use this as part
  of the test output in spec/build.ninja because we need these extra build
  dependencies in ninja.
* Extract the list of .fble files from fble deps. Compile each of those with
  fble-compile.cov to a .s file somewhere (temp file?)
* Compile all the .s files to .o files somewhere, handling the main .fble file
  slightly specially.
* Link all the .o files to an executable somewhere.
* Then run the test.
* Then delete the temporary files?

Support for temporary files/directories via tcl?
* We can stream the .s directly to .o. No need for intermediate .s to be
  saved.
* file tempfile has a way to create a temporary file, but it opens the file
  for us, which doesn't work great with the assembler? What if we don't pass
  -o to as, does it stream it out? That would be okay then. No. 'as' without
  -o creates an a.out file. Maybe that's okay.

Idea: use tcl's file tempfile, create named files for .o and the compiled
executable. Pass the filenames to as and gcc. Run the stuff, then as cleanup
regardless of error condition, delete all the temp files.

Better yet, there is a linux command 'mktemp', that can create a temporary
directory. Make a temporary directory, generate whatever I want into there,
then delete the directory recursively when the test is done running. Easier to
clean up 

---

One idea: we could make fble-deps work regardless of whether the .fble can be
compiled. Just ignore any errors and write down as many dependencies as we
see. That way we could do fble-deps as part of build by running it on all
.fble files in spec directory. We need to add these missing dependencies
regardless, even before adding support for running compiled tests.

It shouldn't be hard to parse an fble-deps output file. For example:

out/pkgs/core/Core/Stdio.fble.d: pkgs/core/Core/Stdio.fble \
 pkgs/core/Core/Bool.fble pkgs/core/Core/Unit.fble pkgs/core/Core/List.fble \
 pkgs/core/Core/Maybe.fble pkgs/core/Core/Process.fble \
 pkgs/core/Core/String.fble pkgs/core/Core/Char.fble

Split by whitespace, remove '\' entries, remove first entry, then for each
.fble file, trim the prefix directory.

For it to work properly, in theory, we do need fble-deps to work on .fble
files with compilation errors. Otherwise we might miss important dependencies
for compile-error test cases.

My vote is that fble-deps can handle anything, it ignores errors while
loading, just puts out whatever it has seen as a dependency so far. The only
slightly annoying thing is how to handle stderr from parser error messages.
From the point of view of fble-deps as a tool, it makes sense to propagate
those to the user via stderr. But we want the option to ignore those, which is
easy enough with 2>/dev/null. What about exit code? I vote, exit success as
long as we were able to write a meaningful fble-deps file to output.

That sounds like a reasonable next step to me. Add support for this slightly
unusual fble-deps use case of deps for invalid .fble files.

Next question is: should we run fble-deps as part of build or as part of spec
test? We need it for build certainly. I assume eventually I'll want to
move most of spec test logic into build. So it doesn't really matter to get
started. Let's do it at build time blindly, and for now pass the .d file to
fble-spec-test.tcl for it to parse.

---

I think it will work well to create a temp directory that we can put
intermediate files in, then delete non-conditionally at the end of the
fble-spec-test script. Parsing the .d file is not hard. So, all we need to do
next is:
* create a tempdir at the start of the test. Clean it up at the end of the
  test.
* invoke the compiler on each of the .fble files.
  Pass in the FbleFooMain function to the compile procedure.
* invoke gcc to link them together.
* Run the compiled binary where and as appropriate.

And then we're all set!

---

On further thought, let me use a directory in the 'out' directory instead of a
temporary directory. That way it will be easier to reproduce and run gdb on
failing compile tests. To make sense of build versus fble-spec-test.tcl, move
spec/build.tcl to fble/test/spec.build.tcl or some such, because it fble/
should be downstream of spec/ and running the spec tests are really about
testing fble. If the script and build file are in the same directory, we can
think of them as one module together.

For directory name, let's use out/spec/$FBLE as a directory.
For module names, substitute '/' in the file path for '-'. We want to compile
relative to the test, not the reference module, because multiple different
tests may reference the same module. In the future, if we move everything to
build, then we can share compiled modules across tests. For now, let's just
recompile shared things on a per-test basis.

---

Cool! It's all wired up with separate build and run scripts. Next step is to
port all the rest of the test cases over to the new framework and remove the
old test cases. I don't see a compelling reason to push the run script into
the build script at the moment. We can do that later if a compelling reason
comes up.

---

Here's something annoying. Can I have '.' in a module path? As in, 2.2-Kinds
for the name of the tests corresponding to section 2.2 in the specification?
Because 02_2_Kinds is much harder to relate back to section 2.2 of the
specification. 2.2_Kinds ... 2.2-Kinds is kind of what I rather have. That
language allows that, right, as long as I quote the words? Let me try it at
least.

So, input path is /SpecTests/2.2-Kinds/Equals/KindsMatch%.

In theory I should be able to change it to:
  /SpecTests/'2.2-Kinds'/Equals/KindsMatch%

Or, I could just relax the module path parser to accept the original. Is there
anything wrong with that approach?

The concern is that we would then have two different syntaxes for module
paths. Let me try injecting single quotes and see if that fixes the issue.
Yeah. If we put in single quotes it is fine. That's just annoying to remember
to do from everywhere when we convert a file path to a module path.

And it's even a little uglier, because we have to protect the single quotes if
passing via shell, and different ways of using the module path require
different protection.

Let me limit the introduction of quotes to spec tests for now.

I'm thinking it's probably reasonable to allow funny characters in module
paths passed to FbleParseModulePath, because we know what we are trying to
parse, and you can't really embed a / in a module path word in practice
because there's no way to represent the corresponding .fble file in the file
system. In other words, to parse a module path strip leading '/' and trailing
'%' and split by '/'.

I guess unless we want to allow users to pass quoted module paths too... Sigh.
Let's keep it the same for now. This is really more an issue of the build
system not properly mapping file paths to module paths.

---

One slightly annoying thing about the new format is for test spanning multiple
modules, it's slightly harder to get the gist of the overall test, and the
long module paths are slightly annoying. Oh well. I like the clean up. Lots of
benefits I think, and only the slight annoyance.

---

Cleanup of fble-test to always return error code in case of error, and check
it's the right one in the test runner.

Start with compile-error case. Goal is...
* Execute fble-test
* Capture exit code and stderr.
* Check exit code, check stderr.
* If okay, good. Otherwise, give an error and print stderr.

What exec will let us do:
* 2>@1 to redirect standard error to the output of the command.
* 

---

Revisiting after a while. Question about expect failure case, and what, if
any, cleanup we want to do of tests.tcl.

Thoughts on expect failure:
* It would be nice to have, rather than have to run all tests and skip over
  any that are failing.
* We want a way to specify the original test and the currently failing
  message. Note that we can already do this. We can already write a test for a
  failing case or a passing case.
* Really what's important about expected failure versus a normal test is that
  it's visible somewhere as a thing to fix. So, like, in the final summary
  line. Or as an 'X' that is pretty visible.

Proposed syntax:
  @@fble-test-xfail@@ ...

Same syntax as @@fble-test@@, same subcommands supported, but if this is
present, then the original @@fble-test@@ line is ignored and this is noted as
an expected failure test.

Alternatives:
* Comment out the original @@fble-test@@, add a new @@fble-test@@ with current
  behavior, and use some other method to indicate it is expected failure?

Oh. I remember now. Conceptually it doesn't make sense to mark expected
failure in the test, because it's the implementation where the failure is
expected, not the test case itself. That suggests we would need to store the
info elsewhere. Like, maybe keep a list of expected failures somewhere? With
the name of the test and an @@fble-test@@ line describing the current failure
with the test?

Note: only the first @@fble-test@@ line in the file has effect in the current
implementation.
  
Maybe a practical easy approach to take would be, add an @@fble-test-xfail@@
line to the file. It treats it as a test case, but you give the known failure
message. Then it records it as xfail somewhere. Who cares if it doesn't make
sense conceptually? It's useful for development purposes. Or, just ,
@@fble-xfail@@, which is shorter maybe. Cool. Sounds good to me. Worry about
implementation later.

That brings us to our next question: should we summarize tests? What, and how?

I think having the test failure be a build failure works great. Worst case I
can do build all to see all things.

Having a test summary line at the end is sort of nice, but honestly, not very
useful. It doesn't even print if there's nothing to be done. I don't know the
number, so it won't help me know if I added a test case successfully or not.

---

There are three options for test
1. build failure yes or no.
2. support expected failure yes or no.
3. show status at end yes or no.

Where we have things like:
  no_build_failure ==> show_status
  expected_failure ==> show_status

From a users point of view, expected_failure and show_status don't hurt to
have. The real question is build failure or not. It could hurt to have to wait
for all the tests to finish before seeing that one of them failed. And you can
always do -k0 in ninja if you want that behavior. In which case, status would
be nice. But as currently implemented, -k0 doesn't give you status.

Implementation wise, note that we have spec tests as different kinds of tests
from other tests. Currently it's the build system that outputs PASSED for
passing tests. We don't ever output FAILED for a failing test, for example.

Taking a step back, how does something like dejagnu work in terms of reporting
test results?

The test runner outputs a .sum file, with PASS, XPASS, FAIL, or XFAIL for each
test, and then a summary line with counts at the bottom. It outputs a .log
file with detailed test output.

The unit test protocol is:
* output NOTE: ...
* output PASSED, FAILED, XPASSED, XFAILED, UNTESTED, UNRESOLVED followed by
  test name. This allows multiple tests in the same run I guess.

I suppose the question is, is there a nice way we could support both spec
tests and general ninja tests together in one common format? And have it all
work together? Maybe we could even support fble program test cases that way
too?

I certainly want to take advantage of ninja to only run a test if necessary.

What I want is some generic API for tests. You run a command that is a test,
or a set of tests. It has a way of indicating PASS, FAIL, XFAIL, possibly for
a number of tests. We have a way to run such a program and summarize the
results. For build, we run a bunch of separate programs, concatenate the
results together, and get the overall test summary. But allow individual tests
to fail.

We have output and exit status. Exit status would be a summary status, 0 for
all passed, non-zero for anything else. In build, it's up to us to decide if
we want non-zero exit status to stop build or not. Perhaps different items we
choose differently.

Interesting thought. Interesting idea. The high level thinking is:
* Come up with an output format test protocol that allows multiple tests in a
  single command run.
* exit status is summary of output results: 0 for all good, non-zero for some
  issue.
* Build builds fragments of outputs that we can stitch together at the end.
* We can decide if we want to treat exit 0 as build failure or not per
  fragment.

Is there an existing test standard, like dejagnu, that it would make sense to
reuse?

---

Here's the plan:
* Reuse dejagnu for my tests. So, output PASSED: ..., XFAIL: ..., or FAIL: ...
  as output to indicate test status.
* To support composition, do one of:
 A. See if dejagnu has support for test suites.
 B. Add a special TESTSUITE: tag or some other enter/exit for scoping.
 C. Pass the path to the test suite as input and have it auto add that.
* Update fble tests to have a dejagnu output format.
* Update spec tests to output dejagnu format. Add support for xfail.
* Update other misc tests we have in build.tcl files to use dejagnu format.
* Output of a build.tcl test is written to a file.
* Write a script to read a test output file and convert it into pass/fail
  and/or summary status.
* Choose whether I want to convert to pass/fail at individual build.tcl test
  level, or if we want to cat all the test results together and have one file
  summary thing.

It will be fun to try, fun to learn, support things generally, support status
lines, support expected failures. Basically gives me everything and all
options in a clean way, even if I don't need the full generality.

Next step, then, is to play around with dejagnu to better understand it. And
in particular, see about test suite options.

Reading about dejagnu:
* Seems like I really want to use dejagnu unit testing format. Not the rest of
  the framework in general.
* I don't see anything interesting for test suites or composition of tests.

Can we find some other standardish test format?

I don't see anything obvious. It sounds simple enough. Can I write my own
proposal and go with that?

Goals:
* Tests can pass, fail, or expected fail.
* A test is a program that runs and uses its output and/or exit status to
  indicate test state.
* Test results can be saved to a file and replayed.
* Test results can be composed together into test suites, where a test suite
  is itself a test.
* Tests are identified by name or other uniquely identifiable information that
  can be used to locate the test. Let's say it's a name.

For the purpose of composition, then, we have:
* A leaf node is a test name + status, where status is PASS, FAIL, or XFAIL.
  We also can have any output that would be useful to debug a test, or just
  general info.
* A general test is a list of test results, possibly organized by test suite.

It would be great if you could concatenate the output of a bunch of tests
together into a suite. It would be nice, I think, if you could see what test
is running as it progresses. So, output the name first, then any log/debug
info, and then a status at the end.

I'm not going to worry about compatibility with dejagnu I don't think.

Proposal:
* @@TEST <name>
  Indicates the start of a test with the given name. name is an arbitrary
  string.
* @@PASS, @@FAIL, @@XFAIL
  Indicates whether the previously named test passes or fails.
  If no status is indicated, it is treated as a failure.
  If multiple status are indicated, it is treated as a failure.
* @@SUITE <name>
  Indicates the start of a test suite.
* @@END
  Indicates the end of a test suite.

We can debate what syntax we want to use to make the test keywords stand out,
if you don't like @@.

Exit status is not meaningful, though if the test ends prematurely...

What if a test suite ends prematurely and there is no @@END? How do we
distinguish between a nested test suite versus an adjacent test suite?

The only purpose of @@SUITE is to provide a common prefix...

Next proposal:
* Pass the test suite name to whoever runs the test. Make sure whenever you
  run a test you take a test suite as input, your job is to use that as a
  prefix to your test names.
* This means we could potentially be dejagnu unit test format compatible. You
  can concatenate everything. If we are dejagnu compatible, how do we show
  what test is currently running before showing the result? Do I need that?
  Not for status results, unless we want a way to say a test failed instead of
  not running it at all, which would be nice...

* TEST: <name>
  Indicates the start of a test.
* PASSED: <name>, FAILED: <name>, XFAILED: <name>
  Indicates status of the most recent test.
  If no status is indicated, it is treated as a failure.
  If multiple status are indicated, it is treated as a failure.

Note: we want <name> with the test status so it's easy to confirm which test
passed or failed. We want name before that too, so we know what test is
running and we can know it failed if we don't get a test status from it.

I think it's important to specify the test name at the beginning of a test
case. We are eliminating hierarchy in this proposal. Or rather, doing
hierarchy by name. It's annoyingly close to dejagnu, but different.

To make it compatible with dejagnu, we either assume it's okay to use TEST:,
even thought that is reserved. Or, we use a non-reserved variation. For
example: TEST: at the start of a line instead of after a tab.

That sounds good to me. I can be dejagnu standard, but also have extra info to
know what test is running and detect test failures that resulted in not being
able to print the test result.

Biggest downside for now is requiring every test to accept a prefix to use for
the name. It means you have to know the full hierarchy of tests when running.
You can't reorganize after the tests have finished running. The benefit is you
can use that as a way to do test organization with dejagnu?

Looks like dejagnu is tricky to use in the first place. So maybe not worth
even trying to be compatible.

How would a non-compatible protocol look?

* @@ TEST <name>
* @@ PASSED <name>
* @@ FAILED <name>
* @@ XFAILED <name>

Cool. And that's it. The entire protocol. Test names have to be unique. When
you run a test, you give it the name of whatever test suite you want it in. We
can easily concatenate test results together to get a bigger suite.

I'll want some utilities:
* Given a test results file, print out a summary and return exit status 0 or
  non-zero.
  - Concise: '.', 'F', 'X' for results, plus a summary line.
  - Summary: Show all @@ test lines, ignore the rest, plus a summary line.
    Or, convert to: @@ TEST <name> <STATUS>, as you go. I like that.
  - Full: output the full test results file, plus a summary line.
  - Maybe have an option to capture logs between TEST line and status line and
    print those only in case of test failure?
* Given a command to run and a test name, wrap it in @@ TEST , @@ <STATUS>
  lines, based on exit code of command. Something like:
   echo TEST $name ; <cmd> && echo PASSED $name || FAILED $name

Cool. That sounds like a plan to me. What does this look like in practice?
* Write the two utility programs.
* Update build.tcl and spec tests and fble tests to support this format.
* Combine everything together and try it out.

Maybe I start with fble test runner. That gives me good data to start with.
Then write the summary script. That let's me test that manually. Then convert
all the rest.

---

Should we support single line test output? Like:

@@ TEST <name> PASSED

As opposed to:

@@ TEST <name>
@@ PASSED <name>

Maybe we could use a different delimiter to allow both on the same line? Like:

@TEST[<name>]
@PASSED[<name>]

And then we could do:
@TEST[<name>] @PASSED[<name>]

That's a bit awkward.

Maybe the TEST command can be optional? Like, I'm going to run a test, and it
may take a while or there is a chance it could crash, so if you don't see a
result for it, take that as failure? But no. I want to see what test is
running.

---

What if we used something more like a build failure? That's done with
something like:

foo.c:13:13: error: invalid operands to binary == (have ‘U’ {aka ‘union

In other words...

NAME: STATUS: MSG

Or we could do:

NAME: PASS
NAME: XFAIL

The nice thing about this is it supports putting the name first as you run the
test, and putting the error message on the same line. Does it support having
output between the name and the status though?

How can we separate the test lines and status from other arbitrary output?

@[<name>]: @PASSED

@[My Test]: @PASSED
@[My Test]: @FAILED: ...
@[My Test]:
Has output some misc stuff.
@FAILED: ...

You know? I kind of like that syntax.
* It's nice to allow the error message on the same line as status.
* It's nice to allow the status on the same line as the test.

This removes line-by-line syntax. Think of it as a stream of characters then.

@[...] refers to the name of the test. Test name can't have ']' in it.
@PASSED says the previously named test passed.
@FAILED says the previously named test failed.
@XFAILED says the previously named test failed as expected.

Output for a test is everything between @[...] and the next test.
If there is no status between @[...] and the next test, the test is considered
failing.

Yeah. I like that proposal. It looks nice to run. Very flexible.

---

I think it would be cool to have one big summary of all the tests that we run
at the end. Have the verbose part while they are running. That way you can
easily see what test takes a long time to run, and also a nice summary at the
end with all the failures and expected failures? I'm not sure.

---

I notice now while running tests that it would be nice to see which test is
running rather than just '.'. Like, I want the verbose output from tests now
instead of the shortened version.

---

Here is what I propose:
* During build, don't show output of tests. Save it to a file. No need to see
  anything about the running test during build.
* Fail build at test failure for now, at which point it shows the output from
  running the test to show the error.
* At end of build, show summary output for all tests that includes
  - output from failed tests (should be none if build got that far)
  - counts of tests run, passed, failed, xfailed.
  - summary 'fails' if any tests failed.
* We could not treat individual test failure as build failure, in which case
  we fail build at the end and see all the test failures at the end.

What this means in terms of implementation:
* fble test runner can always be verbose. No need for the option.
* build.tcl test should run the test, save to file, add appropriate tags.
* summarizer: reads through file, prints errors as it goes, then gives summary
  counts.

Let me start by writing a summarizer. Can repurpose tests.tcl for that.

Steps:
1. Rewrite tests.tcl to be a summarizer. Try it on output of verbose run
tests. Comment out that part of build.
2. Change fble tests to always be verbose.
3. Change fble tests to save output to file and use tests.tcl to summarize all
of them together at the end.
4. Update spec tests to use proposed output format.
Add those to the list of tests summarized by the summarizer.
5. Change miscellaneous tests to save output to file, using helper function in
build.tcl.

At this point we should be all set.

---

TODO: How to give prefix to fble-stdio test? I should add an option for that.

Okay. I'm not sure how reliable it is yet, but we have a start now. Let me
see. I'll want to double check every now and then that test failures stay
failing.

---

It would be great to have some way to do a ninja command that outputs stdout
and stderr, to dupe those to a file, and still exit with the commands status
code. Maybe I should do some tests with ninja. In particular, I want:

* If no error, just saves everything to a file and continues.
* If error, prints at least stderr and maybe stdout on screen, saves it all to
  a file, and treats it as a build failure. Rerunning make reruns the command
  and triggers the error again.

That's how normal building of things works, right? Sort of?

My tests just output stuff to, let's say, stdout and stderr. So let's do some
experiments around that.

* stderr and stdout who up as build output. I think after the build rule has
  completed. Regardless of whether the command passed or failed.
* Output is written and survives after build, even in case of failure.
* Next time we run ninja, it tries to rerun the failing test case only.
* Looks like ninja records the rules that succeeded in its log, which is how
  it knows to rerun the failing rule.

I guess most of the build commands I run are silent. Like gcc, it just does
its work quietly, without giving output except in the case of error. We see
the errors as build progress. We see warnings too, even if the command
completes.

Okay. That simplifies things. So what I want for a test is...
* Do we want stdout/stderr of test running, or just save that to the file?
  Let's say yes. We want to see the output so we can see what failed if it
  fails. Why not? It's up to the test really if it wants to have useful
  output. Probably it should.

And I know I want to save the output to a file, so we can concat test results
later, and see them logged. What's the magic to do this while getting exit
codes? Something fancy I can do with shell redirection?

Goal:
* stdout goes to stdout
* stderr goes to stderr
* stdout and stderr together go to some file foo.txt

So, 
  3> foo.txt   would redirect fd 3 to foo.txt

  3>&4   would say 3 is now the same as 4, so wherever 4 goes, 3 goes.

I think this issue is, when you write to some fd, say stdout, that has to go
one place. There's only one thing it can refer to. A file. Or stdout. Not
both?

* set -o pipefail ; ./foo 2>&1 | tee out.txt
  Should fail if foo fails.
* { ./foo 2>&1; ret=$?; } | tee out.txt; (exit $ret)
* bash -o pipefail  -c "command1 | tee output"
* command > >(tee out.txt)
  That's sneaky.

Let's try some of these.
  
Here's the magic:
   ./foo > >(tee out.txt) 2>&1

Shall we try it?

Complains because it's run using sh, not bash. Can I write a wrapper script?
What would I call it? How about... 'tee'?

Except passing commands to a bash script is no fun. That would be ideal
though, because I can hide it all behind the abstraction.

teed out.txt -- ./foo

Or... log?

log out.txt -- ./foo

log out.txt ./foo ...

---

Sort of works nicely. All except the funny tests that redirect output and
such. Can I write those as proper tests? Like, put them in separate .sh files?

The funny tests are easy to fix by separating them into two separate build
rules: one to run the test and generate the output, another to do the
assertion on the output.

---

How to switch spec tests over to new test format:
* Change 'test' to 'testsuite' in spec-test.build.tcl
* Change spec-test.build.tcl to use log wrapper function.
* Change spec-test.run.tcl to output @[name], with name the FBLE file.
* Change spec-test.run.tcl to output @PASSED when it passes, @FAILED when it
  exits with an error.

Easy, no?

---

I wonder if I should use 'testsuite' as the name of my testing protocol.

Anyway, next step is to add support for expected failures to spec tests.

It's pretty straight forward. Have some indication we an turn into a flag
about whether a test is expected to fail or not. From inside the tcl file, we
just predicate the final output as @PASSED or @XFAILED depending on that flag.

How to specify the flag value?

