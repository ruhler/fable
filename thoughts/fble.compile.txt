Fble Compilation Challenge
--------------------------
Should we compile fble to machine code before evaluating it, or should we
continue to use an interpreter?

Advantage of compilation:
* Potential for substantial performance improvements.

Disadvantage of compilation:
* It's more tedious to run a program if you have to invoke a c compiler, for
  example.

What about AOT vs JIT vs ...?

That's more a question of what representation a program should be distributed
in. The tradeoff is: too high level a representation requires compilation /
interpretation to be done somewhere. too low level a representation takes up
more space and is limited in where it can be run.

I guess today the representation is .fble, which is relatively compact, but
takes some time to compile and is slow to interpret. If we compile, the
representation would be .o, which may or may not be as compact, but is much
faster to run.

Regardless, we need to go to well specified formats (.fble, .c, .o, etc.), and
in general it's good for the framework to support separate pre-processing
passes, so I think this is worth introducing.

This is an end user API change. Instead of loading a .fble program, you want
to load a .o file. Or maybe you want to link against a compiled .o program.

Proposal: given a .fble program, compile to an object file that exposes the
following API:

FbleValue* Eval(FbleValueHeap* heap);

And in general, once you have an FbleValue, you can run it efficiently with
the existing FbleApply and FbleExec APIs, it just that code will be
represented as pointers to machine code instead of pointers to instruction
blocks.

---------

It seems worth doing some more detailed research and up front design for this.
Consider the following options to start:
* Generate c code.
* Use llvm.
* Use c--.
* Generate a custom, compact, efficient bytecode.

C-- looks obsolete now, and I'm not inspired by Haskell's performance.

llvm looks like a very complex entity I'll have to learn a lot more about if I
want to use it.

C code will be pretty clunky.

My vote is to experiment with llvm to start, and maybe spend some time
thinking about a custom bytecode.

Things to figure out how to do in particular in llvm:
* call into an fble runtime library
* tail call behavior - calling into a function as if by goto instead of
  pushing something on the stack
* storing pointers to code

Perhaps a nice way to avoid a direct dependency on llvm would be to generate
llvm's text format assembly language. Not sure. We'll have to see.

---

It seems fine to use llvm as the bytecode instead of a custom bytecode. It's
presumably just as compressible as whatever I come up with, but has the
advantage that there is existing technology to convert it to machine code.

We still want to have control over threading, however. Which means llvm isn't
at the bottom of the execution stack, it's in the middle, between the thread
scheduler and the actual computation.

Let me start calling llvm native code. The value of using native code over an
interpreted bytecode is you get hardware support for incrementing the pc,
fetching the next instruction, predicting branches. That could easily give you
10x improvement over interpreting instructions.

The concern is that the thread scheduler, or interaction with it, may start to
dominate performance overhead. I expect the native code would be a function
you could call that would do some finite amount of work on a thread and then
return a continuation. This means we can't use the native stack, unless the OS
supports some form of voluntary context switching we could leverage. And it
means we'll end up with a scheduling loop on top things that doesn't take
advantage of hardware support for fast pc incrementing, etc. If the amount of
work that gets run on a thread is small, that would be bad. Which suggests we
want to inline as much as we can to get decent size chunks of uninterrupted
native code.

In summary, some things to think about:
* Inlining to get decent size chunks of interrupted thread execution.
* See if llvm has any support for cooperative multithreading, or manipulation
  of the thread.

---

In terms of a custom bytecode. If we wanted all instructions to have the same
size, we could do it with:

FbleInstrTag tag;
bool exit;
FbleLoc loc;
size_t tag; // or blockid, or pc.
FbleFrameIndexV args;
FbleLocalIndexV dests;
FbleInstrBlock* code;

Notes
* exit could be incorporated into tag
* args is a vector for struct and scope, otherwise it could be 2.
* dests is a vector for fork, otherwise it could be 1.
* code could be a pc in theory, if all the instructions were saved into one
  big array. Though I guess we would need some other way to initialize the
  locals memory.

---
It looks like llvm has a tailcc calling convention that can be used to support
tail calls. Good.

It doesn't look like llvm has any general support for multithreading.

Let's see if we can implement our own multithreading mechanism. The idea is to
separate execution into three parts:

1. thread scheduling. Decides which thread to execute next.
2. cooperative multithreading. A way to run a computation for a fixed amount
of time (in terms of instructions executed probably), then snapshot the thread
state in a way that can be later restored.
3. running computation for a single thread.

(1) is easy enough, assuming we have a way to represent thread state.
(3) is easy enough. Just compile to native machine code. Take full advantage
of hardware support for pc, instruction fetch, stack manipulation.

(2) may be doable. Here's what I envision: at the start of each block, we add
a 'yield' instruction. The yield instruction says how many instructions are in
the block. It checks to see if the current time slice is up, and if so,
magically calls into the thread scheduler. For example, maybe it causes
registers to be saved to the stack, records the stack pointer and pc
somewhere, then switch out the stack pointer and pc for another thread to
resume.

I expect thread scheduling to be low overhead, because switching threads
happens so rarely. The big cost here is having to keep track of how many
instructions we have executed and whether we need to yield or not. Because
that needs to be run very often.

I wonder if there's any value to yielding randomly instead of strictly by
count. That way we don't have to pass the count around everywhere. We would
still need to compute a random number, but we could use a global variable for
that? We could use the same random number as we do for profile sampling? As
long as it is pseudo-random, it will be deterministic. Is that meaningful? At
least is is runtime environment agnostic, if not agnostic to the generated
instructions.

---

gprof suggests there isn't that much to gain from compiling. Because the
execution time is dominated by allocations and memory management work.

---

Threading revisited: we can use pthreads if we want, or pth, or getcontext.
For pthreads, we just need a mutex that a thread has to acquire before it can
do any work. That way only one thread runs at a time.

Problems with pthreads:
* non-deterministic behavior is, right out of the box, causing lots of
  problems. Bugs go away under the debugger. Different bugs from different
  runs. That's going to be a debugging nightmare as long as we use pthreads.
* limits on number of threads. Like, 32K number on linux? That's not really
  acceptable. Max user processes of 7314 by ulimit. Definitely not acceptable.
* only the main thread has a dynamically sized stack. Though I'm not sure how
  we could ever get around that limitation. gcc's -fsplit-stack is not
  supported by the gcc preinstalled on my computer.

---

Idea is to compile blocks of instructions together that we can execute
atomically, while still using the interpreter to drive execution. Hopefully
this cuts down noticeably on the cost of instruction dispatch, while still
supporting full control over thread execution.

We can start by converting the existing instructions to abstract functions
that the interpreter calls. This will make the interpreter compatible with
abstract functions operating on thread state. The compiler can then generate
some new abstract functions and that works fine with the interpreter.

The concern is there will be too much overhead to calling these abstract
functions. Let's do some experiments.

The most commonly called instruction at the moment in fble-bench, by far, is
FBLE_RELEASE_INSTR. It accounts for over 40% of instructions executed.

One nice benefit of putting instructions into functions is then profiling can
tell us which instructions take a lot of time, and how much time is spent on
instruction dispatch.

Baseline fble bench: 2m9.762s

Experiment:
  Access thread state through thread pointer instead of local variables.

  fble bench: 2m10.890s

Experiment: 
  Pull ReleaseInstr into its own function.

  fble bench: 2m12.889s
  
If we assume all instructions regress the same amount, then total I would
expect 7.5s overhead changing all instructions to functions. But then we can
get rid of the binary search from the switch statement, which I expect would
get us all of that back. We also get the advantage of per-instruction
profiling breakdowns and the opportunity for compilation. I think it's worth
trying. Let's do it.

After converting a bunch more instructions over:
  fble bench: 2m12.251s

After converting all the rest of the instructions and getting rid of the
switch statement entirely:
  fble bench: 2m16.704s

That's too bad. Total regression of 7s, even after removing the case
expression.

Well, if it's any consolation, we get 1m12.644s with -O3 turned on and no
profiling arcs.

---

Starting up again to try and get an fble compiler.

Let's start as simple and basic as we can. Generate C code which the user can
compile using a C compiler. No need to bother with llvm at this point. I can
make my own abstractions for representing C programs if I don't want to just
output strings everywhere.

Primary goal: replace the interpreter while loop with straight line code that
calls the same exact exec functions the current interpreter does.

Today we have two kinds of functions:
* FbleFuncValueTc,
* FbleCompiledFuncValueTc

The first represents a function value as an abstract syntax tree of values
after type check. The second represents a function value as an fble
instruction block. Maybe we can add a third type that represents a function
value as pointer to a C function to call?

It's starting to come back to me now.

Goal is to separate thread scheduling from function execution. We currently
track thread state in thunk values. It would be nice if we could have two
kinds of thunk values: one tracking func, pc, and locals for interpreted code,
and one with a function pointer and locals for compiled code?

Maybe what we want is an abstract data type representing thing that can be
partially computed.

 Inputs: 'this', heap, thread, io_activity
 Results: updated 'this' or finished or aborted, etc.

With a single method to 'run some finite amount of time'.

Or, change instructions to hold the actual C function pointer for executing a
function. Then the compiler could merge adjacent instructions that can be
compiled together into new C functions and insert them into the instruction
stream?

I don't know. I need to think on it a bunch more.

---

Two big challenges for compiling fble that I keep getting stuck on:
1. Fine grained multithreading. Which is painful, because I feel like I could
get away without it fble in practice, but that we really want it in theory.

2. General support for tail calls not smashing the stack. I double checked.
gcc and llvm can be made to support a special kind of tail call where the
function being called has the same prototype as the caller. Is this good
enough? Maybe, depending on how we abstract away function calls.

But I can't get a simple example to work for gcc using -O2 and
-foptimize-sibling-calls if the calls span across multiple .o files. Seems
frighteningly restrictive.

---

Here's my initial proposal. We support multithreading and tail recursion to
start. There will be, perhaps, modest performance gains initially, but
hopefully there's lots of opportunity to improve from there.

Well generate a C function for each InstrBlock. The C function is the compiled
form of the vector of FbleInstrs of the InstrBlock.

The C function has the following API:

Status F(FbleValueHeap* heap, Thread* thread, bool* io_activity);

The function will read the value of the pc from the top of the stack to
determine where to start executing from. Given instructions a, b, c, ...,
we'll use a case statement to allow the function to jump directly to the
current pc. For example:
  
  switch(thread->stack->pc) {
    case 0: a;
    case 1: b;
    case 2: c;
    ...
  }

This will incur an overhead when resuming a function after a context switch.
Hopefully the overhead is small in the default case when we start from pc 0,
or we could always add an explicit check at the start of the function to try
and optimize that case. Once the function finds where to start, I assume
fall through to the next instruction will be as if we didn't have the switch
statement at all.

Initially we should add logic around every instruction to check for profile
sample and time slice yield. We should emit profiling op instructions directly
too to support profiling.

If we need to yield or block, it should be as easy as returning the status
code, because we'll keep the thread state up to date in the Thread* pointer at
every instruction.

To do a normal function call, we can update the thread state for the new
function and then directly call that function's compiled C function. To do a
tail call, we can update the thread state for the new function and then return
'RUNNING' to the caller. Then it's the callers job to call back in to whatever
is on the top of the stack? It's certainly doable, but some details need to be
worked out. Like, how does it know if the function that it called finished or
has a continuation? And we would need special logic every time you call a
function to wait for the continuation to be completed.

To ease the transition, maybe we can add another field to FbleThunkValueTc
which is the native C function to call to execute it. Initially they all share
the same native function: the equivalent of RunThread, but that only executes
code from the one function, not any functions being called. That way we get
the API in place, and deal with having to call functions recursively and
handling tail calls, but without having to worry about compilation yet.

Once we have compilation, we can switch to the native function and ignore the
FbleInstr** pc. Hmm... Maybe FbleInstr** pc should change to an int pc that we
read from func->code if needed. Maybe better to add the native function to
FbleCompiledFuncValueTc. And once we switch to native, we can ignore the
'code' field or set it to NULL. Yeah. That sounds good.

We can do initial work in the interpreter to prepare the way so the switch
compilation is easy:

Add function pointer to FbleCompiledFuncValueTc that should be called to
execute the body of the function instead of RunThread (or in collaboration
with RunThread). Initially everything FbleCompiledFuncValueTc should use the
same generic C function. We'll have to figure out how to manage function
calls, tail calls, continuations at this stage. It's probably a good time,
too, to factor out the code for running the body of a function from the thread
scheduler.

---

Well, we have a problem. I tried compiling Fble/Tests.fble to a C function
that allocates the corresponding FbleCompiledFuncValueTc. It's over a million
lines of C code, and gcc isn't too happy about that (I gave up after 2.5 hours
compiling and 64% of my computer's total memory).

Perhaps we loose some sharing by compiling. I don't think for InstrBlocks or
Instrs. Maybe Locs? Or at least source file names. But I don't feel like that
would account for a million lines of code. More like we just have a lot of
instructions. Think about a literal string for example. Lots of instructions
there. I should disassemble Fble/Tests.fble to see how many instructions.
Maybe multiply by 5 or 10 or so to get expected number of lines of C code?

I'm thinking we're going to need some form of modular compilation. Would it be
enough to put each FbleFuncValueTc* into its own C function all in the same
file? It will still be over a millions lines of source code though. That seems
unrealistic.

Can we do per-fble module compilation? That sounds complicated, but is almost
certainly what we'll want eventually.

---

Disassembly of Fble/Tests.fble shows:
* 1073 instruction blocks
* 14204 profiling ops
* 115267 lines.

More worrying is that the main instruction block is over 60% of the entire
thing. It has 65K instructions. A lot from the sat solver test and invaders
graphics. That means even if we output each instruction block in a separate
file, we're not going to be able to compile the result.

Two ideas come to mind:
1. Run the main function in the interpreter, compile the resulting function to
C code. That should work around needing to compile the main function.
Hopefully the rest is small things and pre-computed values. Those precomputed
values could be pretty expensive to serialize though, and we would definitely
have to preserve sharing in the generated code.

2. Compile each module's code into a separate C program. In other words,
figure out how to support modular compilation.

---

Modular Compilation

Here is what I'm thinking.

Module Interfaces
-----------------
The interface to a module is its type. When you reference a module, we really
only need to read its interface, not its entire value. But we can derive the
interface from the value if needed. I propose for a module path /Foo/Bar%, we
can track that to two possible .fble files, one containing the implementation
(Foo/Bar.fble), and one containing the type (Foo/Bar.@.fble?).

If the type is present, we use that without reading the implementation. This
is where you get the savings from modular compilation. If the type is not
present, we read the implementation and get the type from there. When we
compile a module we check its type against its type file (if any). The
compiler has an option to output a type file for a module automatically from
its implementation if you don't want to do one by hand.

Ideally the type file is a normal .fble file. It describes a module whose
value is a type that you could refer to normally in the implementation file if
you wanted to. That leads to the tricky question: given a module /Foo/Bar%,
what should be the name of its type module? Maybe /Foo/Bar@%?

Alternatively, it seems like you would never need the type of a type file,
because it contains the same content as the original type file. Then maybe
it's okay to, for example, use '.fble' for implementation and '.fbli' for
interface. Perhaps if you try to reference the '.fbli' from the '.fble' it
automatically figures out the right thing?

TODO:
* Try writing some of these interface files by hand. See how it looks. Would
  we ever want to write them by hand? Would we always want to write them by
  hand?
* Experiment with different mappings from implementation module path to type
  module path.

Generated Module
----------------
We need to compile fble modules to C modules. What does such a C module look
like? I propose we generate a C module with 3 functions:

1. 'Load' - for internal use. A function to lazily allocates and caches an
FbleValue* corresponding to the fble module, given a FbleValueHeap* pointer. I
guess we cache per heap, unless we want an implicit global FbleValueHeap.

This loads dependent modules as a side effect. Multiple calls to Load
increment a reference count.

2. 'Unload' - for internal use. Decrements the Load reference count. If that
reference count drops to 0, frees the cached FbleValue* so that GC can reclaim
it. Unloads any dependent modules.

3. 'Value' - for external use. Calls Load, grabs a reference to the value,
then calls 'Unload'.

This way we don't have to explicitly provide values for dependencies. It's all
done implicitly, with internal sharing as appropriate. We Load, Unload, and
Value functions are named based on the module path. We can compile .fble to
.c, compile .c to .o, link all the .o together, and everything just magically
works if you call 'Value' for the top level module that you want.

TODO:
* Figure out how to map module path to C function name for 'Load', 'Unload',
  and 'Value'.

And that's it. Modular compilation solved.

It's unclear to me if we want to maintain the interpreter separately from
modular compilation or not. Extra burden. Extra testing. But the interpreter
is much more convenient to use than compilation, no? Let's see if I can't
share as much code between them as I can as long as I can before giving up on
the interpreter.

---

It's not too bad to define a type interface for a module, except:
* You kind of have to redefine any types in both the interface and definition.
  You need the type in the interface to get the type of it. You need it again
  in the implementation to return it, but you can't reuse what you define in
  the header for the implementation.

* It's annoying to have to worry about what order things are defined in.

I'm not convinced this is something that should be forced on the programmer.
It sounds like you would want three things
 1. type declarations.
 2. interface (depends on types)
 3. implementation (depends on types)

Or, in other words, types are always defined separately. The interface and
implementation only deal with collections of values.

Or we could make the interface polymorphic in types, but then it doesn't match
the type of the implementation.

This suggests to me that, perhaps, we ought to always generate the interface
during compilation, by the compiler, and for the compiler. That way we have
some more confidence it matches the implementation. The user can always
separate interface from implementation manually if they want to do it for
other reasons. No reason to tie that together with module compilation.

So maybe compiling /Foo/Bar% gives generates an interface file Foo/Bar.fble.@
that the compiler can use, that just happens to be a normal fble file? I'm not
sure. It probably wants all the same information as a type declaration, but it
could pack that some special way. Keeping it a straight fble file would just
be an implementation detail.

For list, I would expect...

@ Unit@ = *();
<@>@ List@ = <@ T@> { +(*(T@ head, List@<T@> tail) cons, Unit@ nil); };
*(
  @<List@> List@,
  <@ T@>(List@<T@>) { List@<T@>; } List,
  <@ T@>(T@, List@<T@>) { List@<T@>; } Cons,
  List@ Nil,
  <@ T@>(List@<T@>, List@<T@>) { List@<T@>; } Append,
  <@ T@>(List@<T@>) { List@<T@>; } Init,
  <@ T@>(List@<List@<T@>>) { List@<T@>; } Concat,
  <@ A@, @ B@>(List@<A@>, (A@) { B@; }) { List@<B@>; } Map,
  <@ A@, @ B@>(List@<A@>, B@, (A@, B@) { B@; }) { B@; } ForEach,
  <@ A@, @ B@>(List@<A@>, B@, (A@, B@) { B@!; }) { B@!; } ProcessEach
);

So we still have names and lets, but it's all inlined. That's pretty cool to
see. And then we save more time by not reading a whole bunch of dependencies
just to get type declarations. I think that's good. No change needed for the
user.

For the first version of modular compilation, we can always read the
implementation file to get its type. Worry about generating these .fble.@
files later as an optimization.

---

There's a slightly awkward issue that fble-compile takes a .fble file, not a
module path. That way you can pass a .fble file not in the module hierarchy.
But then what name do we pick for the generated C function?

How about we have two things we can do when we compile:
1. Generate C functions to use the compiled value as a module.
2. Generate a C function to use the compiled value as an end user.

For (1) we provide the module path on the command line, something like
  fble-compile -m /Foo/Bar% Foo/Bar.fble

For (2) we let the user pick whatever name they want.
  fble-compile -e FooBar Foo/Bar.fble

As long as we can reserve a prefix for the compiler to generate the module
names in a deterministic way from their path.

To summarize, there are three ways you could compile: export module only,
export top level only, or export module and top level. You have to give an
explicit module path to export module, and you have to give an explicit
function name to do top level.

A nice thing about this is that users should never have to call the module
entry functions. That's only called internally. So users shouldn't have to
know how we map module path to function name.

What prefix to pick for reserved? Maybe __Fble_Load_XXX, __Fble_Unload_XXX?

/Foo/Bar/Sludge_Thing% turns into... something like:

__Fble_Load_Foo_2f_Bar_2f_Sludge_95_Thing

This is a mapping with:

a-z, A-Z, 0-9: unchanged

All other characters:  _XX_, where XX is the hex representation of the ascii
code for the character. Which I guess is assuming ascii instead of general
unicode. I suppose if we want to do unicode, we could say anything outside of
what you could fit in _XX_ you do _XXXX_ instead. We don't have to be able to
go backwards or have unique encodings. Just so long as we can go from the name
to the encoded value in an agreed upon deterministic way.

---

Next step is to work out how to go about implementing all this. My vote is to
not worry about .fble.@ to start. The only difference when we go to switch is
whether we compile an expression for its type, or compile an expression for
its value which must be a type.

We'll want to merge modules after compilation instead of before. We can
compile a module namespace variable to something that looks up a global static
variable in scope for the function and return an InstrBlock with statics equal
to the number of module dependencies and the list of module dependencies.

If we want to compile, now it's easy: load the module dependencies, add them
to the scope of a CompiledFuncValueTc and return that. If we want to
interpret, same thing, but use 'let' to load?

So perhaps the first step should be convert the interpreter to join modules
after compilation instead of before.

---

Here's a strawman:

* LoadModule - loads a Module as:
  - a module path,
  - a reference count,
  - an FbleExpr* with the body of the module (optional),
  - an FbleExpr* with the type of the module (optional),
  - the list of Module* that this module depends on.

The body will be provided when loading a module directly, or when loading all
modules in a program. The type will be provided when loading a .fble.@ file
with the type of a module when you don't also need the body of the module.

* LoadProgram - loads all modules in the program.
  Let's us share the results of searching for modules across the entire
  program, which is much more efficient than doing one at a time.

* CompileModule - given a list of types to use for the module's dependencies
  and a loaded module, returns a CompiledModule. A CompiledModule stores the
  list of dependent modules as a list of paths and an InstrBlock with the body
  of the module. Err... maybe the compiler should be responsible for figuring
  out the types of dependant modules. Yeah. So we take a Module, compile
  bodies and types of other modules as needed to find dependencies, then
  produce an InstrBlock and list of dependencies for the module.

  The InstrBlock will be for a function that takes zero arguments and one
  static variable for each immediate module dependency in order.

* CompileProgram - compiles all loaded modules.
  Let's us share the types of modules across he entire program, which is much
  more efficient than doing one at a time.

* Link - takes a list of CompiledModules and produces an FbleValue
  representing the value as a CompiledFuncValueTc.

* GenerateC - takes a single CompiledModule and generates C code for it.

Use cases:
1. interpreting code - LoadProgram, CompileProgram, Link, Exec.
2. compiling code - LoadModule, CompileModule, GenerateC, gcc.

For handling of .fble.@ files, let's say we only try to read them when you
call LoadModule, not when you call LoadProgram. If a .fble.@ file does not
exist but the corresponding .fble file does, too bad. Report that as an error
to load the module.

Allow .fble.@ files to be searched in a different directory? Well, probably we
expect the user to pass the directory with the .fble.@ files instead of the
directory with .fble files, so that's fine.

When you call fble-compile, you pass in the name of the .fble.@ file you want
to generate for the compiled file. fble-compile will be smart enough to only
update the .fble.@ file if it has changed, to help with build dependency
systems. fble-compile should have an option to generate gcc style .d
dependencies to replace fble-deps.

This means we probably want a CompiledModule to include the module's type as
well as it's implementation. Then maybe instead of attaching an FbleExpr with
a module's type to Module, we directly attach an FbleType, and expose a
separate function that makes use of Load and Compile to read a type from a
.fble.@ file. Maybe that's the same place we put the logic for writing a type
to a .fble.@ file. Call it interface.c? FbleLoadInterface? FbleWriteInterface?
Yeah.

For the first round, let's not bother with interfaces. We'll always call
LoadProgram? I'm not sure.

Changes needed:
 - Store a list of FbleName module dependencies in FbleModule.
 - Change FbleProgram to just FbleModuleV, where the module name for the main
   module is NULL or some such sentinel value. So it reuses the list of given
   dependencies.
 - Remove deps argument to FbleLoad. We should be able to get that from the
   list of loaded modules.
 - Define CompiledModule, which is FbleName, FbleType*, InstrBlock*, and list
   of dependencies by name for a module.
 - Define CompiledProgram as a list of CompiledModule's in dependency order.
 - Change FbleCompile to return a CompiledProgram.
 - Define Link, that takes a CompiledProgram and produces an FbleValue.
 - Change FbleGenerateC to take a CompiledModule as input.

So there are a few details about whose job it is to tell the compiler what the
type of a dependent module is that need to be worked out. But I think this
should get us a pretty good step in the right direction.

---

A bug in the above design: loading a module shouldn't load the module value,
it should load a function to compute the module value. But that means we can't
have every module execute its dependency module functions, because we would
end up duplicating execution of any module used more than once.

Instead, loading a module should add that CompiledModule to a deduplicated
list of modules. Once we have that list, we link the modules together into a
single FbleValue function that we can execute.

To start, let me defer on the modular compilation stuff. It just seems easier.
So do it like this:
* Internally in FbleCompile, we compile each module separately, then link them
  together. The FbleValue we produce will not have one massive top level
  function.
* In GenerateC, let's generate a separate C function for each InstrBlock in
  the program. We should no longer have any really big functions; hopefully
  gcc can handle lots of medium sized functions much better than one big
  function. If gcc can't handle that, I can try manually splitting the c code
  into one function per c file. If gcc can't handle those on their own, then
  I'm going to have to go back to the drawing board.

---

Generating a separate C function for each InstrBlock is not enough. gcc still
needs more memory than I have on my computer to compile the c program. I'm
going to have to manually split it into different C files. I'm confident that
will solve the problem. It's just annoying and tedious.

---

Actually, we may still run into trouble. The Graphics.fble module, which is
1300 lines of .fble, turns into 220,000 lines of .c code. Is there something
very wrong going on? Let's see if gcc can do anything with it...

The good news is it doesn't take long at all to generate .c code for all the
.fble modules, even though we aren't doing modular compilation. I guess I
thought the long startup time for /Fble/Tests% was compilation. It must be the
time to generate the tests before we start running them.

Nope. We're not there yet. A lot of the modules compile fine. But a lot of
them are using upwards of 50% or more of the memory on my computer. It's
getting stuck on /Sat/Aim% right now. That's only 429 lines of .fble. It's
using lots of lists and literals.

I need to sit down and understand why we have so much code blow up for
literals. Perhaps there's a bug where we are duplicating code? Or perhaps we
can choose some smart helper functions to drastically reduce the size of the
generated code.

---

Understanding /Sat/Aim%.

The input is very regular.
A list of clauses, each of which is a list of three variables, each of which
uses an Int literal consisting of up to 3 digits.

There are 426 clauses.

Int|22: 
  l11 = struct();
  l12 = union(2: l11);      // the digit '2'
  l11 = struct();
  l13 = union(2: l11);      // the digit '2'
  l11 = struct();
  l14 = union(1: l11);      // nil to mark end of list
  l11 = struct(l13, l14);
  l13 = union(0: l11);      // cons 
  l11 = struct(l12, l13);
  l12 = union(0: l11);      // cons

This works out to:
  2 for nil at the end of the list.
  for each digit:
    2 for the letter
    2 for the cons

So we expect 10 instructions for a two digit int literal.
We expect 14 instructions for a three digit int literal.

401 * 3 * 14 = 16842 instructions solely to describe the int literals,
assuming 3 digits for each literal.

The first clause is 53 instructions total. Sounds reasonable. And enough to
lead to explain the 20000 total instructions needed for all of /Sat/Aim%.

Now let's see how we go from 20,000 instructions to  200,000 lines of C code.

l11 = struct();
  FbleStructValueInstr* v3b = FbleAlloc(arena, FbleStructValueInstr);
  v3b->_base.tag = FBLE_STRUCT_VALUE_INSTR;
  v3b->_base.profile_ops = NULL;
  FbleVectorInit(arena, v3b->args);
  v3b->dest = 11;

  FbleInstr* v3c = &v3b->_base;
  FbleVectorAppend(arena, v0->instrs, v3c);

l13 = union(2: l11);
  FbleUnionValueInstr* v3f = FbleAlloc(arena, FbleUnionValueInstr);
  v3f->_base.tag = FBLE_UNION_VALUE_INSTR;
  v3f->_base.profile_ops = NULL;
  v3f->tag = 2;
  FbleFrameIndex v40 = { .section = FBLE_LOCALS_FRAME_SECTION, .index = 11 };
  v3f->arg = v40;
  v3f->dest = 13;

  FbleInstr* v41 = &v3f->_base;
  FbleVectorAppend(arena, v0->instrs, v41);

There you have it. For the union, 10 lines of C code. That looks reasonable.

I think the best we can do is to try and make helper functions for building
these instructions.

What if, to start, we ignore profiling ops. We don't have a mechanism in place
to load the profile yet anyway. If you want profiling, run the interpreter?
It's a bit hacky. But maybe we start this way, we optimize, and then we can
add profiling back later when we have a complete story for it. Let me give
that a try.

That's not going to be enough. It doesn't change the size of Sat/Aim.fble.c
noticeably at all. But it should make it easier to start using helper
functions.

For example, now we can reasonably say something like:

AppendUnionValueInstr(arena, 2, FBLE_LOCALS_FRAME_SECTION, 11, 12, v0->instrs);

Then you get a 10x improvement. Let's give that a try.

And the other thing I need: empty struct and 2 argument struct. That should
cover all the literal cases. Oh, and 1 argument call. That gets us down to
21000 lines of code generated for Sat/Aim.fble.c, which gcc should have no
trouble with.

Okay. Now we can deal with it, if we compile one at a time. The worst is
/Md5%, which has a lot of struct access instructions. Let me add a helper
function for that too.

Yeah, now things are looking okay. My computer overheats if I run with default
number of threads, but I don't think it will run out of memory now. I call
that a success for now.

For the record, it takes about 3m30s to do a full build, including all the .o
files for all the .fble files on my computer with -j 2. That's definitely
acceptable, and should get better as I start to generate real C code instead
of C code that builds instructions.

