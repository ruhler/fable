Fble Bind
=========
Not sure the right name for this, but bind seems good enough to start.

Processes provide nice syntax for side effects, but they are fairly limited:
 - you can't execute processes inside pure functions
 - there is a single global state that has to be threaded.

'Monads' are very flexible for modeling side effects, but the syntax is a pain.

When I say 'Monad's here, I really mean any abstract data type that operates
on some internal state in some sequenced fashion using functions. Monads are
an example of that kind of thing.

My question is, can I get the best of both worlds somehow? Make it easy to
define and use abstract data types that require some sort of sequencing and
not requiring tedious syntax that also supports IO and processes?

For example, consider writing a parser for dimacs format. Or anything really.
I want to read line by line using processes to avoid loading the entire file
in memory, but I want to read a whole file at once without using processes for
testing purposes or other internal uses. How can we easily write one parser
that can be used in both cases?

I want to write a parser that interacts with the world with some abstract
methods:
* GetLine - gets the next line of input, or end of input.
* Error - report an error and stop parsing.
* Return - return a parsed result.

This is naturally represented using an abstract state type.

GetLine: S@<Maybe@<String@>>
Error: <@ T@>(String@) { S@<T@>;}
Return: <@ T@>(T@) { S@<T@>; }
Bind: <@ A@, @ B@>(S@<A@>, (A@) { S@<B@>; }) { S@<B@>; }

In this case, it's the Bind function that forces sequencing, which orders side
effects. I could implement process functions using this kind of interface. In
that case, we would have:

<@>@ S@ = <@ T@> { T@!; }

In other words, the monadic interface is more generally useful than the
process interface. So why use the process interface at all?

1. Because processes require built in language features for exec and links,
and it's always nice to have syntax for builtin language features, even if
they can be wrapped in functions.

2. Because := syntax does type inference that you don't get with function
bind.

3. Because := syntax implicitly refers to the process monad, so you don't have
to specify explicitly which state entity you are referring to.

Note an important aspect of using S@<T@> to describe the type instead of,
e.g. (S@, T@), is that it prevents you from referring to and making copies of
the underlying state. That is important for IO like things. It's not
necessarily important for things like pure state monads, where copying is
okay.

So, the question is, why can't we get all the niceness of process syntax using
the more general approach? Can we change the fble language to support that? If
so, how would it look?

I don't care about builtin-ness of processes. That can be wrapped. We can
provide library functions that cheat. That's not the interface problem.

So the problem is: type inference and, let's say, underlying state inference?
It's just too tedious otherwise?

Let's compare. What I like to write is things like:

  String@ line := get;
  Unit@ _ := F(line).?(true: put('blah'), false: put('other blah'));
  !(True);

What I would have to write, assuming we now have explicit types and explicit
bind:

  String@ line <- Bind<S@, Bool@>(get<S@>);
  Unit@ _ <- Bind<S@, Bool@>(F(line).?(true: put<S@>('blah'), false: put<S@>('other blah')));
  Return<S@, Bool@>(True);

I could write some wrapper functions to simplify perhaps:

  String@ line <- getM<S@, Bool@>;
  Unit@ _ <- Bind<S@, Bool@>(F(line).?(true: put<S@>('blah'), false: put<S@>('other blah')));
  Return<S@, Bool@>(True);

But that doesn't scale well and doesn't help all that much.

What do we need for type inference to work? We need to know as part of the
bind syntax: the bind function, which let's assume includes S@. But we also
need that information to be propagated to argument and body.

in := syntax, bind function is obvious and type is obvious in argument and
body.

So the trouble comes from trying to generalize one kind of monad to multiple.
if it's one kind of monad, no need to specify which one in the := syntax or in
the arg and body. And we know the type of process bind, so compiler can do
inference from result to top level expression.

Because the other thing about process is that 'exec' has a pretty specific
type rule that is built into the compiler. We don't have a generic way to
teach the compiler about rules like that.

How to be more generic without having to supply tediously more info? How to
abstract away, so that it's clear from the type of 'Bind' what type
information is needed?

In general, Bind is:
  ((A@, B@, ...) { X@; }, A@, ...) { Y@; }

We can't link Y@ to X@ in general.

Unless we can somehow describe that using a polymorphic type transformer
thing?

---

Imagine the bind operator took a polymorphic function and automatically
applied arguments. So, if bind is like

  A@ a, B@ b, C@ c <- BIND(...);
  ...

Where the type of BIND(...) is a function ((A@, B@, C@) { X@; }) { Y@; },
then we turn BIND into a polymorphic function where A@, B@, C@, and X@ are
passed as type parameters automatically. What does that give us?

Depends. Are we passed polymorphic functions or concrete? Let me try to add
some more detail.

  <<@>@>@ I = <<@>@ M@> { *(
    M@<String@> get,
    (String@) { M@<Unit@>; } put,
    <@ T@>(T@) { M@<T@>; } return,
    ... bind
  ); };
     

  <<@>@ M@>(I@<M@>) { M@<Bool@>; }
  Parse = <<@>@ M@>(I@<M@> m) {
    String@ line <- m.bind(m.get);
    Unit@ _ <- m.bind(F(line).?(
      true: m.put('blah'),
      false: m.put('other blah')));
    m.return<Bool@>(True);
  };

Notice, we don't need to supply M@ with m.get, m.put, m.return. We do need
to say the argument type with m.return, because that could be anything.

What do we need the type of m.bind to be to make this work?
It depends on... the type of the argument and the type of the return of the
function. So it looks like:

Trouble. The argument we have is Unit@ and M@<Bool@>. How do we go from
M@<Bool@> to Bool@? It would have to be something like:

  <@ A@, @ B@>((A@) { B@; }) { M@<???>; }

So that doesn't work. It would work if there was someway to see inside of A@
and B@, but there isn't.

What can we do for the type of bind?
  <@ A@, @ B@>(M@<A@>, (A@) { M@<B@>;}) { M@<B@>; }

Notice: we don't have to pass M@ again, because that's captured by the
interface type of m. We do have to know the ultimate return value though,
and the argument value.

Argument value we could infer with bind syntax. That's part of that.
How can we infer the return type? Actually, we could this way. We could do
it as:

  <@ A@, @ X@>(M@<A@>, (A@) { X@;}) { X@; }

Except, the implementation of bind needs to know that X@ is M@<B@>. That's
where it breaks down. And in general, maybe same thing goes for argument?

  <<@>@ M@>(I@<M@>) { M@<Bool@>; }
  Parse = <<@>@ M@>(I@<M@> m) {
    String@ line <- m.bind<String@, Bool@>(m.get);
    Unit@ _ <- m.bind<Unit@, Bool@>(F(line).?(
      true: m.put('blah'),
      false: m.put('other blah')));
    m.return<Bool@>(True);
  };

What if we define bind locally with the return type we know it should have?
That way we can get rid of the return type. And maybe it's allowed to take
argument types as input.

Or, actually, maybe we don't care what the X@ type is? No. We have to if we
want to infer things from it.

  <<@>@ M@>(I@<M@>) { M@<Bool@>; }
  Parse = <<@>@ M@>(I@<M@> m) {
    % B = m.bind<Bool@>;
    String@ line <- B(m.get);
    Unit@ _ <- B(F(line).?(
      true: m.put('blah'),
      false: m.put('other blah')));
    m.return<Bool@>(True);
  };

So it would seem the only thing I can't do today is inference of the
arguments. We can't infer results, but we can factor that out, because it's
the same for everything in the monad.

  <<@>@ M@>(I@<M@>) { M@<Bool@>; }
  Parse = <<@>@ M@>(I@<M@> m) {
    % M = m.bind<Bool@>;
    String@ line <- M(m.get);
    Unit@ _ <- M(F(line).?(
      true: m.put('blah'),
      false: m.put('other blah')));
    m.return<Bool@>(True);
  };

Still, much less nice than:
  (I@) { Bool@! } Parse = (I@ m) {
    String@ line := m.get;
    Unit@ _ := F(line).?(true: m.put('blah'), false: m.put('other blah'));
    !(True);
  };

What if we had syntax to apply a function, taking type arguments from
arguments of the function. Let's say
  ...<>(x, y, z)

Is syntax for
  ...<@<x>, @<y>, @<z>>(x, y, z)

Now we could do
  m.return<>(True), instead of m.return<Bool@>(True).

Could we improve bind that way? I'm not convinced.

What if we had a syntax :name is the bind operator. Or, what would look
niceish with single letters?

  String@ line <M get; 
  String@ line <IO get;

That's not too bad actually. But in practice, with the extra type info?

  % m = M<Bool@>;
  String@ line <m get;
  Unit@ _ <m F(line).?(...);
  return<>(True);

---

Let me explore from a different direction. I like the process syntax. Can we
make process a little more flexible? Like, could we implement a monad via
process? The idea being, you control the state parameter via the arguments you
supply rather than the bind operation. Everything shares the bind and return
operations.

  @ I@ = *(String! get, (String@) { Unit!; } put);

  (I@) { Bool@! } Parse = (I@ m) {
    String@ line := m.get;
    Unit@ _ := F(line).?(true: m.put('blah'), false: m.put('other blah'));
    !(True);
  };

Now, how do we implement the instance of I? Say I want the implementation to
be, like, a pair: strings to read, and strings written.

 (List@<String@>) { I@!; } MkI = (List@<String@> inputs) {
   @ S@ = *(List@<String@> ins, List@<String@> outs);
   S@ ~ load, save;
   save(S@(inputs, nil));
   
   String! get = {
     S@ s := load;
     Unit@ _ := save(S@(s.inputs.cons.tail, s.outputs));
     !(s.inputs)
   }

   (String@) { Unit!; } put = (String@ x) {
     S@ s := load;
     save(S@(s.inputs, Cons(x, s.outputs)));
   };

   @(get, put);
 }

That's not so unreasonable. But, is there any way we can run this as a pure
function now? The idea is, wouldn't it be nice if we could run it as a pure
function, so long as it only does puts and gets on ports defined internally?
Because if you aren't accessing external ports, there is no issue there. And
assuming it doesn't use exec with multiple arguments that could leave to
nondeterministic behavior.

We could add a type to proc type, so it's go two types: the state type and the
return type. And make it, like, polymorphic in the state type. And have a way
to run a polymorphic state thing. If it's polymorphic, you know it can't refer
to any concrete external types.

For example, maybe call the type Proc@<S@, T@>.

Then I could have a run function:
  <@ X@>(<S@> { Proc@<S@, X@>; }) { X@; } Run

Because the argument is polymorphic, we could make up any type for S@, so that
can't match it.

And we would pass S@ when we instantiate a link:

  Bool@ ~<S@> get, put;

It would be nice if we could write a function that does IO where some are
restricted and others are not. We run to eliminate all the ones that are
restricted. All the others still stay there. Like:

  <@ X@>(<A@> { Proc@<A@, B@, ..., X@>; }) { Proc@<B@, ..., X@>; } Run

Or, maybe we have data types that provide operations, where you take the value
as an argument. You can't allocate new links internally. That's all done
externally in the passed argument. Then you can run a function that takes the
value of the interface as an input?

Like, what if 'Proc' was really an abstract type meaning: some computation
involving some thing that supports bind and return. No link. No exec.

Let me stew on this. I think I'm getting somewhere. The idea that link and
exec are somehow special to IO based monad, but in general ... we could have
other things special to other monads. Like a smten monad. And you have special
run functions that work. Like...

  runIO :: (MkLink, DoExec) -> Proc(X) -> X

But, does that just bring us back to haskell do notation? Where you can't
easily mix and match monads and it's not very general?

Like, imagine if bind and return worked for any polymorphic type? No, we need
to define the bind and return functions.

So, we have a special monadic type. Maybe a function that takes bind and
return as inputs? Huh?

No. Let it stew. I'm getting confused now.

---

Function bind is currently not used anywhere, and yet I'm afraid to make it
overly specific. I claim a fully general function bind is not useful if it is
too tedious, because we aren't using it at all.

So, what if we made bind much more specific, with the goal of removing the
extra type information that makes it tedious. And then see if I can reasonably
use that to replace processes with a library.

The use in processes is to replace exec, which would have the function form:

<@ A@, @ B@, ..., @ X@>(M@<A@>, M@<B@>, ..., (A@, B@, ...) { M@<X@> }) { M@<X@>; };

Single argument bind is a special case of this:

<@ A@, @ X@>(M@<A@>, (A@) { M@<X@>; }) { M@<X@>; }

Syntax option is?

  BIND A@ a := ma, B@ b := mb, ...;
  ...

That's an interesting idea. Have the 'BIND' operation on the left. Which is
easier to do now that we pass ma, mb, ... as part of the syntax instead of
pushing that into BIND.
  
  :bind A@ a := ...;
  ...

  :f(x) A@ a := ...;
  ...

  f(x): A@ a := ...;
  ...

Playing around with the syntax a little more...

:exec<A@, B@> a <- ma, b <- mb;  No. Not nice.

Let's say the type M@ is part of the BIND function. So, if I were to
generalize, I would say BIND is polymorphic in argument types and result type?

Because the thing is, to implement BIND, we want to know A@, B@, ..., X@ as
types. We don't want an abstract M@<A@>, M@<X@> type.

  bind: String@ line <- get<S@>;
  bind: Unit@ _ <- F(line).?(true: put<S@>('blah'), false: put<S@>('other blah'));
  Return<S@, Bool@>(True);

I like that the syntax lines up everything on the left.

What if we don't want to have one-to-one argument to values? Like, maybe we
want a foreach for a map that extracts separate key and value?

 (Map@<K@, V@>, (K@, V@) { X@; }) { X@; }

Then I would want:
 foreach: K@ key, Value@ v <- map;
 ...

Now the syntax is slightly less pleasant, because it looks like partial
indent. You see that above with the bind too. The fact that the last line is
not aligned.

How bad is it to do:

  A@ a, B@ b, C@ c <- Exec(ma, mb, mc);
  ...

One of the few examples I have in practice:

  Unit@ _ := Pad(input, put_padded, /Md5/Bits%.Bit64.BitN.zero),
  ABCD@ md5 := PaddedMd5(get_padded, ABCD0);
  !(HexFromABCD(md5));

Would turn into:
  Unit@ _, ABCD@ md5 <- Exec(
    Pad(input, put_padded, /Md5/Bits%.Bit64.BitN.zero),
    PaddedMd5(get_padded, ABCD0));
  !(HexFromABCD(md5));

That's pretty unpleasant.

Compare to:

 foreach: Entry@ entry <- map;
 entry.key, entry.value ...

But we really want BIND to be separate if we have multiple x := ... entries in
the syntax, rather than make it part of the arrow.

  BIND: A@ a := ...;
  BIND: ...;
  ...

Could we do parallel exec as a sequence of binds instead of all at once?

Semantics would be...

Start first thing in the background. But then the result is not allowed to be
visible to the second thing. That's the trouble.

---

It would be nice if we could define a single value interface with static
methods that could be reused. I'm okay if we want to order calls to methods.
We can do bind internally in the implementation of the methods. That is: you
are given sequencing from the external syntax, you have to manage that
internally yourself.

But again, how would we run such a thing in a pure function?

Should we always push the bind into the operation? Like, instead of Get being
pure get, have it be get + bind? Or, at least, have an alternative that's like
that? The individual operations with side effects have the sequencing built
in?

That's unpleasant from a point of view of composition. We would like to factor
the sequencing function out, then we could combine it anywhere. But it does
let us eliminate the BIND object in the syntax. Does it allow parallel
sequencing? I kind of doubt it.

---

About whether we can 'run' something with general side effects in a pure way.
I think we can, as long as it doesn't use, like, exec. That is, we can do it
if we know what all the primitive side effect operations are. We'll want
separate run functions for different kinds of monads. And a special one for
real IO/non-determinacy.

Side effects have to be part of the type. They can't be hidden entirely.
That's important.

Let's revisit the parser example.

I want to define an object with the following possible side effects:
  - return a result 
  - report an error
  - get the next line of input.

Or, the IO object with the following possible side effects:
  - input a string.
  - output a string.

The type is a polymorphic result type to represent side effects. We just want
a syntax that knows how to wire up a bind type specific to the side effect
type.

A@ a <- ma, B@ b <- mb, ... ;
...

From this syntax, we know A@, B@, M@, and the final result type (which we
could maybe or maybe not restrict to M@ and then extract the subtype from it).

How about, syntactically, there is a difference between ',' and ';'. For ',',
you don't see the variable in the next entry, for ';' you do. That's two
different kinds of binds.

In other words, we think of
  A@ a <- ma, B@ b <- mb

As combining somehow into a function... but that's the problem. Types
changing. Names of things difficult.

So we have one general comma list semicolon terminator combination.


  m.do: String@ line <- m.get;
  m.do: Unit@ _ <- F(line).?(true: m.put('blah'), false: m.put('other blah'));
  m.return(True);

  String@ line <- m.do: m.get;
  Unit@ _ <- m.do: F(line).?(true: m.put('blah'), false: m.put('other blah'));
  m.return(True);

  String@ line <- m.do(m.get);
  Unit@ _ <- m.do(F(line).?(true: m.put('blah'), false: m.put('other blah')));
  m.return(True);

The middle one's not bad above. Separates the bind from the argument
syntactically, so we could pass type information to it.

What about this:
  A@ a <- x: ma;
  z;

Is syntactic sugar for:
  x<A@, Z@>(ma, (A@) { M@<Z@>; }) { M@<Z@>; }

Where type of x is:
  <@ A@, Z@>(M@<A@>, (A@) { M@<Z@>; }) { M@<Z@>; }

The type of Z@ is extracted given the type A@ and M@<A@> and M@<Z@>.
  
And now, if possible, an we allow comma syntax too?

  A@ a <- x: ma, B@ b <- y: mb;
  z;

This should say:
  We pass a function (A@, B@) { M@<Z@>} as an argument to y?

* let f = (A@ a, B@ b) { z; }
* ma and mb do not see a or b.
* how do x and y interact?

One of x or y or a combination is passed function f. Let's say 'y'.
It would be nice if x and y could be the same. It would be nicer if I only had
to mention it once. That would be easy.

  A@ a <- x: ma, B@ b <- mb;
  z;

Then:
  let f = (A@ a, B@ b) { z; }
      Z@ = inferred from A@, @<ma>, @<z>
  in x<A@, B@, Z@>(ma, mb, f);

But now the syntax is abnormal.
  
  A@ a <- x: ma,
  B@ b <- y: mb;
  z;

Then:
  let f = (B@ b) { z; }
      g = { M@<B@> mb' = mb; (A@ a) { y<B@, Z@>(mb', f); }; }
      Z@ = inferred from A@, @<ma>, @<z>
  in x<A@, Z@>(ma, g);

Is that reasonable? In this case, types of x and y are:
  <@ A@, @ Z@>(M@<A@>, (A@) { M@<Z@>; }) { M@<Z@>; }

So types look okay, and this means the same function can be reused for all
comma entries. The next question is, how could we implement a parallel thing
using this? What's the implementation of x for exec?

Conceptually we want to end up with:
  fork ma
  fork mb
  a <- join ma
  b <- join mb
  z

Could we change exec to not have this specific behavior? Like, use a different
interface? For example: let's say we have par and seq functions.

  seq: (M@<A@>, (A@) { M@<Z@>}) { M@<Z@>; }
      Executes all of ma, takes result, applies it to f.
  par: (M@<A@>, (M<A@>) { M@<Z@> }) { M@<Z@>; }
      Starts execution of ma, creates ma', which will block on the result of
      ma, applies that to f.
  
Or maybe instead: 

  par: (M@<A@>) { M@<M@<A@>>; }

Kicks of ma in background, returns a process that blocks on the result.

Then, to do ma and mb in parallel, we would do something like:

  M@<A@> ma' <- seq: par(ma),
  M@<B@> mb' <- seq: par(mb);
  A@ a <- seq: ma';
  B@ b <- seq: mb';
  z;

If we wanted, for 'seq', we could say waits for all spawned processes to
complete before finishing execution. That way you can't leak spawned
processes, for example:

  M@<A@> ma' <- seq: par({ M@<C@> mc' <- seq: par<mc>; ma; }),
  M@<B@> mb' <- seq: par(mb);
  A@ a <- seq: ma';
  B@ b <- seq: mb';
  z;

Here we say running ma' blocks on mc' finishing.

And, if we want, we can now generalize with comma support internally:

  A@ a, B@ b <- x: ...;
  ...

Okay? So I have a suggested syntax, suggested type rules. Let me state it for
the record:

  A@ a, B@ b, ... <- x: m,
  z;

Then:
  let f = (A@ a, B@ b, ...) { z; }
      Z@ inferred from m ???
  in x<A@, B@, ..., Z@>(m)

where type of x is, for some M@, S@:
  <@ A@, @ B@, ... @ Z@>(M@<S@>, (A@, B@, ...) { M@<Z@>; }) { M@<Z@>; }

No, I don't think that works. We can't go from fixed S@ to polymorphic A@, B@,
... So we can't support the general comma format.

Stick to the simpler:
  A@ a <- x: m;
  z;

Then:
  let f = (A@ a) { z; }
      Z@ inferred from @<m>, @<z>
  in x<A@, Z@>(m)

where type of x is, for some M@:
  <@ A@, @ Z@>(M@<A@>, (A@) { M@<Z@>; }) { M@<Z@>; }

---

Okay, so we have a proposal now. I have two concerns with this proposal:
* Does it support everything we want for processes. I think so, just want to
  double check.
* Is it overly specific to monadic bind? For example, in haskell do notation
  is useful for applicative functors too. Is there some other use cases that
  we would want to have the nice syntax for that don't match this somewhat
  specific interface for the BIND function?

In terms of generalization, the summary of key points is I need enough
information to do a type rule and provide the fundamental types to the bind
function.

It's really all about type inference now. Particularly now that we simplify
parallel processes.

The argument types are clearly fundamental and make sense to pass. But the
BIND function may not be polymorphic in those, which is a pain? Or can we just
have polymorphic types that we ignore and let type checking verify it's as
expected?

Then the real challenge is the fundamental part of the return type to infer.

Could we have a general type inference rule that applies only in the bind
syntax? For example, given a polymorphic type like:

 <@ A@, @ B@, ...>(...);

We are given arguments as input. Infer all type parameters from the arguments
and fill in every thing we inferred. No need to include all the type
parameters in the polymorphic list. Just the ones the function depends on.

Which suggests maybe we want a general form of type inference, and then we can
use the more general bind syntax.

What we need for type inference is to know all the arguments to a function.
Let's say we can do type inference in those cases where the type parameters
can be inferred from the arguments. You have a special application syntax:

  f<>(...)

f<> is not a standalone thing, it's part of the application syntax. Then we
can infer the type parameters and return type of the function. The arguments
have to be typeable in isolation still, so I think that's fine.


  String@ str <- io.do<>(io.get);
  io.return<>(True);

That's not bad. Much nicer than, say:

  String@ str <- io.do<String@, Bool@>(io.get);
  io.return<Bool@>(True);

If the problem really is type inference, which I think it is, then how about
we add a feature for type inference and keep the super general function bind
syntax? I like that idea. It just depends on how hard it is to implement type
inference.

Remember, type inference is based on the idea that you do poly + <type params
to infer> + arguments to use for the inference. We still build types from
bottom up. You'll never be able to infer, for example, an empty list's type
based on its result.

Let's explore the type inference approach a little more. Because that sounds
useful and promising to me.

---

Type inference for bind. I have something like:

  <@ A@, @ B@>(M@<A@>, (A@) { M@<B@>; }) { M@<B@>; }

Given: M@<A@> and (A@) { M@<B@>; }, can I infer A@ and B@?
* A@ I can infer from the function type.
* In general, can't infer B@. Because, for example, M could be something like:
  <@>@ M@ = <@ T@> { Bool@; }
  If I have Bool@, I can't come up with anything for T@.

But that's a weird case. It just means we can't infer phantom things. Like we
can't infer things from return types alone. 

Imagine something more likely, where M is something like:
  <@>@ M@ = <@ T@> { *(S@ s, T@ x); }

Then we have, for example, *(S@ s, Bool@ x). And we compare that against
M@<A@> which is *(S@ s, A@ x). Then we can infer A@ must be Bool@.

Can we do this with the purely generic bind syntax? Where it takes a single
function-argument function?

  String@ s <- io.bind<String@>(Get)<>;
  ...

It's awkward, because we have to split up bind:

  <@ A@>(M@<A@>)<@ B@>((A@) { M@<B@>; }) { M@<B@>; }

We can only get the type inference on the second? No. We can do both. But it
would look something like this:

  String@ s <- io.bind<>(Get)<>;
  io.return<>(True);

Can we do type inference implicitly? Like, if you have a poly and you try to
do an application, that's never allowed. So how about we always try to infer
when you do function application (and function bind)?

Syntax is pretty nice:

  String@ s <- io.do(io.get);
  io.return(True);

What if you accidentally forget to supply the type argument? What's an
example... say for Cons function.

  Cons(True, l);

In this case, it can infer the type. So you're fine.

There will be cases where you cannot infer the type. Then you get an error.

Is this bad? Does this go against my philosophy for the type system?

What's the philosophy:
* You can determine the type of an entity in isolation. You don't depend on
  external context.

There are two interpretations of Cons(True, l)
1. We can't figure out the type of Cons, because it depends on the application.
2. Cons is a polymorphic type. You can apply arguments to a polymorphic type,
it works by type inference.

In the second case, I think things are okay. And they are:
  Cons(True, l)  - Cons is a known poly type
  Cons<Bool@>(True, l) - Cons<Bool@> is a known non-poly type.

That is, I'm saying Cons is short for Cons<Bool@> or Cons<Unit@> depending on
the environment. I'm saying Cons is different from Cons<Bool@> and has
different typing rules for application, because poly (value) application is
different from function application.

That's a cool idea. I think it might be worth trying. I don't think it's too
hard to implement. We'll need a specification for type inference, but I also
don't think that's too hard. And it's pretty nice. A way to have type
inference, which is one of the annoying things about fble, but in such a way
that we still give things unique types independent of context, and so
hopefully types are still obvious.

In this case, type inference is a built in part of poly application. That
seems natural to me. Poly, because you don't need inference otherwise, and
application, because you don't have anything to infer from otherwise.

And hey! This solves the tediousness of monads problem. And it does so in a
general, nice way.

I think we could end up doing inference for a ton of the places where I
explicitly specify types. Note you can always explicitly specify a type still
if you want.

So really, the only concern I have with this type inference idea is the fear
that you could simultaneously forget to annotate the type and make a mistake
in the type of one of the arguments, and as a result infer the wrong type
without giving a type error.

I think it's worth a try.

So, in summary, I'm proposing the following:
1. Add support for type inference to poly apply.
2. Make sure we have support for type inference for function bind too.
3. Redesign process IO to be something we can easily wrap without any special
syntax support (though it will still need special language support). Primarily
by splitting exec into separate 'fork' and 'seq' operations.

And hey, if I wanted to, I could provide common operations, like:

  String@ s <- io.getM;
  io.return(True);
where
  io.getM = io.do(io.get)

---

Redesign of IO process. What should it look like? I have two alternatives
proposed right now:
1. Minimal. sequence, return, primitive side effect (stdin get, stdout put,
  stderr put, event get, effect put, etc.). In particular, no links, no
  generic get/put, no concurrent processes.

2. Sequence, return, links, gets, puts, concurrent processes. Which at some
point I felt was a fundamental thing we could justify.

Current uses of links in all the programs I have written:

* Used to drive Md5 tests, because Md5 relies on get to get inputs.
  An alternative would be to provide a non-io side effecting 'get' operation,
  assuming we've generalized Md5 to work with any monad that supports a 'get'
  operation.

* Md5 implementation, to combine padding versus processes processes.
  An alternative would be to implement get_padded by keeping track of explicit
  padding state: pad versus padz, which zero byte we are outputting, which
  length byte we are outputting.

* Benchmark for md5. Same as md5 test driver case.

* BenchmarkGame test driver.
  Alternative is to replace primitives for get/put with no-op functions.

* TicTacToe AI
  Alternative: Replace AI with single 'ChooseBestMove' operation.

* App benchmark driver.
  Alternative: custom primitive operations.

All of my current use cases could be statically scheduled. In that case the
only real difference between concurrency and not is whether we track state
implicitly in the position of the executing program or explicitly using some
data structure and switch statement. Being able to describe things using
concurrency is nice, but not required.

The tictactoe AI example is, to me, the best motivator for concurrency.
For example, imagine if we wanted to play a game and there were two different
AIs, and whoever made a decision first gets to go next. How could I model that
without concurrency?

The answer would be to manually split things up into finite chunks of work and
implement a scheduler manually on top of that. So, I guess the question is,
should it be a user defined scheduler or a scheduler built into the language?

The main program gets to use implicit thread state. Any other parts of the
program have to use explicit state. That seems unfair and unscalable.

Another argument for processes is that in real life, not everything is under
the control of a single scheduler. You have multiple machines interacting. You
have people acting with machines. You could come up with a scheduling
protocol so that one thing goes at a time, but that's generally inefficient
(because everyone else is stopped for the rest of the time) and not how things
work in reality. Is there not something fundamental about concurrency, then,
that we should be capable of modeling in the language?

On the flip side, there's not really any such thing in the real world as
"instantly create a new independently running process". Giving birth or
building a new machine is a sort of rare event one-time thing. That would
suggest having a model where all processes are known statically is as
justifiable as the parallel exec model we have in fble today.

Another example I like to use is implementing a 'screen' app in fble.
Interesting. I guess in that case everyone can think they are in charge of
scheduling, but calling into another process acts as an implicit yield.

Another example: imagine you want to implement an app that runs over a
distributed collection of computers. You want a way to model multiple
processes running concurrently with explicit communication between them.

If we want concurrent processes, then I think Links with get, put are what we
want. And some way to fork. For example, maybe something like:

<@>@ Link@ = <@ T@> { *(IO@<T@> get, (T@) { IO@<Unit@>; } put) };
<@ T@> { IO@<Link@>; } Link;
  Creates a new link, returning operations to put and get from link.

<@ T@> (IO@<T@>) { IO@<IO@<T@>>; } Spawn;
  Spawns a new concurrent process, immediately returning an operation to block
  on the result of that process.

  And maybe with the condition that a parent process will block on all child
  processes before the parent process finishes. Which seems like an entirely
  reasonable requirement to me. If some internal child process is still going,
  conceptually the parent process is still going, because the parent has to
  manage or take over the work of the child.

That's a pretty straight forward proposal. It needs primitive support.

While I'm at it, we also need primitive support for something like smten in
fble, which we be another kind of special primitive monad. What would that
look like? Free, MPlus, Query.

If only there was some generic way to implement support for all of these
different things. If only concurrency and symbolic execution didn't need to be
implemented as a core part of the language internally.

---

If we want to support concurrent processes, links with get and put makes sense
as a way for them to interact.

Composition argument: You are writing a program in fble, such as a stdio
program or an App@ application. It's fine if that program controls its own
schedule. The composition argument says it's important that we have the
language features necessary to compose those top level stdio and app programs
together. For example it should be reasonable to implement a program like
'more' or 'screen' by composition of the existing apps the way they are
written.

I believe in the composition argument. What's not clear to me is if we need
concurrent process to compose.

Computation scaling argument: In any large program, having a single CPU will
be a bottleneck. You need some way to scale to multiple CPUs, otherwise
programs written in the language are fundamentally limited in scale.

The computation scaling argument rings true to me. And I don't believe it
should be on the implementation to do all that for you. Or, in other words,
you should describe computation using separate threads of control, it's up to
the implementation to schedule those appropriately.

So, just as you can allocate a large number of objects in the language,
we should let you execute a large number of threads.

But if you believe that concurrent threads of execution are such a core part
of the language, then shouldn't they be more a core part of the language? As
in, no need to put things in a special IO monad?

Not necessarily. Pure functions are composing blocks within a thread. We could
need a separate thing for composing threads into a concurrent program, aka,
processes.

Now, if we have processes, I want the external interface for things like stdio
and app to use processes, not abstract primitive side effects. Except,
primitive side effects can support both in-thread composition and cross thread
composition, whereas ports can only support cross thread composition. For that
reason, it would be nice when describing a program to think in terms of
abstract primitive side effects.

Where does that leave us?

* Programs should be described in terms of abstract primitive operations.
* If you want to use multiple processes, then you should be able to provide
  implementations of primitive operations based on links and puts and gets.
* If you don't want to use multiple processes, then you should be able to
  provide implementations of primitive operations based on normal function
  calls, and not require the process monad to execute your composed programs.

That suggests to me that whatever interface you use for accessing side effects
should not have 'processes' as part of the interface. It should either use an
abstract type variable (which could be tedious to pass around everywhere), or
somehow hide the real implementation internally.

It would be great if we could hide the real implementation of an abstract
interface internally, not as part of the types. But I'm not sure that's
feasible. Which suggests everyone who is using side effects should be
parameterized by interface type.

The ideal interface is one where you have a constant interface that has side
effects under the covers. So you don't have to change the interface every time
you call, but somehow calling the same method multiple times can have
different results every time. One way to do that is put everything in an
abstract M@ type with a bind function for sequencing. The state is
updated/propagated through the bind operation, the interface acts as diff
functions on whatever state is passed in.

---

Consider 3 different layers.

1. Parameterize code by some abstract type.
Downside is you have to pass the parameter around everywhere. I'm not sure how
tedious that is once we have type inference in place. Upside is you can
describe exactly the interface you want to program against, and implement it
all sorts of different ways.

2. If we had some sort of STRef like monad. That is, a monad that lets you
sequence things and allocate a reference to a value that can subsequently be
modified. That would be enough to allow you to have a constant interface to
changing state. Concurrency is not required. Link, get, and put is one
implementation of this.

It's possible to hide the internal state from the type because you can capture
the references internally in the interface methods. You don't have to pass the
state between bind calls.

The problem with having a specific monad for these kinds of things is you
can't control which operations can be done. Anyone who has access to an
operation in the monad could use it.

3. Concurrency support, for the purposes of scaling computation and, if it
turns out to be useful/necessary, supporting composition of what would other
wise be top level programs.

In summary, I think we should get into the habit of using approach (1)
everywhere we can. We should also have a built in syntax for concurrency, but
use it primarily via (1) instead of hard coding it. It's okay to require
concurrency to run concurrency. But don't give a syntactic advantage to
concurrency over approach (1), because we want people to prefer approach (1).

---

Updates to processes based on the above:
1. exec does not support parallel exec anymore.
2. need a new syntax for spawning a thread.  

Let's play around a bit.

  Maybe@<Bit8@> ~ get, put;
  Md5Hash@ md5 := Md5(get), Unit@ x := PutAll(put, bytes);
  !(md5);

Becomes:

  Maybe@<Bit8@> ~ get, put;
  Unit@! _ := &(PutAll(put, bytes));
  Md5(get);
  
Another example:

  Streamed@<Bit8@> ~ get_padded, put_padded;
  Unit@ _ := &(Pad(input, put_padded, /Md5/Bits%.Bit64.BitN.zero));
  ABCD@ md5 := PaddedMd5(get_padded, ABCD0);

In general:

  A@ a := ma, B@ b := mb, C@ c := mc;
  ...

Becomes:
  A@! a_ := &(ma);
  B@! b_ := &(mb);
  C@! c_ := &(mc);
  A@ a := a_;
  B@ b := b_;
  C@ c := c_;
  ...

So, proposed syntax is:

  &(<expr>)

to get a process the executes the given process in the background and returns
a future to get its result.

---

I want to circle around again. What's different between put/get and some
generic, primitive, side effecting operation? For stdio and app, we pass ports
as abstract processes. Why do those have to be ports? Why can't we pass as
input arbitrary abstract processes?

And if we are happy to do that, why have a built in syntax for processes
instead of just pass in the primitive interface that implements it?

Something to do with the compiler not being able to compile it, rather
treating the primitive as some opaque thing? But that's what we want to do
with approach (1) above anyway.

I don't know. Needs more thought.

At the very least, I think it makes sense to start with implementing type
inference support, and see how that makes bind syntax more useful. We can
worry about what to do about processes later.

---

For process, maybe background process would be better syntax as:

  A@! a &= ma;
  ...

It combines the background with a bind. If you want without the bind you could
always do:
  { A@! a &= ma; !(a); }

I feel like this is more consistent with sequential syntax.

---

Implementing poly inference:
* Expand TypeEquals implementation to take a list of pairs of type variable
  and inferred type. Inferred type starts NULL, but is updated as soon as
  something is inferred. Add a separate entry point to access this inference
  capability. The implementation should be straight forward.
* On poly apply: collect the type args into a list, make sure the final body
  is a function, do type inference on each argument in turn, retaining
  whatever was inferred for previous arguments. If any of the calls fail, we
  fail. At the end, make sure we inferred something for each type parameter.
  Then use those type parameters to compute the return type. Or just rewrite
  it to a poly apply + func apply and rely on the existing code generation for
  that.

It's not that hard I don't think. Just some details to put in the right place.

---

Type inference is implemented now! I feel like I should try it out. And see if
it works okay with bind.

Would it be bad to start taking advantage of poly inference everywhere? For
some reason I'm afraid to do so.

One thing I need to work out is how to describe bind in the spec. Is it
general syntactic sugar that applies to just func_apply and poly_infer, or
does it also apply to abstract_value and struct_value? If so, maybe it should
be in a separate section and called simply 'bind' instead of func_bind.

Certainly it would be worthwhile to work for both func_apply and poly_infer.
Using it for abstract_value and struct_value seem weird to me, but is there
any harm in doing so? Regardless, I need to clarify in the spec.

Anyway, what's next in theory, now that we have type inference? Sounds like
next steps are:
* Spec and test bind for poly_infer.
* Rewrite IO spec to use background process instead of parallel exec.

Though, honestly, function inference is a big enough change to the language, I
feel like I should write some more fble code before doing more language
changes. The next thing for sat solver is see about using more interesting
data structures. Might be worth doing now.

---

On the specification for bind. I see two options.

1. This is a purely concrete syntactic sugar. It turns anything that matches
the form:

  A@ a, B@ b, ... <- xxx;
  yyy;

Into:

  xxx((A@ a, B@ b, ...) { yyy; })

Regardless of what xxx is.

2. Have bind syntax apply specifically to func_apply and poly_infer, or some
other explicit list. It could be described as another concrete syntactic form
for those cases, or as, e.g. func_bind and poly_bind abstract syntaxes.

Considerations for choosing between (1) and (2):
* Today we implement bind at the concrete syntax level. The parser doesn't
  know the type of xxx, so it can't distinguish, for example, between
  func/poly and some other form. That means either we use (1), or we need to
  change the implementation.
* If bind is a single concept, we can describe it in one place in the spec. If
  it's specific to func_apply, and poly_infer, then we may have to duplicate
  the explanation in the spec.
* Should we support bind syntax for struct value and abstract value too? Seems
  like a much less common case, but is there any harm in generalizing?
* You could imagine the compiler wants to take advantage of bind syntax to do
  optimization. But, on the other hand, you could imagine it's easy enough for
  the compiler to detect this kind of inline function argument case and
  optimizing without the special syntax. So I'm not sure that's an argument
  one way or the other.
* Error messages may be easier to comprehend if bind isn't just a concrete
  syntactic sugar. Because if it is, you get the error message on the
  desugared expression, which you don't see in the source code.
* Case statements set precedent for alternative concrete syntax for give
  abstract syntax entities. But in that case there is only a single abstract
  syntax it would map to.

This also makes you wonder if we should be treating func_apply and poly_infer
as separate (abstract) syntax. In the implementation we represent both using
misc_apply.

---

More thoughts on parallel execution. The question is whether it's a bad idea
to change parallel process to be based on background processes rather than
fork/join parallel child processes.

* Background processes are as powerful as fork/join parallel child processes.
  We can simulation fork/join parallel child processes using background
  processes.
* Fork/join parallel child processes are not as powerful as background
  processes, in the sense that they don't allow the parent process to run
  while there exist any child processes. That's the key difference.
* Conceptually it makes sense for parent and child processes to run at the
  same time. Think about people in a company. You are responsible for a job,
  you hire some workers (spawn some background processes). you continue to
  work while those workers work. When the jobs done, you stick around until
  all the people you hired are done, and then you wrap up. It feels arbitrary
  and convoluted to say the parent process can't run at the same time as
  children.
* My concern with background processes is that it's hard to tell by looking at
  the source code what processes are running. Think goto considered harmful
  kind of argument. With fork/join parallel child processes, it's clear at any
  point in the code what's running: the outside world does its thing (you
  don't care) and either you are running, so no children to worry about, or a
  fixed number of children are running while you wait, and then you can focus
  in on a particular child and replace the view. For background processes,
  there's no easy way to see from the code when a process ends, or what
  processes are running at any point in time. I think that's a significant
  enough problem to warrant the restriction to fork/join parallel child
  processes. Thing how much of a headache it is in pthreads or in bash
  background processes to know if all children are killed or not when the
  parent dies, or if you'll end up with zombies?
* We can have the fork/join parallel child process be reasonable with respect
  to conceptually having parent and child run by pretending the parent takes
  the task of running the first child. No need to have an idle processor in
  practice.
* I would argue parallel process syntax not being easy to do with function
  bind is a reasonable feature, because parallel processes really are a
  special thing. You can't reasonably expect things to run in parallel without
  them. For example, how would you abstract parallel execution using bind
  syntax? You would say you run a thing that doesn't return until you run its
  result, but in practice, it can only run in the background in the very
  special case of the built in processes. Otherwise it has to be some form of
  cooperative multithreading, which has fundamental implications for causality
  and deadlock.
* Worst case, you can support parallel processes with function bind. For
  example, limit to two parallel children at a time. Then, instead of:
    A@! a_ &= { ... };
    B@! b_ &= { ... };
    A@ a := a_;
    A@ b := b_;
    ...

  You have:
    A! a_ = { ... };
    B! b_ = { ... };
    A@ a, B@ b <- parallel(a_, b_);
    ...

  Which is not in any sense worse. It's only worse if you want more processes
  forked, and then, not by too much?
    A! a_ = { ... };
    B! b_ = { ... };
    C! c_ = { ... };
    A@ a, B@ b, C@ c <- par3(a_, b_, c_);

  Or:
    A! a_ = { ... };
    B! b_ = { ... };
    C! c_ = { ... };
    Pair@<A@, Pair@<B@, C@>> r <- par(a_, par(b_, c_));
    A@ a = r.first; B@ b = r.second.first; C@ c = r.second.second;
    ...

  Or...
    A@ ~ get_a, put_a;
    B@ ~ get_b, put_b;
    C@ ~ get_c, put_c;
    Unit! a_ = { ...; put_a(result); };
    Unit! b_ = { ...; put_b(result); };
    Unit! c_ = { ...; put_c(result); };
    Unit@ _ := par[a_, b_, c_];
    A@ a <- get_a;
    B@ b <- get_b;
    C@ c <- get_c;

So, in summary, I propose we keep processes as is in fble, no change. We can
provide function-bind friendly wrappers around it. Just not a fully general
parallel exec wrapper, which makes sense given the fundamental and special
nature of concurrent execution and is a price definitely worth paying to be
able to see clearly from the code what threads are running.

---

A great application to try next would  be to rewrite the sat parser to work
based on an abstract interface with the following methods:
* get_line, error.
* add_clause.
* do, return.

Which we could then implement to work with io, or pure, construct a list of
clauses, or construct whatever internal data structure we want.

Or, more generally, have two separate interfaces, one for reading input, the
other for writing output? That's an interesting question, and worth playing
around with.
