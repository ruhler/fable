
@ Decimal@ = /Core/Digits%.Decimal@;
% Decimal = /Core/Digits%.Decimal;

@ Int@ = /Core/Int%.Int@;
% Int = /Core/Int/Lit%.Int;
% Div = /Core/Int/Div%.Div;

<@>@ Undef@ = /Core/Undef%.Undef@;
<@>% Undef = /Core/Undef%.Undef;

Undef@<Decimal@> NotADigit = Undef<Decimal@>;

# Digit --
#   Converts an integer in the range [0-9] to a decimal digit.
#
# Behavior is undefined if the integer is not in the range [0-9].
(Int@) { Decimal@; } Digit = {
  (Int@ d) {
    d.?(n: NotADigit.undefined, 0: Decimal.0, p: d.p.?(
        1: Decimal.1,
        2p0: d.p.2p0.?( 
          1: Decimal.2,
          2p0: d.p.2p0.2p0.?(
            1: Decimal.4,
            2p0: d.p.2p0.2p0.2p0.?(
              1: Decimal.8,
              : NotADigit.undefined),
            : NotADigit.undefined),
          2p1: d.p.2p0.2p1.?(
            1: Decimal.6,
            : NotADigit.undefined)),
        2p1: d.p.2p1.?( 
          1: Decimal.3,
          2p0: d.p.2p1.2p0.?(
            1: Decimal.5,
            2p0: d.p.2p1.2p0.2p0.?(
              1: Decimal.9,
              : NotADigit.undefined),
            : NotADigit.undefined),
          2p1: d.p.2p1.2p1.?(
            1: Decimal.7,
            : NotADigit.undefined))));
  };
};

# LeastSignificantDigit --
#   Returns the least sigificant decimal digit of the given integer.
(Int@) { Decimal@; } LeastSignificantDigit = (Int@ x) {
  Digit(Div(x, Int|10).r);
};

@(LeastSignificantDigit);
