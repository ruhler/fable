Sat Application
===============
Goal is to optimize the fble-based sat solver.

Ideas:
* Avoid lots of repeated traversals somehow.

Looks like there are already thoughts at pkgs/sat/thoughts.txt that we can
start with. Or start from scratch with profiling, setting up a benchmark,
seeing where we are, and going from there.

I like the idea of starting from scratch.

Step 1: Do we have a sat benchmark still?

Yes. sat-bench. It's not built by default, but there are rules for.

How about rename it to fble-sat-bench and install it.

Current status:

real    1m17.150s
user    1m16.747s
sys     0m0.381s

This is 400 iterations on /Sat/Aim%.

Remember, goal of this exercise is to improve the sat algorithm, not to
improve the fble implementation. It could be tricky using a set number of
iterations. I almost prefer to shrink the sat problem until we get something
we can reasonably solve. But that could be hard too.

Yeah, the challenge with the benchmark is number of iterations isn't well
defined. If we change the algorithm, we change what goes into an iteration.

A better benchmark is:
time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/unif-c1000-v500-s1356655268.cnf

real    0m47.738s
user    0m47.143s
sys     0m0.573s

It's not limited by a timeout or number of iterations. See if we can cut the
time in half to 25 seconds or less as a first goal.

Reminder of high level algorithm in use here:
* Simplify as much as possible with repeated unit clause and pure literal
  substitution.
* Try all possible variations for a particular variable and recurse.

First goal is to keep that same high level approach, just use data structures
to implement it much more efficiently. We can worry about improving the
algorithm after.

Profiling shows GetPure dominates the time. It's only called 211 times. Let's
focus here then.

It goes through each clause, for each variable tracking whether it is pure or
not. Then it filters out which of the variables ended up pure. All the time is
in the first part: tracking the status of each variable. And all that time is
the cost to insert a variable into the map.

There's a couple things here:
1. It's sad that we don't have arrays we could otherwise use for constant
update.

2. We could avoid doing the traversal over and over again by only look at
variables that may have changed since the previous traversal.

(2) is the premise for what I had proposed before in pkgs/sat/thoughts.txt.
Basically, keep track of changes as we do substitutions.

Map VarId -> ([ClauseId] positive, [ClauseId] negative)
Map ClauseId -> [(VarId, Polarity)]

Now I'm back up to speed. Idea is to create an API with some internal state to
track the state of a formula and build it up as we parse.

State is:

(Clauses, Vars, Assigns, Todo).

We construct incrementally, where Todo is START.
Then we run a 'process' function that processes all todos until there are none
left.

At the end, we should either have:
* No clauses ==> unsatisfiable (?). or mark as unsat some other way.
* No vars ==> satisfiable.
* Stuck => copy the state, pick a var and add an assignment todo both ways.

Methods:

Empty - Initial empty state.
AddClause - Adds a clause. TODO state better be START, else invalid.
Simplify - Runs through TODOs, returning result.

Operations are:
* Start
* Assign
* others as needed.

Things we'll want to do quickly are find pure literals - that is, literals
with an empty polarity for clauses. And unit clauses: that is, clauses with a
single variable. Why not maintain those as separate lists then?

So, state is:

Variables: VarId -> ([ClauseId], [ClauseId])
Clauses: ClauseId -> [(VarId, Polarity)]
Assignments: [(VarId, Polarity)]
Operations: 
  Assign VarId Polarity.
  RemoveClause ClauseId
  RemoveVarFromClause ClauseId VarId.
  RemoveClauseFromVar ClauseId VarId Polarity.
 
Initially everything is empty.

Add a clause:
* Update variables and clauses with new clause.

Start Solution:
* Iterate through all variables and clauses, collect together initial
  assignments as assign operations.

Then go through all operations as long as any remain.

Assign VarId Polarity:
* Remove VarId from Variables map. If it's there:
 - Add variable assignment.
 - For each + clause, add a RemoveClause operation.
 - For each - clause, add a RemoveVarFromClause operation.

RemoveClause:
* Remove clause from clauses. If it's there:
 - For each variable
   - Add a RemoveClauseFromVar operation.

RemoveVarFromClause:
* Remove the var from the clause if the clause is there.
* If the clause ends up with a single var, add an AssignVar operation.
* If the clause ends up with no vars, mark unsat and exit.

RemoveClauseFromVar:
* Remove clause from var if the var is there.
* If the var ends up with a single polarity, add an AssignVar operation.

Keep doing the same until either it's unsat or all operations are done.

At the end, if there are no variables left, it's satisfiable and we're done.
Otherwise, pick a variable to assign, fork, and repeat.

Easy, no?

---

Draft of code is one. I like it. Pleasant to write. One interesting thing:
* There's no way to break out of a ForEach loop. I want to get any element of
  a map. Maybe add a 'First' or 'Last' method to Map to be able to get just
  the first or last?

We changed the interface, so I'll need to change my test driver.

Maybe, for now, it's worth wrapping the original interface so we can run our
existing tests?

Yeah. I think so.

With the wrapper, just some error messages to fix, and all tests pass first
try!

Now for the benchmark... It's likely it's trying solutions in a different
order, so I'm not sure what to expect. If it's wildly different, it could be
as much due to the different order as to the optimization. Let's see where we
are at.

fble-sat-bench:
@[Sat]: @PASSED

real    0m2.745s
user    0m2.697s
sys     0m0.040s


time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/unif-c1000-v500-s1356655268.cnf

real    0m9.190s
user    0m9.024s
sys     0m0.141s

Yeah. That's a definite improvement. Easily achieving my 25s goal.

---

Profiling on pkgs/sat/bench/unif-c1000-v500-s1356655268.cnf says:

50% showing the result.
25% computing the result.
25% parsing.

That's pretty crazy.

In terms of computing the result, 40% is adding clauses. Most of computing the
result is the less than comparison operation for doing map lookups.

For showing the result, it's all in Concat for showing the list of elements.

Search was called 66 times, to give a sense of how many alternatives we tried.

Let me run fble-sat-bench without any timeout, and run on all the benchmarks
from that same package with unif-c1000-v500-s1356655268.cnf to see what kind
of numbers we get.

---

I ran it over night. No luck yet. Haven't managed to solve any of the
benchmarks in less than 5 minutes, and haven't finished fble-sat-bench yet.

Some insight into time taken may be nice. Can I run perf on one of them for 5
minutes and get anything useful out? Let's try.

---

Random sat algorithm idea.

A 3 term clause gives us 7 combinations of those terms that are compatible
with that clause. For 2 clauses, that's 7 + 7 nodes, which we can cross to get
49 potential nodes representing all possible assignments that satisfy those
two clauses. Repeat that all the way down until each original clause is
included. Any composite node that survives with each original clause is a
satisfying assignment. If there are none, it's unsat.

The question comes down to how to efficiently factor out the ways of combining
clauses to get to an answer or rule out all possible answers as fast as
possible.

---

Anyway, I have my perf profiling data now. Here's what it shows ...

It's hard to get a meaningful picture, because everything calls
FbleThreadCall, and FbleThreadCall calls everything. It would be better if I
could get a real fble profile I think.

Let's find a decent timeout value to set on aim and use that.

Again, not much of a meaningful picture. We spend a lot of time doing ops and
searches. I think we need a better algorithm to reduce how much time we spend
searching different possible assignments.

---

I forgot to turn on time logging of my sat runs. The only info I'll get is
whether we were able to solve it or not in less than 5 minutes. Maybe that's
still a useful number in aggregate.

Anyway, two things to work on next for the sat solver:
1. Conflict driven clause learning.
2. Accept general format formulas, rather than forcing CNF.

I don't entirely understand how (1) works yet. The idea is as soon as you get
a conflict, you learn what caused it and add an extra clause. For example:

(a + b + c)
(a + d + 'c)

After picking a, b, and d, we get the conflict for c. We go back and say, oh,
well, you know, we also have

(a + b + d)

Then jump back up to where we chose a, and b, and continue, this time setting
d to false right away.

I'm not sure how far is safe and useful to jump back.

For (2), the interface I want is something like:
* True, False,
* Var
* Not, And, Or, Cond

The idea being we allocate a new variable for each sub-formula and turn each
of these functions into constraints.

x = Not(a) - Return the negated form of the variable a. (No need for new var).

x = And(a, b)
  Constraints:
    a, b => x
    a' => x'
    b' => x'
    x => a
    x => b

x = Or(a, b)
   a', b' => x'
   a => x
   b => x
   x' => a'
   x' => b'

x = Cond(p, a, b)
   p, a => x
   p', b => x
   a, b => x
   p, a' => x'
   p', b' => x'
   a', b' => x'

So really we just need a way to represent constraints of the form
   a, b => c

Given a bunch of those, solve the problem.

Unit clause comes for free. Once we pick a, b, we get => c. Then we know c.

Pure literal is saying a variable always appears one way on the left and
another way on the right. Then it's safe to pick such that it doesn't
constrain anything and it is never constrained. Harder to see how this happens
in practice with the above constraints.

Any way to check in our profile how often we do unit clause versus pure
literal?

PureLiteral is Cnf.fble line 228. DoRemoveClauseFromVar.nops.: 50398
Unit clause is Cnf.fble line 198. DoRemoveVarFromClause.nops.: 66614

So I guess they both happen a bunch.
   
---

Here's my new proposal. Conceptually we have a collection of facts. A fact has
the form ([Var], Var). For example: a, b => c. If all of the left hand side
variables are true, that means the right hand side variable must be true.

Same idea as CNF.
* Unit clause: Any fact with nothing on the left hand side.
* Pure literal: If a variable only shows up pos on lhs and neg on rhs. Or if a
  variable only shows up neg on lhs and pos on rhs.

  For example, if x is pos on lhs and neg on rhs, then you can safely set x to
  false, because there is no possibility of learning otherwise and it has no
  consequence for other variable settings.

To handle true/false in sat format problems, we can define a special variable
T from the start. Use T for true, T' for false, and the first assignment we do
is assign T to true.

To handle unsat detection, there are two ways we could do it.
1. Define a variable X to mean 'impossible'. Anytime we assign a variable,
replace it with X on the right hand side of any fact that says the variable
should be different.

2. A faster way: Anytime we assign a variable, in any facts that says the
variable should be different:
* If lhs has no vars: unsat.
* If lhs has one var a: assign a'
* If lhs has two or more vars a, b, ..., s
  Replace it with the fact: a, b, ... => s'

There are two forms of knowledge here: facts and assignments. Once we make an
assignment, we can simplify the facts based on that.

On assignment:
* If the var appears positive on the lhs of a fact, remove that var from the
  lhs of the fact.
* If the var appears negative on the lhs of a fact, remove that fact.
* If the var appears positive on the rhs of a fact, remove that fact.
* If the var appears negative on the rhs of a fact, see (2) above.

The API to this solver can be you add a bunch of facts (rather than clauses).

Converting from CNF:
AddClause (a + b + c) 
Turns into adding the following facts:
  a', b' => c
  a', c' => b
  b', c' => a

Converting from SAT:

Each formula is represented as a (possibly new) variable. And we add clauses
based on that. To solve a formula, we assign the variable for that formula to
true and solve. We are done when all facts have been discharged.

True: true
False: false
Var: new var v.
Not(true): false
not(false): true
Not(x): x'

And(true, x): x
And(false, x): false
And(x, true): x
And(x, false): false

And(a, b): 
  New var c. And the following facts:

  a b | c
  0 0 | 0
  0 1 | 0
  1 0 | 0
  1 1 | 1

  a,b => c
  c => a
  c => b
  a' => c'
  b' => c'

Or(a, b):
  New var c with the following facts:

  a b | c
  0 0 | 0
  0 1 | 1
  1 0 | 1
  1 1 | 1

  a',b' => c'
  a => c
  b => c
  c' => a'
  c' => b'

Cond(p, a, b):
  New var c with the following facts:

  p a b | c
  0 0 0 | 0
  0 0 1 | 1
  0 1 0 | 0
  0 1 1 | 1
  1 0 0 | 0
  1 0 1 | 0
  1 1 0 | 1
  1 1 1 | 1

  p,a => c
  p,a' => c'
  p',b => c
  p',b' => c'
  a,b => c
  a',b' => c'

Don't worry about conflict driven clause learning yet. But I bet it's easier
to integrate with this fact based format. You keep track of the original fact
that led to the initial assignment for a conflict, and the original fact that
led to the conflict. Merge those facts together and add the result. It's just a
question of when/where to add the new fact and where to back track to.

I'm thinking we add yet another module to reimplement this from scratch. Call
it Solver. So, eventually, Cnf becomes the Cnf interface to the solver. Sat
becomes the Sat interface to the solver. And Solver is the raw fact based
interface and implementation.

wikipedia says following is terminology:
 P -> Q, P is antecedent, Q is consequent.

Let me use: Var@, Fact@, lhs, rhs.

Cool. Just need to figure out all the ops to support, then should be good to
implement this fact-based solver.
  
---

On more thought, it's better to use clauses than facts.

Consider: a, b => c

From that, we also know:
  a, c' => b'
  b, c' => a'

So it's odd that we make it asymmetric. Much easier to represent this
directly, symmetrically, as:

  a' + b' + c

This says the same thing: either the left hand side has to be false, or the
right hand side has to be true.

Otherwise we would be adding a bunch of special case logic for left hand side
versus right hand side. This way, we can use the existing solver logic.

If we want to support cnf: we get that right away.
If we want to support 'sat', we can do the same thing I was thinking above,
just tweaked slightly:

True: true
False: false
Var: new var v.
Not(true): false
not(false): true
Not(x): x'

And(true, x): x
And(false, x): false
And(x, true): x
And(x, false): false
And(a, b): 
  New var c with the following clauses:
  (a' + b' + c)
  (a + c')
  (b + c')

Or(a, b):
  New var c with the following clauses:
  (a + b + c')
  (a' + c)
  (b' + c)

Cond(p, a, b): (p & a) | (p' & b)
  New var c with the following clauses:

  (p' + a' + c)
  (p' + a + c')
  (p + b' + c)
  (p + b + c')
  (a' + b' + c)
  (a + b + c')

  Maybe the last two are redundant and we can learn them? Let's try:

  (p' + a + c') conflicts with (p + b + c'), telling us that
  (a + c' + b)
 
  So yeah, that looks redundant. We can just use the four clauses and learn
  the rest later:

  (p' + a' + c)
  (p' + a + c')
  (p + b' + c)
  (p + b + c')

Next steps, then:
* Clean up the existing Cnf solver code
 - Rename to Solver.fble
 - Rename S@ to Solver@
 - If desired, split Ops into VarOp@ and ClauseOp@ subcategories to factor out
   some common code and make the names simpler.
 - Change the Main dimacs program to construct the formula via solver instead
   of making a formula first and then adding all the clauses.
* Add a Sat.fble interface that has a wrapper around the solver that adds its
  own variables and clauses as above.
* Work on conflict driven clause learning.

Thoughts on conflict driven clause learning:
* pure literals don't lead to conflicts.
* otherwise, for any conflict (empty clause), you have the clause that
  originally assigned a variable and the clause that became empty. Those are
  the two conflicting clauses, and you know which variable they conflicted on.
  Say the clauses are (a + b + c + ... + x), (p + q + r + ... + x'). You learn
  the clause: (a + b + c + ... + p + q + r + ...)
* We want to learn the clause for the entire search, not just where we are.
  For example, maybe you pick variables m, n, o, then you run into the above
  conflict. You don't want to run into that same conflict for each of the 8
  variations of m, n, and o.
* In terms of how far to backtrack when you hit a conflict:
 - At least as far back as the last variable in the learned clause, because
   there's no point otherwise.
 - Presumably you don't want to go back all the way to the beginning.
 - I'm not sure. I kind of feel like you want to go back decision by decision,
   retry where you are given the new clause (re-apply simplification to take
   advantage of it), and go up. But if you're going to do that, may as well
   jump to the front right away to avoid duplicate work? Not sure.
* How to implement adding a clause mid-search? Needs thought.

---

Let's move on to conflict driven clause learning. The high level alrgorithm is
clear to me.

Our current search algorithm looks like this:
 Search S:
   S' = Simplify(S)
   if unsat or sat: return;
   V = some unassigned var in S'
   A = Search(assign V=true in S')
   if unsat:
     return Search(assign V=false in S')
   return A

With conflict driven clause learning, we want:
 Search S:
  S' = Simplify(S)
  if unsat or sat: return;
  V = some unassigned var in S'
  A = Search(assign V=true in S')
  if unsat:
    S'' = Simplify(S' with with learned clauses)
    return Search(S'')
  return A

In other words, replace the 'assign V=false' step with 'apply learned
clauses'. Easy.

Simplify will give us a single learned class if unsat.
Search can return multiple learned clauses. In particular, it returns learned
clauses from both the "assign V=true in S'" case and the "S''" case which need
to be combined together.

Why this is the right place to apply learned clauses:
  If assigning V=true led to a conflict, then whatever clause we learned
  should prevent us from picking V=true after simplifying. So we do as good as 
  assign V=false would, but even better because we could infer other variables
  as well.

  If we didn't hit this conflict before, then going back up another level,
  simplification would not cause the learned clause to become a unit clause.
  (I claim and believe. It's not a totally convincing argument here.). So no
  value in doing that.

Anyway, let's start with this proposed high level algorithm, which should give
us many of the benefits and is conceptually simple.

The harder part is deciding how to track conflict clauses. I'm thinking to
start with the idea of describing a conflict as a pair of clause ids and a
variable. Then we can apply a conflict to state S by concatenating the vars in
the conflict clauses and filtering out the variable.

The alternative would be describing a conflict clause as the full original
clause. To apply that to a state S, we would need to substitute any already
assigned variables.

Note that regardless what approach we take, there's an O(N^2) thing going on
here: each conflict clause we learn has to be applied to each state S on the
stack. I'm proposing we start by applying the full list of learned conflict
clauses to whatever the next state S on the stack we go back to, as we go back
to that next state S.

Ideas for tracking conflicts:
* Any time we assign a var, we say if it was:
  PURE, ATTEMPT, or UNIT
  If it's PURE or ATTEMPT, there can be no conflict.
  If it's UNIT, there's a clause id associated with it.
  Worst case, we can pick clause id 0 associated with pure and attempt, and it
  wouldn't do any harm to learn a clause.
  (a + b + c) * (d + e + f) => (a + b + c + d + e)
  Track this as part of 'assigns'. Maybe use a map for assigns for quick
  lookup by var.
* Simplify has a clause id and var id when a conflict is detected. We can look
  up the clause id associated with the var, and return the two clause ids and
  var id as the conflict.
* Given (clause, clause, var, S), it's easy to 'AddConflict' like I said
  above: concatenate clause lists (one of them may be empty) and filter out
  the var. Add that as a new clause.
  
The next part to figure out is how to track all the conflicts, and how to
manage clause ids for learned clauses so that we can refer to them in new
conflicts.

---

Here's how we track the conflicts:

Keep a map from ClauseId@ to Conflict@, which is a global map we pass through
the entire search process.

Simplify returns a Conflict@. When Search sees Simplify return unsat, it adds
the Conflict@ to the conflicts map, incrementing the clauseid. It returns the
updated conflict map and clause id.

When Search sees unsat returned from its variable assignment attempt, there
are new clauses for each ClauseId from whatever was the next clause id to
whatever new clause id was returned. Add those new conflict clauses into the
pre-attempted simplified S@, update the clause id, and pass that to search.

And that's it. Easy.

Changes to state:
* Pass a conflicts map as an extra argument to Search.
  We could put this in Solver@ if we want, or keep it separate.
* Search Result@ for unsat should include a pair of (Conflicts map, new clause
  id).
* Change assigns to a map from VarId to (value, clauseid).

Pretty simple and straight forward.

---

Before we get to implementing the conflict driven clause learning, there's a
stack smash bug we need to fix. Let's see if we can track it down.

Core/Monad/State.fble:17

It's like that all the way down, until it gets to 
Core/Monad/State.fble:18, then Core/Stream/IStream.fble:32

Could it be we have a lazy state monad implementation here, and we need to
make it strict somehow? What would that even mean?

The IStream reference is to GetChar.

Looks like it's the call to GetLines in /Sat/Main%, which reads all the lines
of the cnf into a big string.

It's definitely been implemented in a non-tail recursive fashion.

The easy fix should be to re-implement GetLines so it is tail recursive. The
more general fix might be to parse the cnf and create the solver line by line
rather than all as one big string. We should be able to stream this in.

Let's do the easy fix first.

---

Where do we really need to track the clause associated with a variable for the
purposes of tracking?

The conflict is generated in DoRemoveVarFromClause. We filter out the var from
the op. So who creates that op? It comes from DoAssign.

Where does DoAssign come from?
 - Seeding pure literals. No conflict clause.
 - Seeding unit clauses: We know the clause id there.
 - DoRemoveVarFromClause: We know the clause id there.
 - RemoveClauseFromVar: Pure literal. No conflict clause.
 - Search attempt: No conflict clause.

No need to save this information in a map. We just need to record it as part
of the Assign op.

My vote is to add a second arg to AssignOp@ which is 'why'. Option is:
  +(Unit@ attempt, Unit@ pure, CluaseId@ inferred).

My claim is that we will never get a conflict for 'attempt' or 'pure'. Worst
case, if we do, we can just give ClauseId@ 0 or -1 there. But I'd like to
learn first if we can ever get 'attempt' or 'pure'.

We also want to pass that 'why' to DoRemoveVarFromClause.

---

My claim is apparently wrong, based on what I observe in practice. Let's see
if we can come up with a simplified example where we get a conflict on a
non-inferred clause.

How about this:

0: (a' + b)
1: (a' + b')

And we attempt a to be true.

* Push: assign a=true by attempt.
* assign a=true by attempt
  Push: remove a from 0
  Push: remove a from 1
* remove a from 1 by attempt.
  Push: assign b=false inferred from 1
* assign b=false inferred from 1
  Push: remove 1
  Push: remove b from 0
* remove b from 0
  Push: assign a=false, inferred from 0
* Ignore assign a = false, because we already assigned a.
* remove 1
* Remove a from 0 by attempt
  CONFLICT due to attempt.

Let me see if I can reproduce this in a small test case. Sounds hard, because
of pure literals, etc.

(a' + b)
(a' + b')
(a + b)

There, nothing is pure, nothing is unit, hopefully we pick a true by default.

How should this be handled? We could ignore the conflict, but that's wrong.
There's a real conflict here we should be learning, right?

I wish it went something like this:

* assign a = true.
  ==> (b)(b')
* learn b = true from 0
* see conflict from 1
* conflict clause: 0, 1 on b (a' + a'). So a needs to be false?

If we don't learn any clause here, then my algorithm above won't work, because
we'll keep trying a = true.

This shows another issue: when creating conflict clauses, we should remove
duplicate variables: a' + a' ==> a', a' + a => true

Maybe we should sort all of the variables in a clause by id. Could help speed
up filtering out variables and merging of conflict clauses.

---

Quick digression: how should we detect the case when an empty clause is
provided? We could detect that in SeedOps?

---

Feels like the conflict clause issue would be fixed if we got the ordering of
things right. When I do an 'assign', process everything from that completely
before any other operations.

Same with pure literal? I think pure literal has a race condition like this
too. What if we learn it's pure at the same time we learn it has to be it's
value from another variable assignment? Wait, is that possible?

If a is pure, unlike assign, that doesn't lead to removing vars from clauses?
Well... it does lead to an assignment, but it shouldn't lead to removing vars
from clauses, unlike attempted. So maybe don't need to worry about that.

In summary, I'm hoping that if I process 'attempted' clause removals before
any other, then all conflicts should be from 'inferred' things.

The most natural way to do this would be use fifo instead of lifo for
processing ops. Any easy way to implement a fifo in fble?

---

I implemented a fifo and switched ops to use it. It wasn't hard. Looks like my
claim is holding up better now. What's the next step for conflict driven
clause learning?

* Implement a 'Normalize' function for clauses that sorts variables by id and
  removes redundancies.
 - Add a test cases where we add non-normal clauses.
* Implement a 'RemoveVar' function for clauses that assumes clauses are
  normalized.
* Implement a Conflict function for clauses that joins them, normalizes, and
  removes the var in question.
* Implement a Conflict function for a solver that looks up the relevant
  clauses and then applies the conflict function for clauses to add a new
  clause.
* Add a conflicts field to Solver@ that tracks and applies conflicts.


Is normalizing clauses important or significant?

Clauses are small, I assume, so we don't save much for removing variables.
Unless we learn some pretty big clauses?

What if we have a clause: (a' + a')? We will fail to learn that a is false and
have to wait until a guess instead. Is that a big deal? Or just an
optimization?

I think it's a big deal, because it means, yet again, an attempted variable
assignment could lead to a conflict. I bet I can write a test case for this.

(a' + a')
(a + a)

Right?

Yes.

---

Are we at last ready to add the conflict clause stuff?
* Add map from ClauseId to Conflict@ to solver state.
* Add Conflicts@, ClauseId@ to Unsat return from solver.
 - May want to distinguish between internal Result@ and external Result@.
* Apply conflict clauses after failed assign attempts.

Maybe start by factoring out code for IdMap definition?
  /Solver/Id%
    Id@, EqId, Map, where Map is parameterized by value type and returns a
    struct with methods: Empty, Insert, InsertWith, Lookup, Delete, ForEach.

---

Some details to work out:
* Initial 'unsat' case: I think SeedOps should return unsat right away instead
  of seeding an 'unsat' op. So that the only kind of unsat we can get from
  doing ops is a conflict.

---

We're real close now. I just need to convert the Conflict@ into new clauses,
add those after our failed attempt, and search from where we added the
conflicts.

Is it possible the conflict clause we add turns out empty or unit? Empty would
be bad, but unit should be doable by adding an Assign op?

---

Okay, draft of conflict driven clause learning is done. I'm not totally sure
it's correct. Let's see if we can solve anything now that we couldn't before.

---

Looks like somethings wrong. A problem we could solve before we apparently can
no longer solve:

bench/sat-2002-beta/generated/gen-11/gen-11.2/okgen-c1050-v300-s1804534846-1804534846.cnf
sat: [-17, +149, -146, -34, +158, +125, +47, +203, -152, -117, -88, +43, -175, +37, +199, -154, -222, +91, -137, -113, +153, +242, +145, -41, +64, +237, -196, +108, +53, -189, -20, -111, -65, +210, +57, -129, -74, +86, +215, +138, +44, +106, +235, -23, -9, +120, -195, -121, +105, +218, -18, -217, -85, +183, +144, -150, +206, +245, -166, -133, +93, +19, -171, -239, -226, +131, -10, -231, -172, +42, -25, +83, -240, +139, +56, -140, -241, -246, +39, -232, +219, +148, -78, -60, +251, +248, -70, +112, +238, -228, -80, +249, +244, -159, -31, -11, +193, -208, -225, -180, +82, +24, +192, -141, -67, +103, +209, +161, -22, +63, +223, +28, +102, -151, -13, +30, +1, -135, -197, +32, -48, +7, -243, +35, -122, +29, -233, -84, -116, -14, -52, -58, -134, -207, -119, -187, +96, -162, -101, -130, -68, +165, +94, +160, -247, +143, +72, -168, -115, +89, +54, +185, +181, +250, +66, +27, -21, +127, +155, +95, +252, -87, +204, -71, +163, +114, -188, -236, +169, -142, +73, +194, -76, -90, +2, -26, -253, +224, +8, -157, -99, +40, +167, -254, -201, -220, +200, -227, +118, +49, +221, -36, +33, -174, -213, +81, +229, -230, +75, +77, -110, -124, -255, -109, -256, +257, +59, +38, +177, -178, -92, +258, +259, +261, +262, -190, +263, -156, -104, -164, -12, +265, -126, -173, -198, -170, -5, +55, -205, +202, +212, +15, -3, -184, -50, +61, +51, -234, +62, -186, +266, -123, -107, +267, +268, +147, +269, -46, -6, -182, -214, +98, -179, +100, +270, -16, +216, -176, +271, +272, +273, +274, -260, +275, +276, +277, -97, +279, +280, +281, +191, -211, +282, +283, +284, -4, -69, +285, -79, +286, +287, +288, -278, +289, +290, +291, +292, -128, +293, +294, +45, +295, +296, +297, +298, +299, +300, +264, +136, -132]
0:05.38

Let's use this for debug.

Maybe it's worth starting even simpler, and walking through the code on an
example like:

0: (a + b + c)
1: (a + b + c')
2: (a + b' + c)
3: (a + b' + c')
4: (a' + b + c)
5: (a' + b + c')
6: (a' + b' + c)
7: (a' + b' + c')

Here's what I expect to happen.

1. Nothing to reduce, so attempt a = true.

4: (b + c)
5: (b + c')
6: (b' + c)
7: (b' + c')

2. Nothing to reduce, so attempt b = true.
6: (c)
7: (c')

3. Infer c = true from 6.
Conflicts with 7. 
Conflict 8: (6, 7, c)
Attempt of b = true failed.

4. Add conflict clause after (1).

4: (b + c)
5: (b + c')
6: (b' + c)
7: (b' + c')
8: (b')

5. Infer b = false from 8.

4: (c)
5: (c')

6. Infer c = true from (4).
Conflicts with 5.
Conflict 9: (4, 5, c)
Attempt of a = true failed.

7. Add conflict clause at top level
Interesting that we jump back to the top level here instead of after (4)?
Maybe that's okay and expected? Because we already know (4) is unsat.

0: (a + b + c)
1: (a + b + c')
2: (a + b' + c)
3: (a + b' + c')
4: (a' + b + c)
5: (a' + b + c')
6: (a' + b' + c)
7: (a' + b' + c')
8: (a' + b')
9: (a' + b)

8. attempt a = true
Interesting that we try a = true again. Don't we know a is false by now?

4: (b + c)
5: (b + c')
6: (b' + c)
7: (b' + c')
8: (b')
9: (b)

9. Infer b = false from 8
Conflicts with 9.
Conflict: 10: (8, 9, b).

10. Apply the clause to the top level.

0: (a + b + c)
1: (a + b + c')
2: (a + b' + c)
3: (a + b' + c')
4: (a' + b + c)
5: (a' + b + c')
6: (a' + b' + c)
7: (a' + b' + c')
8: (a' + b')
9: (a' + b)
10: (a')

11. Infer a = false.
At which point, in this case, everything we learned so far becomes irrelevant.

0: (b + c)
1: (b + c')
2: (b' + c)
3: (b' + c')

12. attempt b = true

2: (c)
3: (c')

Conflict 11: (2, 3, c)

0: (b + c)
1: (b + c')
2: (b' + c)
3: (b' + c')
11: (b')

Infer b' = false

0: (c)
1: (c')

Conflict 12: (0, 1, c)

0: (b + c)
1: (b + c')
2: (b' + c)
3: (b' + c')
11: (b')
12: (b)

Infer b' = false.
Conflict 13: (11, 12, b)

Done. Nothing else to try. Final set of clauses:

0: (a + b + c)
1: (a + b + c')
2: (a + b' + c)
3: (a + b' + c')
4: (a' + b + c)
5: (a' + b + c')
6: (a' + b' + c)
7: (a' + b' + c')
8: (a' + b')
9: (a' + b)
10: (a')
11: (a + b')
12: (a + b)
13: (a)

And we have proven unsat by learning a must be both false and true.

I think that is as designed, even though it feels slightly round about that
we attempt a = true multiple times.

I wish I had true printf debugging so we could run through and trace what
happens. I wonder if there's a way to tell gdb to break at a specific point in
code, and whenever it is reached, run a gdb print command and auto continue.
Then we could write a Debug function like:

(String@) { Unit@; } Debug = (String _) {
  Unit@ u = Unit; # break here, print FbleStringValue(_), continue.
  u;
};

According to the internet, I could do:

set print elements 0
break <...>
commands
print FbleStringValueAccess(_)
end

Specifically, trying:

set print elements 0
break /Sat/Solver%.Trace!
commands
silent
print FbleStringValueAccess(_)
continue
end

Looks like there's something wrong with debug info for function args?

There's something very wrong here. The debugger is not showing the right
values for these variables at all. A regression perhaps? I think I have the
right gdb commands, just something wrong with generated dwarf for local
variables?

---

(String@) { Unit@; } Trace = (String@ s) {
  String@ prefix = Str|'P:';
  String@ x = Append(prefix, s);
  x.cons.head.?('A': Unit);
  Unit;
};

Turning off optimization shows:
  gdb 'Str' is actual 's'
  gdb 's' is actual 'x'
  gdb rest are bogus.

Let's see what disassembly shows for this.

1 arg (s)
3 statics (Str, Append, Unit
4 locals prefix, x,

Question: why does it say there are 4 local variables? There should either be
2 (if args not included), or 1.

Anyway, the rest looks okay:

Str is s0
Append is s1
Unit is s2
s is a0
prefix is l1
x is l0

That all looks right.

Why is block->num_locals wrong here?

In theory it's number of local variable slots used/required. It does not
include statics or args. So that should very definitely be 2 in this case, not
4.

Comes from scope->locals.size. Okay. That makes sense. Because some local
variables are anonymous.

In this case:
l2 is x.cons
l3 is x.cons.head

Perfect. bytecode looks good. This must be a problem with dwarf generation
then.

Expect: String name, block1 location

var_tags are 0x86, 0x85, 0x84 based on var.tag.

That should be:
  ??   TAG    reg  dreg  dop     code
  0x86 STATIC x23  23    breg23   0x87
  0x85 ARG    x22  22    breg22   0x86
  0x84 LOCAL  x21  21    breg21   0x85

Looks like I was off by one.

0x70 means reg x0. So 0x87 means reg 23.

Does this explain what we saw? ARG was pointing to LOCAL, STATIC was pointing
to ARG

So, 'Str' is pointing to 's'. Yup.
And 's' is pointing to 'x'. Yup.

And the locals are bogus as expected. Perfect.

How can we add a test for this case? We need to check values for them somehow.

Expect:
  unit is: struct of 0: packed as 0x1
  True is: union tag 0 of 2: packed as 0x3
  light = red is union tag 0 of 3: packed as 0x3

---

Okay! Now that the debug info bug is fixed, using the following kind of thing
works:

set print elements 0
break /Sat/Solver%.Trace!
commands
silent
print FbleStringValueAccess(_)
continue
end

Except, it looks pretty slow, and it prints stuff around the strings, like:

@[Sat.EmptyClause]: $5 = 0x547988 "Assign +1\n"
$6 = 0x547248 "Assign +2\n"
$7 = 0x55f348 "Assign +1\n"
$8 = 0x55f4f8 "Assign +0\n"
$9 = 0x55f238 "Assign -0\n"
$10 = 0x546a88 "Assign +2\n"

We can use this instead:
  printf "%s", FbleStringValueAccess(s)

That looks nice.

Shall we codify this and document it?

---

Next question: what do we want to print to help debug what the solver is
doing? Something like what I wrote above:

* List of clauses numbered by id, in solver passed to Search.
* Every assign op executed.
* When we learn a new conflict.

That will be good I think.

Now: how do we cleanly separate the 'Show' code from the solver code?

Maybe split up the code more:
 * /Sat/Types% (VarId@, Var@, Clause@, Solver@, Assignment@, Result@)
   Or split into /Sat/Var%, /Sat/Clause%, /Sat/Solver%?
 * /Sat/Simplify% (DoOps renamed to Simplify)
 * /Sat/Solver% (NewSolver, AddClause, Solve)
   Or /Sat/Search% and /Sat/Sat% (For high level API?)?
 * /Sat/Show% (all the show functions)

Or, maybe define a Show struct in Solver% and export that?

---

Looking at logs...

* Looks like pure literals aren't being propagated correctly?
  Or I'm labelling it incorrectly? Yeah. Incorrect labelling.

Otherwise, basic tests look good. 

You know, it's really tedious to enter the printf debug commands in gdb... Is
there no better way I can implement Debug.Trace?

Let's try my 3 var unsat example from above, see if logs give any insight.

---

Yes. Much insight. There's a bug. It never finishes. I should add this as a
test case.

{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3]; }
Assign +3 (attempt)
{ 1 => [+2, +1]; 3 => [-2, +1]; 5 => [+2, -1]; 7 => [-2, -1]; }
Assign +2 (attempt)
Assign -1 (inferred from 7)
Assign +1 (inferred from 3)
Conflict 3, 7 on 1
{ 1 => [+2, +1]; 3 => [-2, +1]; 5 => [+2, -1]; 7 => [-2, -1];
  8 => [-2]; }
Assign -2 (inferred from 8)
Assign -1 (inferred from 5)
Assign +1 (inferred from 1)
Conflict 1, 5 on 1

{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3];
  8 => [-3, -2]; 9 => [-3, +2]; }
Assign +3 (attempt)
{ 1 => [+2, +1]; 3 => [-2, +1]; 5 => [+2, -1]; 7 => [-2, -1];
  8 => [-3, -2]; 9 => [-3, +2]; }
Assign +2 (attempt)
Assign -1 (inferred from 7)
Assign +1 (inferred from 3)
Conflict 3, 7 on 1

Look! We failed to apply the assignment through to the conflict clauses.
Make sure when we add conflict clauses we also update the vars data structure!

---

I want to clean things up a little bit so it's easier to reuse code.

AddClauseAt:
* Takes a clause id and vars, does the 'addvars' part, but also does the
  'unit' clause ops part. So we can reuse this code for AddClause and
  AddConflictClause.

SeedOps:
* Change to find pure literals.

Simple, right? Let's see.

No. Too many different assumptions about whether the clause can be empty or
trivially satisfied.

---

Next attempt, now that things should be fixed:

{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3]; }
{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3]; }
Assign +3 (attempt)
{ 1 => [+2, +1]; 3 => [-2, +1]; 5 => [+2, -1]; 7 => [-2, -1]; }
Assign +2 (attempt)
Assign -1 (inferred from 7)
Assign +1 (inferred from 3)
Conflict 3, 7 on 1
{ 1 => [+2, +1]; 3 => [-2, +1]; 5 => [+2, -1]; 7 => [-2, -1]; 8 => [-2]; }
Assign -2 (inferred from 8)
Assign -1 (inferred from 5)
Assign +1 (inferred from 1)
Conflict 1, 5 on 1
{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3];
  8 => [-3, -2]; 9 => [-3, +2]; }
{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3];
  8 => [-3, -2]; 9 => [-3, +2]; }
Assign +3 (attempt)
Assign +2 (inferred from 9)
Assign -2 (inferred from 8)
Conflict 8, 9 on 2
{ 0 => [+1, +2, +3]; 1 => [+1, +2, -3]; 2 => [+1, -2, +3]; 3 => [+1, -2, -3];
  4 => [-1, +2, +3]; 5 => [-1, +2, -3]; 6 => [-1, -2, +3]; 7 => [-1, -2, -3];
  8 => [-3, -2]; 9 => [-3, +2]; 10 => [-3]; }
Assign -3 (inferred from 10)
{ 0 => [+2, +1]; 2 => [-2, +1]; 4 => [+2, -1]; 6 => [-2, -1]; }
Assign +2 (attempt)
Assign +1 (inferred from 2)
Assign -1 (inferred from 6)
Conflict 6, 2 on 1
{ 0 => [+2, +1]; 2 => [-2, +1]; 4 => [+2, -1]; 6 => [-2, -1]; 11 => [-2]; }
Assign -2 (inferred from 11)
Assign +1 (inferred from 0)
Assign -1 (inferred from 4)
Conflict 4, 0 on 1
unsat

Awesome. Looks good.

Now we can re-run the benchmark and see what we can solve now?

Let me just disable the tracing for the moment.

---

Let me categorize sat problems as follows:
* easy: Some version of fble-sat is able to solve.
* medium: minisat is able to solve.
* hard: Neither fble-sat nor minisat can solve.

Where we give, say, a 2 minute timeout. Then it makes sense to focus on what
improvements we get for 'easy', and what new problems from 'medium' we can
solve.

Once I get some numbers back, might be good to enable tracing on some more
easy unsat problems to get a feel for the kinds of clauses we learn and get
insight, if any, into what to try to optimize next.

---

Looks like it's possible for our added conflict clause to be empty. I should
figure out how to deal with that case. And add a test for it.

---

Two ways to deal with debug trace:
1. Write a script to call gdb for us, and pass a function instead of a string
value so we don't have to build the string unless tracing is enabled.

2. Use normal fble code for printing the debug logs, like on a switch.

(2) works for everything: interpreted, compiled, any backend. It's easy to
use.

(1) only works in special cases and is fairly hacky.

So why would I do (1)? Why not do (2)? Just because it's annoying to have
everything be in a monad all the time?

(1) is tricky because we don't have easy access to a heap to call a function
from C code. We need another way to be able to enable/disable debug from gdb.

How would we do (2)? User interface would be:
* --debug flag to fble-sat.
* Solve takes a monad. I suppose all of /Sat/Solver% could take a monad.
  And a function:
   ((Unit@) { String@; }) { M@<Unit@>; } Trace
* We have the choice to pass 'Debug' or 'NoDebug'
  Maybe we have a helper:
  Debug: (Trace@) { M@<A@>; } -> M@<A@>

I don't know. It's tedious, you know?

---

Trace for 3col20_5_4.shuffled.cnf:

Looks like what happens is we end up with learned clauses:

a: [-19]
b: [19]

Because of that, we get two assigns: -19, 19.

Assign -19: should cause 186 to go away, then conflict on 187. But the
conflict is on 19, so then you add [-19 +19] and that ends up as [].

Any easy way to reproduce this? Is it as simple as:

[-1]
[1]

?

But we need those to be learned conflict clauses?

[1 2]
[1 -2]
[-1 2]
[-1 -2]

Why didn't we hit this in any of the test cases already?

Maybe:

[-1]
[1]
[-2]
[2]

Says: attempt -1, get 

[-2]
[2]

learn -2, conflicts with [2]?

---

Here's a minimized case of adding an empty conflict clause:

@[Sat.EmptyConflict]:
{ 0 => [-1, +2]; 1 => [+1, -2]; 2 => [-1, -2]; 3 => [+1, +2]; 4 => [+5, +3];
  5 => [+3, -5, +4]; 6 => [-4, -3]; }
{ 0 => [-1, +2]; 1 => [+1, -2]; 2 => [-1, -2]; 3 => [+1, +2]; 4 => [+5, +3];
  5 => [+3, -5, +4]; 6 => [-4, -3]; }
Assign +5 (attempt)
{ 0 => [-1, +2]; 1 => [+1, -2]; 2 => [-1, -2]; 3 => [+1, +2];
  5 => [+4, +3]; 6 => [-4, -3]; }
Assign +4 (attempt)
Assign -3 (inferred from 6)
Assign -3 (pure) <<< WRONG? This should be inferred from 6, not pure?
{ 0 => [-1, +2]; 1 => [+1, -2]; 2 => [-1, -2]; 3 => [+1, +2]; }
Assign +2 (attempt)
Assign -1 (inferred from 2)
Assign +1 (inferred from 1) <<< WRONG? We shouldn't assign both ways?
Conflict 7: 1, 2 on 1
{ 0 => [-1, +2]; 1 => [+1, -2]; 2 => [-1, -2]; 3 => [+1, +2];
  7 => [-2]; }
Assign -2 (inferred from 7)
Assign +1 (inferred from 3)
Assign -1 (inferred from 0)
Conflict 8: 0, 3 on 1
{ 0 => [-1, +2]; 1 => [+1, -2]; 2 => [-1, -2]; 3 => [+1, +2];
  5 => [+4, +3]; 6 => [-4, -3]; 7 => [-2]; 8 => [+2]; }
Assign -2 (inferred from 7)
Assign +2 (inferred from 8)
Conflict 9: 8, 7 on 2


Hmm... The conflicts are:
 7: 1, 2 on 1 ==> [-2]
 8: 0, 3 on 1 ==> [+2]

It's pretty clear what's happening, right?

We learn from two conflicts: [-2], [+2].
When we add those clauses, each triggers an inferred value.

What do we expect to happen in this case? What we want to happen is to infer
-2, say, do the assignment, which removes +2 from the clause, and we see
things are unsat. Why unsat? Because [-2] and [+2] conflict.

I guess the point is, if you have conflicting unit clauses, the formula is
unsat. But if you happen to be in the right branch of searching, that unsat
gets turned into a conflict rather than giving up.

It's almost like, once again, we want to add an 'Unsat' op somehow?

Maybe unsat should be we learn the empty clause? Keep learning what you can
until then?

Remember how things work today:
1. Try to reduce. If conflict, done.
2. Attempt a variable. If unsat, learn and try again.

What do I want? I think it's fair to try and learn that [-2], [+2] tells us
[]. And to have [] as a clause to learn. But if we learn that clause, give up
right away, right? Just like, if we encounter that in the first place, give up
right away?

Needs thought.

---

I worry about order of processing ops, but I shouldn't. As long as 'assign'
ops are processed in order and they remove the var and future assign ops to
the same var are ignored, it should be fine. 'pure' variable assigns after
inferred variable assigns will be ignored.

What if we allowed there to be multiple conflict clauses learned? Is that
useful? Or would it lead to silly redundancies? The idea being, we return a
list of conflicts, and that list could be empty for trivially unsatisfiable.

Let's not confuse things. Let's say simplification can return three states:

* conflict - a conflict was found.
* unsat - trivially unsatisfiable.
* simplified.

I suppose in this case unsat could be a Maybe@<Conflict@>. If it's trivially
unsat, return Nothing. Otherwise return a conflict.

Add back an op for trivially unsat.

What do you think? This way we can learn the empty clause. That's fine. We
just generate a trivially unsat op for it, and everything is taken care of.

---

Here's what I did:
* Learn empty and unit clauses in AddClause.
* Remove seed ops entirely. We don't seed pure literals, but I don't see a big
  cost to that. First time we see a pure literal we'll apply it. If we attempt
  correctly, great. If we attempt incorrectly... I suppose that means we
  solve a harder problem that we have to. Oh well.

---

Good. What's next? Let's see if we can handle all the easy problems. And then
see if we can handle any medium problems. And then think about what
optimizations to work on next?

I suppose the first thing to do next is double check conflict driven clause
learning is an improvement over simple dpll. How? Count how many more problems
we can solve, and how many fewer problems we can solve, and see what that is.

It would be nice to be able to easily switch back to dpll, or to clearly solve
way more problems now than we did before.

Problems dpll solves but not cdcl:
* bench/easy/rope_0002.shuffled.cnf (unsat 7.82)
* bench/easy/7cnf20_90000_90000_7.shuffled.cnf (sat 25.68)
* bench/easy/6cnf20_28000_28000_3.shuffled.cnf (sat 24.17)

Just those three. Not bad. Two of them a 'sat' after a while, so I don't
really mind. We could have just gotten lucky before with dpll.

The unsat one is interesting. That's worth looking at I think.

Maybe, let's see if I can reproduce the fast solve with dpll. I think we can
revert to dpll by ignoring learned conflict clauses and going to 'false' once
'true' for a variable fails.

Yes. I can repro the fast solve with dpll. Also, it's pretty easy to switch
between dpll and conflict. The differences are:
1. Do we learn conflict clauses or not.
2. Do we go straight to trying var = false or not.

