State Monad Performance
=======================

Here's the do function for state monad:

  R@<A@> ra = ma(s);
  f(ra.x, ra.s);

ra is a struct. If ma is a recursive call, we allocate the struct in ma,
GcRealloc it on return, then access its fields and throw it away. The
GcRealloc is costing us too much. It's no good to do an allocation every
iteration of a tightly nested loop that we are just going to throw away right
away.

Now, if we had code like:
  
  M@<String@> GetLine = {
     Maybe@<Char@> mc <- m.do(I.GetChar(in));
     mc.?(nothing: m.return(Str|'');
     String@ tail <- m.do(GetLine);
     m.return(Cons(mc.just, tail));
  };

We run into this, because GetLine is in the 'ma' args slot to do in the
recursive call. If we were okay getting the reverse list, we could instead
write this as:
     
  (String@) { M@<String@>; } GetLine = (String@ read) {
     Maybe@<Char@> mc <- m.do(I.GetChar(in));
     mc.?(nothing: m.return(read);
     GetLine(Cons(mc.just, read));
  };

In this case, the only call to do has GetChar in the ma slot, and I assume we
merge stacks on that because it's not a recursive call, so we don't pay
any GC cost for the intermediate result value.

Is it faster to write the code like this, and insert a Reverse call in
m.return? Probably. I should try it. I wouldn't like it to be though.

Can we come up with a better solution that doesn't require rewriting code?

What we need is for the intermediate struct value to be allocated on the
callers stack. Options:
* merge stacks so that callee stack is shared with caller.
  Can't do this forever, blows up memory use. But maybe we only need it one
  layer at a time or some such?
* ???

---

Options:
A. Copy the allocated struct from the callee stack to the caller stack.
B. Allocate the struct into the caller stack directly.
C. Allocate the struct at the start of the callee stack, and bump the caller
stack past that to include it on pop frame.

(C) sounds pretty reasonable to me conceptually. A function hopefully knows
what it is allocating to be returned to the caller versus what is just
temporary. Hopefully it can reorder things or reserve space or something to
put the object where it needs to be right away. Then, in FblePopFrame, we have
the ability to copy back any size object on the stack to the caller.

Currently we have a fairly elaborate frame structure that makes this
challenging, because it gets in the way. You could imagine keeping the frame
structure on a separate stack so that isn't an issue.

I want to step through the code with a debugger to get a better sense of what
it looks like. Just in case there's something going on I'm not aware of.

Observations from gdb:
* The stack is absurdly deep. What's with that? #2138 frames deep.
* The PartialApplyImpl is happening to /Core/Monad/State%.Monad!
  Specifically the do function. This must be us supplying the final state
  argument.
* The 'ma' argument is the PartialApplyImpl for Monad itself.
* The 'f' argument is /Pprof/Perf%.Parse!.GetLocs!
  Which is the otherwise anonymous function from Maybe@<Char@> to
  M@<List@<String@>> in GetLocs.

Note that we don't do any allocations in GetLocs directly. It's all via helper
functions like return, Cons, Str, List. That's means for an approach where we
allocate to the front of the frame to work, it has to be able to go through
multiple layers of functions.

Should I experiment again with FastCat to see if we see similar performance
issues there? Or just to have something much similar to better understand
what's happening?

Well, first I need to fix fastcat, because it crashes. I should add a test for
it too.

---

/usr/bin/time -v cat < scratch/pprof.input.txt > /dev/null
  Elapsed (wall clock) time (h:mm:ss or m:ss): 0:04.36
  Maximum resident set size (kbytes): 1428

Looks like there's a memory leak or bug in the --monadic option to fast-cat. I
should fix that. Also, let's truncate the input file to just the relevant
lines.

/usr/bin/time -v cat < scratch/pprof.input.txt > /dev/null
  Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.05
  Maximum resident set size (kbytes): 1348

/usr/bin/time -v ./out/pkgs/core/fble-fast-cat < scratch/pprof.input.txt > /dev/null
  Elapsed (wall clock) time (h:mm:ss or m:ss): 4:04.76
  Maximum resident set size (kbytes): 1760

What's the memory leak in the --monadic case? How do I track that down?

Let's do some profiling, shall we? Maybe that will tell us.

Why are my stacks so deep? I don't expect that, right? Or maybe I do?

(World@) { R@<Bool@>; } MCat = m.do(Stdin, (Maybe@<Int@> byte) {
  byte.?(nothing: m.return(True));
  Char@ char = /Core/Char/Ascii%.Chr(byte.just);
  Unit@ _ <- m.do(stdout_(/Core/Char/Ascii%.Ord(char)));
  MCat
};

MCat is a partial application of do, because we don't have state at this
point. The state comes from the stdio runner. We were over-applied on the
state though, so once we return the partial apply from the monadic.? case
statement, we should then apply the state. What happens?

We call do body:
  r = Stdin(w)
  tail call into (Maybe@<Int@> byte) { ...}

The byte is not nothing.
  Char@ char = /Core/Char/Ascii%.Chr(byte.just);
  m.do(stdout_(char), MCat);

But we should have over applied. And m.do should be a tail call. The stack
shouldn't be going so deep here. What's going on? Let's step through the code
and see?
  
What I see is that PartialApplyImpl isn't doing a tail call, so we end up
smashing the stack and not cleaning other stuff up while we are at it.

This is clearly a bug. I should be able to capture it in a memory test. The
case is doing a recursion where the tail call is applied to a partially
applied function.

---

Okay, that bug is tested and fixed. Unfortunately it doesn't solve the
problem. I'm still leaking memory in fble-fast-cast --monadic.

For the fun of it, let's see if that impacted the performance of pprof at all.

  /usr/bin/time -v ./out/pkgs/pprof/fble-pprof-report < scratch/pprof.input.txt

  Elapsed (wall clock) time (h:mm:ss or m:ss): 9:23.42 ==> 9:04.10
  Maximum resident set size (kbytes): 254964 ==> 253564
 
Anyway, we're still smashing the stack in fble-fast-cat. Here's the sequence:

We're doing a TailCall. The result of the tail call is a function. But we have
an unused argument.

Can I figure out what the result of that tail call is? That's what I really
want to know.

It's PartialApplyImpl, /Core/Monad/State%.Monad!. I bet it's our MCat function
being returned?

How do I make a test case for this? The case is:
* We are doing a tail call with over-applied args.
* The result of that tail called function.

What should happen is we continue in the TailCall loop, if we have enough
args?

Anyway, first step is to add a regression test. How do I make this? Can I
understand where it's coming from in the first place?

MCat returns a function. The state argument is presumably what's overapplied.
Any reason the test I just wrote for partial tail call memory doesn't hit this
case?

Overapply means a function that returns a function, without being defined as a
function itself.

I need a tail call that returns a function. For it to be a call means it takes
at least one argument. For it to return a function means it takes at least two
arguments. The recursion has to be controlled by Nat@ to know when to end, so
that has to be the first argument. The second argument can be Unit.

So, our function takes a Nat@, and returns a function from Unit@ to Unit@.
That's easy enough.

The real question is, can I convince myself this should be doable in constant
memory? Let's see.

f(5), tail calls f(4). That's constant.
f(4) tail calls f(3). That's constant.
...
f(1) returns Id function.
Then we call the Id function.

Yes. This should be constant? So long as we GC, right? We have to GC each
partial application of f? No. There are no partial applications of f. It has a
single argument. You can't partially apply that.

Yeah, this should be clearly constant memory use.

And... it is. We didn't catch the case I was hoping to apparently. Maybe
because we aren't over-applying every step of the way? That makes sense. How
can I overapply every step of the way?

Okay, I maybe got it. Let's double check.
  
(Nat@, Unit@) { Unit@; } f = (Nat@ n) {
  S@ s = S(n);
  s.?(Z: (Unit@ u) { u; });
  (Unit@ u) { f(s.S, u); };
};

f(4) returns a function that references f and s. We tail call that function.
f(3, u) is our recursive overapply. It returns a new function.

We are certainly allocating new functions at every iteration. But GC should
take care of us, right?

Let me get into the stack if this specifically and see what happens. Yes, the
stack looks like the fble-fast-cat case.

Now, how do I solve this one?

We have a function. We have only unused arguments. Where do those unused
arguments come from? What do we know about the function?

The real fear is that the function itself or some of the unused argument
values live in the current frame. I need to copy them out. Have we already
copied them out? We already take care of that I think. Let me see.

Looks like it's working. Awesome. Let's see if that was the last of the
fble-fast-cat memory leak. Yup! We got it. Cool.

/usr/bin/time -v ./out/pkgs/core/fble-fast-cat --monadic < scratch/pprof.input.txt > /dev/null
  Elapsed (wall clock) time (h:mm:ss or m:ss): 5:07.26
  Maximum resident set size (kbytes): 1716

The non-monadic took 4 minutes, so I expect at least that much time spent
here.

And for the fun of it:

  /usr/bin/time -v ./out/pkgs/pprof/fble-pprof-report < scratch/pprof.input.txt

  Elapsed (wall clock) time (h:mm:ss or m:ss): 9:23.42 ==> 9:04.10 ==> 9:19.40
  Maximum resident set size (kbytes): 254964 ==> 253564 ==> 253444

Oh well. You win some you lose some.

---

25% overhead for monadic version of fast cat actually isn't too bad. I think
it's worth look at fast cat in more detail performance wise to start. See
if/how we can optimize it.

First, the non-monadic version.

