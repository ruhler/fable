Abstract Data Types
===================
Perhaps the most significant deficiency in the expressive power of fble today
is that there is no way to define an abstract data type. Anyone who has access
to the name of your type or a value of your type and can guess what the data
structure of the type is can construct and access internal implementation
details of the type.

We want some way to say the fields of a data type are private to the developer
of the data type. The developer can use them to construct, access, and
condition on, but nobody else can. The developer can expose the functionality
that fields provides to users in a controlled fashion by providing helper
functions to construct and deconstruct the data type.

There are two kinds of access control mechanisms in fble today: scopes and
private modules. I use scopes all over the place. I don't yet use private
modules, because I can self enforce. In theory I can self enforce on accessing
fields of types that should be private, but in practice it isn't working out.

Is there any simple access mechanism we can provide to make it easy to specify
private data types? In particular, there should be a way to share the
internals to a hierarchy of modules.

A strawman proposal is to have syntax such as:
  @ Foo@ ?= *(...);
  ...

Where the fields of Foo@ are visible in the body of the let, but not outside
of the body of the let. The trouble here is you don't have a way to make the
fields visible to a hierarchy of modules.

It would be a huge pain to have to pack and unpack data types all over the
place if we were to have two different types FooImpl@ and Foo@ that we could
convert between.

Using a data type like C++ where the fields are member functions also seems
tedious. But maybe it's worth a try? For example, how would I implement the
Map@ data type? And what's to keep the user from providing their own function
implementations that mess up a bunch of invariants?

How can we say: if you have access to some thing X, then you can use the
fields for a type T, otherwise you can't. It's almost like we want different
names for the type and how you access its fields. But today you can access its
fields without any name at all, just from the value of the type.

Let's imagine when we define a type we get two names: the type name, and
something representing the fields of the type.

Foo@ and FooImpl@.

Seems a lot like we just want two different types and a way to convert between
them:

Foo@ - the publicly visible type.
FooImpl@ - the private type.
FooToImpl: Foo@ -> FooImpl@
FooFromImpl: FooImpl@ -> Foo@.

Then we can say:

Foo@ f = FooFromImpl(FooImpl@(...));
FooToImpl(f).x;
FooToimpl(f).?(...)

That's not too bad if we provide a special syntax and only ever use the
accessors in the leaf instances. That suggests we maybe want a type Foo@, and
the access for the type. It's when you combine the two that you can access the
type. The special syntax can be, any time you use .x, .?, or Foo@(...), it
requires the accessor for the type be in scope. We just need a different name
for the type and the accessor so that we can choose to export one or the
other? Or just some way to choose to export the accessor with the type or not.

But what's to keep a user from guessing the value of the accessor and using
that themselves? I think we need a notion of a type name, where each new type
name is unique. Something like:

@ T1@ = ABSTRACT(T@);
@ T2@ = ABSTRACT(T@);

Where in this case, T1@ and T2@ are considered incompatible types. And really
we want two identifiers:

@ T1@, T1.* = ABSTRACT(T@);
@ T2@, T2.* = ABSTRACT(T@);

If T1.* is in scope, you have access to the fields of type T1@.
If T2.* is in scope, you have access to the fields of type T2@. T1@ and T2@
are incompatible types.

So, your implementation module would export T1@ and T1.*. Your public API
would export only T1@.

We shouldn't need to reference T1.* explicitly, so no need to pick a syntax
for it. Just as long as we can indicate whether to export it or not. Maybe we
just need a notion of an abstract type with the following operations:

NEWTYPE(T@) - creates a unique new type with the same interface as the type T@
  (but is considered a different type).
OPAQUE(T@) - returns a type considered identical to T@, except no field access
  is allowed.

Both are needed. If you just had NEWTYPE, you have full access to the type. If
you just have OPAQUE, you could always construct things using T@, which can be
used for an identical type.

Type values are a thing, so this should be semantically meaningful in terms of
identifying types. All I need is to add a couple of syntactic expressions.

To summarize:

 T@ is the normal, fully accessible, create out of thin air type.
 NEWTYPE(T@) is the unique, fully accessible type that cannot be created out
   of thin air.
 OPAQUE(NEWTYPE(T@)) is the unique, opaque type that cannot be created out of
   thin air.

Possible syntax for NEWTYPE(T@): &T@
Possible syntax for OPAQUE(T@): T@.?    (static view of T@)

For example:

@ Point@ = &*(Int@ x, Int@ y);
@ Point@ = &+(Int@ x, Int@ y);
@ Point@ = &Foo@;

@(Point@: Point@.?);

Is it okay that & is part of an expression and not a statement? How could you
ever use the result of & unless you take the name of it? You could pass it to
a struct value and access it by field. So that's fine.

Should it have parens? &T@ vs &(T@)? Do parens suggest it is more like a
function? That's why I lean towards no parens.

Will Foo@.? conflict with Foo.?(...) syntax? Should it bee Foo@.& instead?
Things to experiment with anyway.

Note that this idea depends on a guarantee that when you include a module
value, you get a reference to the unique module value and not a separate copy.
I think that's reasonable. Certainly with today's implementation of
modules. In other words, to the extent you are guaranteed unique values at
runtime, this should work for types. The big difference being that uniqueness
of values is now more than just a question of performance.

---

Discussion on NEWTYPE.

For newtype to make sense, you have to have some way to associate the newtype
with values constructed of that type. This makes sense for struct and union
types, because you specify the type explicitly when constructing them. It
doesn't make sense for function or process types, because there's no way to
specify the newtype when constructing them. It also doesn't make sense for
implicit type struct values.

So let's restrict the discussion of newtype to struct and union types. I
think that's fine. You can always wrap any other type in a single argument
struct type if need be.

If we ignore polymorphic types and implicit type struct values for the moment,
I could argue that union and struct types should always be newtype.

@ Foo@ = *(Unit@ x, Unit@ y);
@ Bar@ = *(Unit@ x, Unit@ y);

Don't consider Foo@ and Bar@ to be equal.

You only need types to be equal when sharing them across APIs, but in that
case, you can share the type declaration as well. You might even say you ought
to share the type declaration as well. So there's no harm making them always
newtype. The language would be simpler then, and we wouldn't need a special
operator at all for newtype.

The challenges with this view are:
* What about implicit type struct values?
* Does polymorphism still work this way?
 
Implicit type struct values are used for:
* Defining namespace structs, where you don't want to specify the types
  explicitly.
* Convenience for returning pairs from functions.
* Convenience to avoid naming a type like a pair explicitly.
* Convenience for specifying a value with explicit field names, or implicit
  field names.

We could say implicit type struct values give you the canonical implicit type,
but then you need to name that canonical implicit type. That's the same as
saying a struct type can be either newtype or not.

We could say implicit type struct values give you a newtype. You can recover
the type using typeof. For namespace structs, that should be fine. There's no
name for the type anyway. You do miss out on the convenience use cases, but
honestly those seem like minor losses.

Here's a survey of where I use the convenience cases today:
* Unnamed tuple field of a compound type. (4)
* Type inference, to avoid writing out a type name.
* To list explicit field names when constructing a value, for clarity. (2)
* To avoid naming a pair type when constructing it. (3)
* To return a pair type from a function without having to name the type. (2)
* For defining character literal specs (not a convenience case).

So, mostly to avoid naming a type and to explicitly list field names. Nice
conveniences we could do without. Maybe we could compromise and allow field:
value syntax in the explicit type struct value for clarity? And you could
always use a single letter named type like P@ for tuple fields you don't want
to write out.


How would polymorphic types work with newtype?

<@>@ Maybe@ = <@ T@> { +(T@ just, Unit@ nothing); };

Maybe@<Foo@> has to be considered the same as Maybe@<Foo@>. But suddenly it's
not clear. Because if you do the substitution, you would think you'd end up
with +(Foo@ just, Unit@ nothing) versus +(Foo@ just, Unit@ nothing), which
under newtype interpretation would be different types. In other words, things
get confusing with polymorphic newtype. Very confusing.

Given we'll often want to have polymorphic abstract data types, this is
something we have to deal with regardless of whether we always or only
sometimes use the newtype interpretation. Polymorphic types work by creating
existing types out of thin air. Abstract data types work by preventing that.
How can they be reconciled?

The only way that comes to mind is to treat polymorphic type constructors
specially, to say the 'newtype' is associated with the polymorphic type
constructor, not the constructed type. But that just doesn't make sense from a
semantic point of view.

I'm starting to think the newtype approach is incompatible with the existing
language design. Perhaps I should start exploring other ways to support
abstract data types.

---

How about the functional interface approach?

Let's say I have an internal struct type:

@ Foo@ = *(A@ a, B@ b)

I want to expose some methods on it:

 Foo, A, X

Hmm...

I guess the point is, any public API for constructing or accessing a type
would have to be part of the type.

Consider the Map@ type as an abstract type. It gets pretty big if we have a
separate field for each method on the type :(. I really just want to say: you
can name the type, but you can't construct, condition, or access fields from
it.

Imagine a special abstract type declaration for struct types:

% Foo = *?(A@ a, B@ b);

Where: Foo._@ is ...

Imagine a completely different way of defining struct and union types:

Foo@, Foo(A, B) = *(A@, B@);

Where Foo@ is the name of the type.
Foo is a function to construct values of the type.
A and B are functions to access fields of the type. You can pick any names you
want for Foo@, Foo, A, and B. A and B would be different kinds of things than
normal functions: they would be tied to type type Foo@ and could only be used
as field accessors. Or maybe we should have a general way to define fields for
a type?

Foo@, Foo, .A, .B = *(A@, B@);

We could come up with a nicer syntax. The key point is just that the name of
the type is different from the constructor function of the type, and that
fields are a special kind of thing. Declaring the type gives you a type name,
a constructor function, and all the field names. Then you always use the
constructor.

Foo@ = *(A@ a, B@ b);

Anyway, regardless of the syntax, the important point here is that Foo@ is a
newtype. Otherwise someone could define their own type Bar@ with the same
fields as Foo@ and they would be the same types. So we need to answer the
question of polymorphism. And I still don't have an answer for it.

Maybe the best way to move forward is first figure out how bad it really is to
use a class method based approach.

---

A class method based approach for Map@.

<@,@>@ MapImpl@ = <@ K@, @ V@> { ... };

<@,@>@ Map@ = <@ K@, @ V@> { 
  *(
    (K@) { Maybe@<V@>; } Lookup,
    (K@, V@) { Map@<K@, V@>; } Insert,
    ...
   );

To implement it is pretty easy. Define 

<@,@> LookupImpl = <@ K@, @ V@>(MapImpl@<K@, V@>)(K@) { ... };

Then you can write your own constructor:
<@ K@, @ V@>(MapImpl@<K@, V@>) { Map@<K@, V@>; } Map =
  <@ K@, @V@>(MapImpl@<K@, V@> impl) {
    @(Lookup = LookupImpl(impl),
      Insert = InsertImpl(impl),
      ...)
   };

It's tedious, but it gives you full control over the public abstract
operations of the data type. The 'primitives' of the type, if you will. No
extra language support needed.

The downside is that you are now passing around a struct full of functions for
each instance of Map@. If there are N functions, now a map takes O(N) space
before you've put anything on it. And it means you can have a dynamic
implementation for the map type at runtime, which is perhaps a little more
powerful than I want. For example, even if the internal data type can be
packed into bits, the abstract data type requires you store functions.

---

What if we could have user defined keys for types. You can construct,
condition, and access values of a type, but only if you have access to the
key. To solve for polymorphism, we allow multiple different types to share the
same key. Maybe there is an implicit default global key.

When I declare a type, I give my own key. Keys are unique.

Or, even easier, I just use the same access controls in use for module
visibility for types field visibility.

Define a type in a public module, the type is public. Define a type in a
private module, the type is abstract. Because it uses existing hierarchy, it's
easy to give restricted access to the type to multiple modules.

I guess the difference is, normally you can re-export internal functions to
make them public. Now your saying you have to define those functions in a
public module? No. Better if we can have a key that we can export like any
other function.

So, every struct and union type is declared with a key. You can access the
type if you access to the key in scope. The key can be passed around as a
field of a struct for you to export?

But then, we could pick a completely different syntax that makes things look
like they are today. Bundle the key with the type name, and give two versions
of the type name: one with the key, one without.

And I suppose the fundamental idea here is that if you define the same type
with different keys, they are considered different types. That's how you can
get away with polymorphism.

---

Proposed solution to newtype polymorphism: define equality of newtypes based
on the syntactic location where the type is defined. If it uses the same
syntactic type declaration, it's the same type. It's slightly, but not
entirely convoluted. We just identify the type with its declaration's initial
syntax node.

With that problem solved, I propose we separate the concept of newtype from
struct and union type. Think of a newtype as an opaque single field struct
type.

So we only introduce a single concept: opaque type and opaque type
construction.

An opaque type is one that behaves like an type variable - you can't do
anything with it except refer to it and test for equality. An opaque type is
associated with an internal concrete type. Two opaque types are considered
equal if their internal concrete types are equal and they share the same
opaque type declaration syntax node.

You declare an opaque type using an opaque type declaration statement:
  
  Expr ::= opaque_type (typename :: Name) (to :: Name) (from :: Name) (body :: Type)

This adds to the scope three new entities:
  'typename' is a type variable referring to the newly declared opaque type.
  'to' is a variable referring to a function that converts values of body type
       to the opaque type.
  'from' is a variable referring to a function that converts values of opaque
       type to the body type.
  'body' is the internal type associated with the opaque type.

For example:

  MapImpl@ = ...;

  Map@, ToMap, FromMap &= MapImpl@;
  ...

Anyone who has access to ToMap and FromMap can make use of the internal type.
Otherwise, even if you have access to Map@ and MapImpl@, there's no way to
extract the internal value out of an opaque value.
  
I think this is straight forward, orthogonal to the existing language, so we
don't make things super complicated and don't loose and features of the
existing language. It relies on existing scoping mechanisms for access
control. The only downsides I see are minor: the weirdness of defining
equality based on syntactic declaration, a little tediousness of having to
convert to and from the opaque type everywhere, and uncertainty as to what a
nice concrete syntax to use is.

Here's how to make this work on a polymorphic type:

<@>% OpaqueList = <@ T@> { 
  List@, ToList, FromList &= ListImpl@<T@>;
  @(List@, ToList, FromList);
};

<@>@ List@ = <@ T@> { OpaqueList<T@>.List@; }

So let's brainstorm concrete syntax.

A. It's similar to link statements, in that you define multiple values in one
construct. We can do something similar as suggested above. It's just
syntactically awkward. Directly analogous would like something like:

  MapImpl@ & Map@, ToMap, FromMap; ...

B. Return a namespace struct with predefined field names. This same approach
could be used for link expression too:

  ~(Foo@) could return a value of type:
      *(Get@<Foo@> get, Put@<Foo@> put)!
  &(Foo@) could return a value of type:
      *(@<_@> _@, (Foo@) { _@; } unpack, (_@) { Foo@; } pack)

Now you can use link like any other process with exec:

  Link@<Foo@> foo := ~(Foo@);
  foo.put(...);
  foo.get(...);

That's pretty nice. Note that we can define a helper type Link@ to name the
type.

And abstract types:

@ Foo = &(MyFoo@);
Foo._@ foo = Foo.pack(MyFoo@(...));
MyFoo@ = Foo.unpack(foo);

In this case, there isn't a way to define a helper type to name the type.
 
C. Return a namespace struct with user provided field names.

  @ foo := ~Foo@(get, put);

  @ Foo = &Foo@(T, pack, unpack);

I like this less. It's not clear from the syntax what it means. Is it so bad
to hard code names into the language?

D. Return a polymorphic function that gets the values being declared?

I'm just thinking that the same approach could be used for list literals too,
instead of turning them into polymorphic functions.

When it comes down to it, when returning multiple results, we can either
distinguish them by name or by position. Why is position better than name?

Well, I suppose we need to specify position regardless. Why not name too?

For link, 'put' and 'get' are obvious names to use.

For list literal... we either leave it as is, or define a custom list type.
Honestly, I'm fine leaving as is. The custom list type does not have obvious
names to use.

For opaque type... we don't have obvious names to use.

If we did everything like list literal, then we could say:

Link: you give a function from (Get@<X@>, Put@<X@>) { T@! }, and we'll give
you a T@!. In other words, if Link were expressed as a pure function, it would
be:
  ((Get@<X@>, Put@<X@>) { T@!; }) { T@!; }

Then we would write things like:

  Foo@ ~ get, put; ...(get, put);

Becomes:
  ~((Get@<Foo@> get, Put@<Foo@> put) { ... (get,put); })

Which is fine, except we introduce a level of nesting that's rather annoying.
It's better if we have bind syntax like was proposed previously:

Get@<Foo@> get, Put@<Foo@> put <- ~<T@>();
... (get, put);

List literal could use this too? No. List is something different. It takes
both a function to construct list elements and a tail. Maybe you have to
provide the literal and the tail. Then you could write something like:

  MyList@ l = {
    (T@ x, MyList@ xs) <- [a, b, c](MyNilList);
    MyCons(x, xs);
  };

But that doesn't add any clarity about anything. Easer just to pass the list.
It's more a case of we give the pieces, list literal fills things out. Where
as link is we fill things out, list literal gives the pieces.

Opaque types are like we fill things out, it gives the pieces.

Note: a user library could be used to rename get and put for link. But user
libraries can't be used to hide opaque declarations, because where the
declaration appears matters. You cannot hide it behind an abstraction. That's
the problem with that kind of 'this depends on where it appears' thing. It
doesn't compose well with other abstractions.

The trouble with a bind syntax is you have to be explicit about the return
type.

Brainstorm for opaque namespace struct field names:

  Foo._@ foo = Foo.'<'(MyFoo@(...));
  MyFoo@ = Foo.'>'(foo);

  Foo._@ foo = Foo.t(MyFoo@(...));
  MyFoo@ = Foo.f(foo);

  Foo._@ foo = Foo.to(MyFoo@(...));
  MyFoo@ = Foo.fr(foo);

  Foo._@ foo = Foo.to(MyFoo@(...));
  MyFoo@ = Foo.from(foo);

  Foo.T@ foo = Foo.pack(MyFoo@(...));
  MyFoo@ = Foo.unpack(foo);

  Foo.T@ foo = Foo.hide(MyFoo@(...));
  MyFoo@ = Foo.unhide(foo);

  Foo.T@ foo = Foo.to_(MyFoo@(...));
  MyFoo@ = Foo._to(foo);

  Foo._@ foo = Foo._(MyFoo@(...));
  MyFoo@ = Foo.__(foo);

  Foo._@ foo = Foo.'->'(MyFoo@(...));
  MyFoo@ = Foo.'<-'(foo);

  Foo.T@ foo = Foo.put(MyFoo@(...));
  MyFoo@ = Foo.get(foo);

---

Could we make the polymorphic case a little nicer by having builtin support
for abstract polymorphic data types?

Let's say you want an opaque version of some type Foo@ of kind <@>@. Should
that give you an opaque type of kind @, or an opaque type of kind <@>@?

Before I guess I was thinking you would only give it a type of kind @. But why
not allow other kinds? Given a type MyFoo@ of kind <@>@, we could give you:

* Foo@ opaque type of kind <@>@.
* <@ T@>(Foo@<T@>) { MyFoo@<T@>; } get function
* <@ T@>(MyFoo@<T@>) { Foo@<T@>; } put function

Implications of this:
* We store values of kind @, not of kind <@>@. If you want to store values of
  kind <@>@, I guess you could wrap them in a struct value of kind @?
* There is clearly a single opaque type for all polymorphic instantiations of
  the type, so we work our way out of that mess.

Then we can say things like:
  MyFoo@<Int@> myfoo = ...;
  Foo@<Int@> foo = Put<Int@>(myfoo);
  MyFoo@<Int@> myfoo = Get<Int@>(foo);

Contrast this with an alternate, perhaps more natural, interpretation:

* Foo@ opaque type of kind @.
* (Foo@) { MyFoo@; } get function
* (MyFoo@) { Foo@; } put function

Is this meaningful? I don't think it is. The kinds don't match up.

We can store a value of kind <@>@ as a field. We can pass it around. But can
we use it in a polymorphic function that expects something of kind @? That's
an interesting question. Can I work with lists of polymorphic values?

No. We get a kind error. You can't pass something of kind <@>@ when something
of kind @ is expected. But you can always wrap it in a struct value. That's
weird. Is that a bug?

Let's assume that's working as intended. Does that tell us anything about how
we ought to make opaque polymorphic types?

The struct trick means we always have a back door for the case when we
want to store polymorphic values in the opaque type.

How about:

* Foo@ opaque type of kind <@>@.
* (Foo@) { MyFoo@; } get function
* (MyFoo@) { Foo@; } put function

That's a natural conversion. Does it give us what we want to put and get?

I want to put a MyFoo@<T@>, but I can only put MyFoo@. So... no, that doesn't
give me what I want.

Let's say we use the polymorphic aware translation:

* Foo@ opaque type of kind <@>@.
* <@ T@>(Foo@<T@>) { MyFoo@<T@>; } get function
* <@ T@>(MyFoo@<T@>) { Foo@<T@>; } put function

What would be a nice syntax to get names for Foo@, Get, and Put functions
given the type MyFoo@? It's annoying to go through a hard coded field named
namespace struct value. I would rather list them directly.

More brainstorming:

  Map@ &= MapImpl@(Put, Get);
  MapImpl@ & Map@, Put, Get;
  
The last one is kind of nice. It's consistent with the link statement: info
type followed by names to declare. And this makes it look kind of like we are
defining the abstract interface to the MapImpl@ type as Map@, Put, and Get. I
like that. Let's just do the put and get in the same order as the link
statement. If it turns out we can find a better syntax, we should use that
better syntax for link statement too.

Abstract Syntax:
  Expr ::= abstract_type (type :: Type) (typename :: Name) (get :: Name) (put :: Name) (body :: Expr)

Concrete Syntax:
  stmt ::= type '&' name ',' name ',' name ';' stmt

Example:
  FooImpl@ & Foo@, Get, Put;
  ...
  @(Foo@);


I think that's enough of an initial proposal to get started specing and
implementing this up. This should maybe go just after the section on
polymorphism, because we need to know about polymorphism to explain the
translation. And as we have seen, abstract types are tied up pretty
significantly with polymorphism in terms of making sense semantically, so that
seems reasonable. Maybe even it should go in the section on polymorphism?

---

Let me try to summarize the outcome of the above discussion, now that it's
been a while and I'm starting to loose track.

Abstract Syntax:
  Expr ::= abstract_type (type :: Type) (typename :: Name) (get :: Name) (put :: Name) (body :: Expr)

Concrete Syntax:
  stmt ::= type '&' name ',' name ',' name ';' expr

Example:
  FooImpl@ & Foo@, Get, Put;
  ...
  @(Foo@);

Define an abstract type based on a concrete type value. In this example,
FooImpl@ is some possibly polymorphic type in scope. This statement adds three
new entities to the scope:
  
  Foo@ - An abstract type.
  Get - A (possibly polymorphic) function to convert a value of type Foo@ to a
        value of type FooImpl@.
  Put - A (possibly polymorphic) function to convert a value of type FooImpl@
        to a value of type Foo@.

The recommended use case is:
* Define your internal data type FooImpl@.
* Use the expression to define the public interface type Foo@.
* Pass values of type Foo@ around everywhere, but when you want to allocate or
  access fields, convert first using Put/Get.
* Only make Put/Get functions available to privileged modules.
  
A user could make up their own copy of the FooImpl@ type out of thin air, but
they can't use that to create or extract any information from a value of type
Foo@.

---

Explicitly having to convert between types seems annoying. Why not allow use
of FooImpl@ internally as equivalent to Foo@, but not externally? Let's
explore a little more.

I want two type names I can use interchangeably, where one of them is opaque
and unique, and the other is non-opaque and unique. How about, then, we have
something that, given a type, returns back two new types that are equivalent,
where one is opaque and one is not? And it knows how to deal with polymorphic
types. And we could use special syntax to access the abstract variant of a
type.

Something like:

<@,@>@ Map@ = & <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };

Here, MapP@ can be a normal type creatable out of thin air.

Now we have:

Map@ 
  - a unique type. The constructor has to be used explicitly to create
    values of this type. Values of this type can be accessed. This is the type
    that will be used by the map implementation.

Map@.& 
  - a unique type equivalent to Map@ type that can be used interchangeably
    with the Map@ type, but that is opaque.

The implementation of Map will export Map@ for internal use and Map@.& for
external use. Everything else works just fine.

There's no way to produce a Map@ out of thin air. In particular the following
types Map1@ and Map2@ are not considered equal:

<@,@>@ Map1@ = & <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };
<@,@>@ Map2@ = & <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };

Note that:
  <@>@ Foo@ = & <@ X@> { ...; }

Means something different from:
  <@>@ Foo@ = <@ X@> { &Foo@; }

The former has that Foo@<X@> is equal to Foo@<X@>. The second has that
Foo@<X@> is different from Foo@<X@>, making the second rather not useful.

Perhaps instead of making & an expression, it would be better in statement
form:

<@,@>@ Map@ &= <@ K@, @ V@> { ... };

Now it is clear we are associating the uniqueness of the type Map@ with the
name Map@.

Cool, so new proposal is:

stmt ::= <kind> <typename> &= <type> ';' stmt
expr ::= <type> '.' '&'

With two new kinds of types: unique types and opaque types.

Equality for a unique type requires pointer equality, but we'll also have to
keep track of the source of a polymorphic application for checking equality.

An opaque type is just like a unique type, except field access is not allowed.

Do I like the syntax choice, or better to use '@' for this?

<@,@>@ Map@ @= <@ K@, @ V@> { ... };
<@,@>@ PublicMap@ = Map@.@;

I like the syntax of that. It seems meaningful to me semantically at first
glance. It supports hierarchical access to internal and external type. It
supports uniqueness of type. There's no need for explicit conversions.

Is there ever a desire to cast a Map@ to the raw underlying type form? For
example, if we wanted a newtype like thing instead of a type wrapper? I don't
think so. We could always introduce a new struct type wrapper and use field
access for that.

Questions:
* What does it mean if the type is a function type? How would construct values
  of the type?

So perhaps a refinement: think of it as a 'newtype' kind of a type. Here's
what you can do with a newtype:

Define a new type, this is paired with a statement, to avoid any confusion
over when you make a new type:

<kind> <typename> '@=' <type> ';' stmt

The type supports the following operations:
  Construction:  <type> '(' <expr> ')'
    Construct a value of the newtype given an expression of the underlying
    type.

  Access: <expr> '.' '@'
    Access the value of the new type. Returns the value of the underlying
    type.

  Abstraction: <type> '.' '@'
    Returns an opaque type considered equal to the given type.

And something to fix up whatever issues there are with polymorphism based on
the kind of the newtype.

That way you could create newtypes for any kind of type, not just data. It's
essentially a one field struct type, though maybe we can implement it more
efficiently at runtime.

I would probably have to start trying this out to see if it works in practice
or what surprising things come up.

If you do typeof on a value of a newtype, you get the abstract type in return.

Of course, for this to be useful for cross module hierarchy stuff, we will
need to have some support for module access control.

---

Thinking about it more, I understand better the original proposal.

We can't have two different kinds of unique type. A value of the newtype has
to have only one type, and some users are able to access that one type, and
others are not. Because we pass values into and out of the API for interacting
with values of that type.

Fundamentally what we need is:
* A newtype, so users can't create and use its implementation type out of thin
  air.
* A value that acts as a token to provide access to convert to and from the
  newtype.

The access token could be Put and Get functions. Or maybe it is some separate
kind of value. My main concern is the syntax overhead for constructing and
accessing the newtype with the token.

Let's explore some syntax:

<@,@>@ Map@, Put, Get @= <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };

Map@ Empty = <@ K@, @ V@> {
  Put<K@, V@>(empty: Unit);
};

No...

<@,@>@ Map_@ = <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };
<@,@>@ Map@, Put, Get @= Map_@;

Map@ Empty = <@ K@, @ V@> {
  Map_@<K@, V@> empty = Map_@<K@, V@>(empty: Unit);
  Put<K@, V@>(empty);
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  Map_@<K@, V@> map_ = Get<K@, V@>(map);
  map_.?(empty: True, map: False);
};

Actually, that's not so bad. This approach says we always cast first thing
from Map@ to Map_@ at the start of an API function that takes a Map@, and we
cast at the end from Map_@ to Map@ at the end of an API function that returns
a Map@.

What if we used a token instead of Put and Get? Would that help with any
confusion over polymorphism?

<@,@>@ Map_@ = <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };
<@,@>@ Map@, Map_ @= Map_@;

Map@ Empty = <@ K@, @ V@> {
  Map_@<K@, V@> empty = Map_@<K@, V@>(empty: Unit);
  Map@<K@, V@>(Map_, empty);
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  Map_@<K@, V@> map_ = map(Map_);
  map_.?(empty: True, map: False);
};

That way we don't have to come up with names for Put and Get, get becomes
function application on the new type value and Put becomes function
application on the new type type. But then, what is the type of a token?

If the token is a static thing, could we make it a type?

<@,@>@ Map_@ = <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };
<@,@>@ Map@, _@ @= Map_@;

Map@ Empty = <@ K@, @ V@> {
  Map_@<K@, V@> empty = Map_@<K@, V@>(empty: Unit);
  Map@<_@><K@, V@>(empty);
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  map<_@>.?(empty: True, map: False);
};

I think Put and Get functions would be cleaner semantically and avoid
introducing a new kind of concept into the language. Maybe we have a
convention to name them Public and Private?

<@,@>@ Map_@ = <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };
<@,@>@ Map@, Private, Public @= Map_@;

Map@ Empty = <@ K@, @ V@> {
  Public<K@, V@>(Map_@<K@, V@>(empty: Unit));
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  Private<K@, V@>(map).?(empty: True, map: False);
};

I think I prefer the syntax 
   <kind> <typename>, <name>, <name> '@=' <type> ';' stmt

Which is more consistent with how we define types.

Should we use that kind of syntax for links too? How would that look?

Get@<Int@> get, Put@<Int@> put ~= Int@;

No, that's hard to follow. I suppose we could reuse function bind syntax,
assuming we had a builtin link function 

  <@ T@, @ X@>((Get@<T@> get, Put@<T@> put) { X@; } f) { X@; }

Get@<Int@> get, Put@<Int@> put <- ~<Int@, X@>;

But having to specify X@ is yucky. Same as with function bind.

Maybe:

  get, put ~= Int@;

I just wish there was some kind of type at the beginning. But then it would be
duplicated with what was at the end?

Map_@ & Map@, Get, Put;
<@,@>@ Map@, Get, Put @= Map_@;

Let's stick with @= for newtype and the existing syntax for link.

---

It's a little sad we have function call overhead associated with what is
otherwise a no-op cast between types. Any way we could get rid of that? Like,
have a special syntax for the put and the get?

That would mean passing around a token again, and may as well make it a type
token so it can be eliminated at runtime.

<@,@>@ Map@, _@ &= Map_@;

  map.<_@> says given an instance of Map@, return it's Map_@.
  Map@.<_@>(...) says construct an instance of Map@ from Map_@?

It's a special syntax for sure, which will let us make it free to run at
runtime. The compiler gives an access error if the type _@ does not match the
type created for Map@ when Map@ was created. Now you can pass around _@
however you want, to whoever you want. The important part is, there is no way
to create _@ out of thin air. _@ is a special primitive kind of type. There is
no way to instantiate a value of that type. It has kind @ though, if you want
to pass it as a template argument. I'm not sure if that's useful. I guess it
would give you a way to pass types and their access tokens to polymorphic
functions that can operate on arbitrary newtypes. Not sure that's significant
for anything.

<@,@>@ Map_@ = <@ K@, @ V@> { +(Unit@ empty, MapP@<K@, V@> map); };
<@,@>@ Map@, _@ @= Map_@;

Map@ Empty = <@ K@, @ V@> {
  Map@<K@, V@>.<_@>(Map_@<K@, V@>(empty: Unit));
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  map.<_@>.?(empty: True, map: False);
};

Maybe the token is the real thing that matters? We can create a token out of
thin air, and that token is a type. Then we can use that to construct newtypes
of kind @. I'm just thinking about how we could simplify the issue of
polymorphism here. And we could reuse the same token for lots of different
types.

For example:

@@ Tok@;  # Creates a new token type 'Tok@'. User choice of name.

<@,@>@ Map@ = <@ K@, @ V@> { Tok@<Map_@<K@, V@>>; };

Map@ Empty = <@ K@, @ V@> {
  Map_@<K@, V@> empty = Map_@<K@, V@>(empty: Unit);
  Map@<K@, V@>.<Tok@>(empty);
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  map.<Tok@>.?(empty: True, map: False);
};

So, newtype only operates on types of kind @. We add the following operations:

@@ <typename> ';' <stmt>
  Creates a new token type and stores it under the name typename.

  The type has kind <@>@. When given a type T@, it returns a newtype for the
  type T@ accessible only with the token type.

<type> '.' <type> '(' <expr> ')'
  Requires the left hand side be a newtype N@ for some type T@. The right hand
  side must be the token type the newtype was originally created with,
  otherwise you get a compiler error. The expression should have type T@.
  Returns a value of type N@ that contains the value of the expression.

<expr> '.' <type>
  Requires the expression be a newtype N@ for some type T@. The type must be
  the token type the newtype was originally created with, otherwise you get a
  compiler error. Returns a value of type T@ that was associated with the
  newtype value N@.

The advantages of this approach, syntax questions aside, is we don't have any
confusion over polymorphism: it works on kind @ arguments, you can manually
reuse the token within polymorphism and across different types as desired. And
we use syntax for the conversions instead of functions, so they don't need to
have any runtime overhead.

Exploring different syntax:

@@ Tok@;  # Creates a new token type 'Tok@'. User choice of name.

<@,@>@ Map@ = <@ K@, @ V@> { Map_@<K@, V@>:<Tok@>; };

Map@ Empty = <@ K@, @ V@> {
  Map_@<K@, V@> empty = Map_@<K@, V@>(empty: Unit);
  Map@<K@, V@>.<Tok@>(empty);
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  map.<Tok@>.?(empty: True, map: False);
};

What if there was no such thing as a special token type. We can use whatever
type we want? Well, we do need such a thing as a unique type. It can be a
unique phantom type. And there's not really much point to allow any kind of
type for token, because if it is not a unique type, anyone could get at the
token type.

So, focus on the token type as the actor? Or, we have the token type and the
newtype as actors.

Abstract syntax:
  Expr ::= abstract (name :: TypeName) (body :: Expr)
    Creates an abstract token type with the given name, available in the body.

    What's the kind of token type? <@>@ in theory. But <@>@ Foo@; ... is not
    good. I like @@ Foo@; ... for concrete syntax.

  Type ::= abstract_type (token :: Type) (type :: Type)
    Given a abstract token type and an arbitrary type, return an abstract
    version of that arbitrary type keyed on the token type.

    Looks like polymorphism again. Why not say the token type is a polymorphic
    type and use concrete syntax such as Tok@<Map_@>;

  Expr ::= abstract_value (token :: Type) (value :: Expr)
    Given an abstract token type and a value of arbitrary type, return an
    abstract value keyed on the token type. The type of the returned value is
    the abstract type of the type of the given value.

    Concrete syntax: <type>(value). To match struct and union value syntax.

  Expr ::= abstract_access (value :: Expr) (token :: Type)
    Given an abstract value and token type, return the value associated with
    the abstract_value if the token type matches the token type used to
    construct the abstract value.

    Why not make abstract_value a polymorphic type? Then this becomes a simple
    polymorphic type application with standard syntax? Thus, concrete syntax
    would be, for example: map<Tok@>.?(...).

There's a couple ways we could think of this. Think of abstract type as a
specific class of type of kind @, like struct and union. Or think of abstract
type as a builtin polymorphic type. I suspect the former will be easier to
understand. My concern with the polymorphic type part is we really want Map@
to have kind <@,@>@, not <@,@,@>@. And I'm not sure <@,@><@,@> is treated the
same as <@,@>@ in my language.

Okay then. There you go. Use the same syntax as polymorphic type application,
but treat abstract types as something different. Confusing? Perhaps. The
example code now:

@@ _@;

<@,@>@ Map@ = <@ K@, @ V@> { _@<Map_@<K@, V@>>; };

Map@ Empty = <@ K@, @ V@> {
  Map_@<K@, V@> empty = Map_@<K@, V@>(empty: Unit);
  _@(Map_@<K@, V@>(empty: Unit));
};

<@ K@, @ V@>(Map@<K@, V@>) { Bool@; }
IsEmpty = <@ K@, @ V@>(Map@<K@, V@> map) {
  map<_@>.?(empty: True, map: False);
};

That looks pretty nice to me. Except that you have to know Map@ is a newtype
to understand why you can do <_@> application to it.

That's pretty cool actually. Only concern syntax wise is the overloading of
polymorphic application for abstract values and abstract types. The other
option would be to say a token type has kind <@><@,@>. You give it a type, it
returns something that given ... no it would be some weird recursive kind
thing. I'd rather not enter that world. Treat it as completely separate from
polymorphism. Have a new section on abstract types.

As with usual, we can revisit the feature or syntax as we start to use it
more. No need for it to be perfect to start. But I'm fairly happy with the
proposal now. Shall I give it a try?

---

While implementing my proposal for abstract data types, it feels like there's
a lot of overlap between the syntax and polymorphism. Could we implement
abstract types using polymorphism?

Here's the idea:
* We keep the ability to get a unique token type.
* We add a special kind which is, say, a type constraint or equality kind.

I would propose that's all we need to support all the same things as currently
proposed for abstract types.

Let's say syntax for the new kind is '=' '<' type '>'

If we wanted, we could change the syntax for token types to be more similar to
poly too:

  <kind> type ';' stmt

Now the simple example for abstract types becomes:

@ Tok@;
<=<Tok@>>@ AbsBool@ = <=<Tok@> _@> { Bool@; };
AbsBool@ t = <=<Tok@> _@> { Bool@(true: Unit); };
t<Tok@>.true;

Okay, so the syntax is pretty unhappy for the type kind, but you get the idea.
What would a nicer syntax be? Maybe we could use a type for a kind?

@ Tok@;
<Tok@>@ AbsBool@ = <Tok@ _@> { Bool@; };
AbsBool@ t = <Tok@ _@> { Bool@(true: Unit); };
t<Tok@>.true;

Yeah, that's nicer.

The semantics of the type constraint kind are the only thing it accepts is a
type that matches the type constraint kind. Anything else is a compiler error.

The user has no way to get at the type of the kind to create it out of thin
air, so we are all set.

Syntactically this is only slightly more tedious than what I originally
proposed.

There is a question over the kind of the abstract entity. The user needs to be
able to write down the kind or the type in order to declare a variable of this
type, and there's no way to do that unless they can access the token type.
That spoils things a bit.

I want to propose a separate improvement to the language though. That you can
use something of higher order kind anywhere something of lower order kind is
expected. For example, if a function expects something of kind @, you could
use something of kind @, <@>@, or <@,@>@. If something expect something of
kind <@>@, you could use something of kind <@>@ or <@,@>@. That limits you to
not being able to make full use of the something your using, but otherwise it
should work fine, right?

Then we could say:

@ AbsBool@ = /AbsBool%.AbsBool@;

Even though the kind of /AbsBool%.AbsBool@ is <Tok@>@.

Allowing a kind to contain a type is slightly tedious from an implementation
perspective.

---

I don't much like overloading the syntax for polymorphism and abstract types.
I don't much like the idea of allowing a type to be used as a kind.

Exploring a different concrete syntax:

I like the idea of being able to use any type as a token type, even though you
would only use it in practice with an abstract type, because then we can use
the nicer syntax for an abstract type:   kind name ';' stmt.

@ Tok@;
@ AbsBool@ = ?(Tok@, Bool@);
AbsBool@ t = ?(Tok@, Bool@(true: Unit));
t.?<Tok@>.true;

Here, using ? as the punctuation for abstract types. .?<> is analogous to .?()
used for union select. It seems much more readable than the polymorphic syntax
to me. ?() reused for both abstract type and abstract value. Does that make
sense?

The syntax for ?() looks a little weird to me. Any other variations that would
be better? In particular, we normally do something more like (type value, ...)
when using that form of syntax to define new types.

Maybe use ':' as the operator? No. That sounds like a potential for lots of
conflicts. I was thinking something like: Bool@:Tok@, True:Tok@.

  Tok@<Bool@>, Tok@(True), t.<Tok@>.true?

Using t.<Tok@> could be thought of as analogous to union values. You access a
tag, which may or may not be the right one, and you get the result or an
error. The difference being the tag is now a type.

Crazy idea, could we have fields be first class objects? So the syntax for
access is <expr> . <expr>, where the second expression has type field?

This is worth exploring. It feels like things would magically work fine if
field access was done by function and construction was done by function
instead of type. Then you have separate named entities for the constructor,
the fields, and the types, so you can share the type without also sharing the
fields. I guess we would still need a way to support unique types though.

How about for a token, instead of using a generic type, we have a value of
type token?

& tok;
@ AbsBool@ = ?<tok: Bool@>;
AbsBool@ t = ?(tok: True);
t.tok;

tok<Bool@>, tok(True), t.tok.

I suppose the benefit here is the token is just a name, so it is an atomic
kind of thing. Not a general expression. But that would mean you can't pass
tokens around, which is not what we want. We'll always want to be able to say
something like:  t.</Foo/Impl%.tok@>.

_@<Bool@>, _@(True), t<_@>

_@(Bool@), _@(True), t[_@]

Bool@[_@], True[_@], t[_@]

/Foo%._@(Bool@), /Foo%._@(True), t.(_@)

@? _@;
@ AbsBool@ = ?(_@: Bool@);
AbsBool@ t = ?(_@: True);
t.(_@);

@? _@;
@ AbsBool@ = <_@>?(Bool@);
AbsBool@ t = <_@>?(True);
t?<_@>;

@? _@;
@ AbsBool@ = _@<Bool@>;
AbsBool@ t = _@(True);
t<_@>;

Struct value, function apply, and abstract value all make sense to share the
same function application form. It's all the same kind of thing. We could just
as well have used normal functions to implement struct and abstract value.

I don't think the same is true of polymorphic application? How is abstract
type different?

_@<Bool@>. We could think of _@ as a polymorphic type that, given a type,
returns an abstract type. That's fine. If we used some different syntax, we
could still write:

<@>@ _@ = <@ T@> { ?(Tok@, T@); };

Can we think of abstract access that same way? A function that gives us a
polymorphic value makes sense, right?

(T@)<@ tok@>{ T@; } _@ = (T@ t) {
  <@ tok@> { t; }
};

You could almost write that today. The only thing missing is the compiler
magic to assert that the value supplied for tok@ matches Tok@. I have no way
to do that with polymorphism, because we don't have access to the applied type
when checking the correctness of the body of a poly. In this case we need to
compare the externally applied type with the internally applied type.

The point is, the use of polymorphic syntax is reasonable based on the idea
that you could almost model an abstract value as a polymorphic value.

Poly is like a forall. <@ T@> { ... } means, for all T@, we have { ... }. And
we check that it will work for all, and then you can pass any type and it will
work.

What we are looking for with abstract types is more like a constraint. For all
T@, such that T@ = Tok@, we have { ... }. Which gets back to the idea of type
kinds.

Could we introduce a special kind of poly binding?

Today we have: kind name. Add as an option: type. Then we have two kinds of
constraints. We would need to support these constraints in kinds though. So
really it would just be a matter of syntax.

@? _@;
<_@>@ AbsBool@ = <_@> { Bool@; };
AbsBool@ t = <_@> { True; };
t<_@>;

@? _@;
@ AbsBool@ = _@<Bool@>;
AbsBool@ t = _@(True);
t<_@>;

I don't know. It's tough. Let's continue down the current path of something
not quite a poly, but that behaves similarly to a poly. Just like a struct
type is not quite a function, but in some cases behaves like a function. You
can always wrap it in a function if you want.

<@>@ Abs@ = <@ T@> { _@<T@>; };
<@ T@>(T@) { Abs@<T@> } = <@ T@>(T@ x) { _@(x); };

I guess you can't wrap the abstract value in a poly though. That's the funny
bit.

---

Experience report trying to use abstract types for Map@:
* You really want to define internal and public types.
  Map_@ and Map@ in this case.
* Seems to be more convenient to declare a local variable of internal type and
  access that rather than use abstract access everywhere.
  e.g. Map_@<K@, V@> map_ = map<_@>; ... map_...
* Actually, for a case like: right.map.left.map.value, it's nicer to do
  right.map.left<_@>.map.value. I'm not sure if that means better to do
  right<_@>.map.left<_@>.map.value too.
* I'm tempted to wrap foo.map in a function to get the field. That would be
  unfortunate, no?
* It's kind of nice that you can easily see who accesses the internals of a
  data type based on who references the _@ token type.

Doing things a little differently:
* We can store Map_@ instead of Map@ for internal fields. That way you can do
  right_.map.left.map.value. That's probably better. Only add the layer of
  abstraction at the interface to the outside world.

  The tradeoff is we tend to need _@(...) around the internal value more often
  now when converting to public value.

* Say I have a function type that takes a private type. Can I cast that to a
  function type that takes the public type to avoid runtime overhead?

---

I wanted to try using abstract data types in my pinball game to define a type
Num@ that's equal to either a rational number, int, or any other number with
sufficient precision, but I'm hesitant for fear wrapping all the functions
will result in significant performance overheads. Sigh. Perhaps that is an
unjustified concern?

I went ahead with it anyway, and it's pretty nice actually. I feel much better
about my ability to change out the implementation of Num@ without breaking
anything. No noticeable performance issues yet.

Performance overhead is not too bad. For example, here we have 1301/89139, or
about 1.5% overhead for the abstraction.

**   164908    89139     1301  /Pinball/Num%.Mul![0d57] **
     164908    57634           /Int/Int%.Mul![012f]
     164908    30204           /Int/Int/Div%.Div![0180]

---

Hmm... I seem to have lost a bunch of thoughts I had about abstract data types
here. The summary was: I still worry about performance and syntactic overheads
of wrapper functions. There was an idea that _@ could be a cast, where you
provide a polymorphic type to describe where to apply _@ within a type, then
you could directly cast a function instead of have to wrap it.

But the syntax was still tedious, or an alternative syntax that takes input
and output types directly, taking advantage of the typeof expression, is
sketchy.

Here's another idea: what if _@ gave us two things: a way to define newtypes,
and a way to write code in an environment where newtypes are considered equal
to their value types.

@? _@;

@ Num@ = _@<Int@>;    # Defines a new type.

Num@ x = Int|4;       # Not legal, Int@ does not equal Num@.

(Num@) { Num@; } Id = (Num@ x) { x; };

Num@ x = _@ { Id(Int|4); }; # Legal within scope of _@.

Now imagine:

@? _@;
@ Num@ = _@<Int@>;

_@ {
  (Num@, Num@) { Num@; } Add = AddI;
  (Num@, Num@) { Num@; } Sub = SubI;
  (Num@) { Num@; } Neg = NegI;
  (Num@) { Num@; } Foo = (Int@ x) {
     FooI(AddI(x, Int|2));
  };
};

If we wanted to, we could create a syntax for the scope of _@. That would get
us back to the original idea if: if you have _@ in scope, then the types are
considered equal. The key differences from before being:

* We have an explicit scope statement for _@, so it's explicit when _@ is
  considered in scope or not.
* We still very clearly label the type of an object as Num@ or Int@. Two
  different types. It's just that we consider them equal within the scope.

For example:
  _@ {
    Num@ n = Int|3;
    Int@ i = Int|3;
    @(n, i);
  };

Will result in the type @(Num@ n, Int@ i).

I do worry this is a little bit confusing. I would rather explicitly cast
types. Can we use this idea to make explicit casting better?

I guess what I really want, syntactically, is the cast where you give input
and output type explicitly. If we have that, any need for a scope?
   
@? _@;
@ Num@ = _@<Int@>;

(Num@, Num@) { Num@; } Add = _@<@<Add>, @<AddI>>(AddI);
(Num@, Num@) { Num@; } Sub = _@<@<Sub>, @<SubI>>(SubI);
(Num@) { Num@; } Neg = _@<@<Neg>, @<NegI>>(NegI);
(Num@) { Num@; } Foo = (Num@ x) {
   _@<Num@, Int@>(FooI(AddI(_@<Int@, Num@>(x), Int|2)));
};

This is as conceptually sound as the scope approach if we make the cast a
special syntactic construct. The meaning of the cast is: compare input and
output types under the scope of equality for the newtype, if the types are
equal, then it's a legal cast, otherwise it's an illegal cast.

If we make it a syntactic construct, we can do some type inference to make it
more convenient. There are two:

* newtype <token type> <value type>
  Creates a new type wrapper using the token type around the value type.

* cast <token type> <target type> <expr>
  Cast an expression using the token type to the target type. It's legal to do
  as long as the type of the expression is equal to the target type when
  treating newtypes for the token type as equal to their values.

This reduces our current abstract, abstract_type, abstract_value, and
abstract_access down to three: abstract, abstract_type, abstract_cast. We use
the same cast to go into and out of the abstract type.

Ideas for syntax: well, like before, except trying to figure out some way to
pair type and value together in the same syntax. I think using <> is fine, but
how do we distinguish between the newtype and the cast operations?

If we want cast to work on arbitrary types, the syntax has to be focused on
the token type, not the argument type. The token type is like any other type,
so we also have to distinguish the syntax from arbitrary other syntax
involving types.

One of the operations can be single argument polymorphic application to the
token type. That fits. Probably that should be the newtype.

How do we do the cast? How do we clearly make it it's own syntax? Either it
has to be a wildly new syntax based around the token type, or it is a special
cast syntax that takes the token as an input. I think the later will be
better. If we do that, maybe we can define a newtype syntax in a similar
manner, and avoid any confusion with polymorphic application?

@? Tok@;
@ Num@ = @?<Tok@>(Int@);
Num@ x = @?<Tok@, Num@>(Int|3);

Okay, here is an option. Special syntax anchored around @?:

@? <typename> ; stmt
  Define a token type.

@? <token type> ( <type> )
  Define a newtype with a token type.

@? <token type, target type> (<expr>)
  Cast an expression to a target type given the token type.

The newtype form is sort of similar to struct and union types, using 
@?<Tok@> in the place of '+' and '*'.

The full example:

@? _@;
@ Num@ = @?<_@>(Int@);

(Num@, Num@) { Num@; } Add = @?<_@, @<Add>>(AddI);
(Num@, Num@) { Num@; } Sub = @?<_@, @<Sub>>(SubI);
(Num@) { Num@; } Neg = @?<_@, @<Neg>>(NegI);
(Num@) { Num@; } Foo = (Num@ x) {
   @?<_@, Num@>(FooI(AddI(@?<_@, Int@>(x), Int|2)));
};

Maybe we could add syntax highlighting for @?<...> so it stands out as a
separate syntactic entity?

Or, random other idea, maybe you can using @? to create things that take _@ as
an argument.


@? _@;
@ Num@ = @?<Int@><_@>;
(Num@, Num@) { Num@; } Add = @?<@<Add>>(AddI)<_@>;

No, I rather have the token type available when we typecheck the cast instead
of making it a deferred thing.

@? _@;
@ Num@ = _@<Int@>;
Num@ 3 = _@.<Num@>(Int|3);
(Num@, Num@) { Num@; } Add = _@.<@<Add>>(AddI);

This is interesting. Avoid the extra @? by using _@.<target>(expr) for cast.
That seems cleaner to me.

@? _@;
@ Num@ = _@<Int@>;
Num@ 3 = _@.<Num@>(Int|3);
(Num@, Num@) { Num@; } Add = _@.<@<Add>>(AddI);
(Num@, Num@) { Num@; } Sub = _@.<@<Sub>>(SubI);
(Num@) { Num@; } Neg = _@.<@<Neg>>(NegI);
(Num@) { Num@; } Foo = (Num@ x) {
   _@.<Num@>(FooI(AddI(_@.<Int@>(x), Int|2)));
};

Yeah, that seems good. Fairly low overhead syntax. No runtime overhead for the
cast. No need for wrapper functions. Basically no runtime overhead for
abstract data types, which is what I really want.

Question: use <> or () for newtype? Does it mean anything to have a newtype of
a value? Like: _@(Int|3)? No. It doesn't. Soo I think <>.

Question: anything other than '.' we could use that would make more sense for
casting? No. I think '.' is like syntax for conditional except using <type> in
place of '?'. _@.?(Num@: Int|3)? To much overhead in the syntax.

Good. I like this idea. Maybe let's give it a try and see how it goes. The
fact that it eliminates the need for wrapper functions and entirely addresses
my two major hesitations for using abstract data types suggests it is worth
pursuing.

Implementation wise, it's just a question of whether we can do the equality
check for types easily enough. Maybe we add a magic flag to an abstract type
saying if it is currently transparent or opaque during the check for type
equality?

It will be interesting to see what effect this has on Map@ use of abstract
type. Maybe I can separate internal from external definitions entirely and
just use a cast at the top level instead of lots of internal casts? Because we
are introducing some overhead to the cast now that you have to supply the
target type, and if we want to hide that behind functions, we are back to
introducing runtime overhead.

---

How to update the language specification to let you provide something of kind
<@>@ where something of kind @ is expected?

Places we currently do kind checks:
* Checking if kinds of arguments to poly types are equal.
* Checking if kinds of arguments to poly kinds are equal.
* In let expression where type is not supplied to confirm the kind of the
  value matches the kind of the variable.
* In polymorphic application to check the kind supplied matches the kind
  expected.

The idea is if I have a poly that expects something of kind @, you should
be able to supply something of kind <@>@, because it's abstract. The interface
is @, and you can do everything with a <@>@ that you can do with an @. The
other way around is not true: you can't do polymorphic application to
something of kind @.

The first idea would be to say: you can pass something of kind <@>@ in
polymorphic application where something of kind @ is expected.

This has implications for equality checks. For example, let's say I have a
poly of kind: < <@>@ >@. In theory I could pass it something of kind @, so it
could be used as if it was kind <@>@. That's a bit more complicated, but makes
sense. If the only reason we check kind arguments to poly kinds are equal,
then we rename KindsEqual to KindsAllowed, or some such, preserve the
asymmetry, and implement it properly.

How about the case of types? If I have something of type: <@ T@> { T@; },
that's clearly different than something of type True@. I don't see how we
could consider them the same.

How about: <@ T@> { T@; } versus <<@>@ T@> { T@; }? Again, we don't have an
equality relationship, because it is asymmetric, but you could use one where
the other is expected? I see no need to support what would be a really rarely
used feature like that. So let's say type equality requires kind equality
still.

Now the question of a let expression. If you have something Foo@ of kind <@>@,
is it okay to write:

  @ Bar@ = Foo@;

In theory this would mean you could do Foo@<X@>, but not Bar@<X@>. But then,
you may as well instead write this as <@>@ Bar@. What do you loose? Better
information. You can do just as much.

One argument would be if you want to treat it as an abstract entity and have
the compiler verify you don't do any application to it. That way you could
change the implementation to stop using polymorphism without effecting users.

I feel like it could be a common mistake to write things like:

@ Maybe@ = /Maybe%.Maybe@;

It would be nice to have help from the compiler to tell you you made a
mistake. On the other hand, I don't think anything can go wrong from this
mistake. Any attempts to do application to Maybe@ will fail, and that will
show you the mistake you made.

Is it possible to manually wrap a polymorphic value as a non-polymorphic
value? What does that even mean? For example, maybe I have a function that
operates on lists of elements of kind @ that I want to work on lists of
elements of kind <@>@, like the Length function. Can I do that in the language
currently?

I don't think so.

Basically that would mean you can't do anything with whatever you are treating
as a different kind. So it would have to have an abstract type.

Potential options:
1. keep things as they are now.
2. allow <@>@ to be passed in poly application where @ is expected.
3. add some way to convert something of kind <@>@ to abstract value of kind @
4. in all language features, allow <@>@ where @ is expected.

I don't think (3) or (4) are justified. Can I get away with just (2)? I think
that makes sense semantically, as long as I recursively define what it means
to allow some kind X to be used where some kind Y is expected.

Interestingly, we could use abstract types to convert something to kind @,
which would provide a work around for the case of applying higher kinded
arguments to poly values. It works better if we make the improvements to
abstract types suggested above that allow us to cast more complex types.

---

Trying out new approach for abstract types. The Map implementation makes me
feel like this is nicer, and leans more towards having separate blocks for
implementation (all internal access) and interface (summarized at the end with
explicit types).

I ended up putting the internal implementation in a completely separate file,
where it looks exactly like how it would if I didn't make it abstract. And
then a separate module for defining he abstract interface that only shows the
types of the abstract interface.

There's still the issue of how to do private modules. To have the full story,
I think I'll want to define the token type in a separate private module. That
will come some time down the line when I've worked out modular compilation
better.

I feel like it works out pretty well for my Num@ abstract type, both for
functions that work directly with the underlying value, and with functions
that need slight modification.

---

After much thought on supporting abstract types across module boundaries, I
want to do another revision. The key idea is that the token type is associated
with a module path representing the set of modules that can access the token,
and we add back direct wrap and unwrap syntax, which is needed to avoid what
would otherwise be too tedious for marking abstract modules.

This is a chance to revisit the concrete syntax. What do you think?

@ _@ = ?(/Foo%);                # token type
@ X@ = _@<X_@>;                 # abstract type
X@ x = _@(x_);                  # abstract value
X_@ y = x.?;                    # abstract access
List@<X@> xs = _@.<@<xs>>(xs_); # abstract cast

Is '?' the right character to use? How about '%'?

@ _@ = %(/Foo%);                # token type
@ X@ = _@<X_@>;                 # abstract type
X@ x = _@(x_);                  # abstract value
X_@ y = x.%;                    # abstract access
List@<X@> xs = _@.<@<xs>>(xs_); # abstract cast

Or '$', which I guess I'm not currently using?

@ _@ = $(/Foo%);                # token type
@ X@ = _@<X_@>;                 # abstract type
X@ x = _@(x_);                  # abstract value
X_@ y = x.$;                    # abstract access
List@<X@> xs = _@.<@<xs>>(xs_); # abstract cast

I'm leaning towards '%', if that doesn't conflict with other syntax.

---

Implementation should be straight forward, because have access to module path
when doing type check. Let's just stash the module path in the scope and use
that for access checks.

Step 1 is to check we don't have any parser conflicts with the proposed
syntax.

All we need to add is expr '.' '%' and '%'(mpath).

The parse seems happy enough with this. Let's do it.

