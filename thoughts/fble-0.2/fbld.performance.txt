Fbld Performance
================

Now that we've completely redesigned and implemented fbld, let's see if we
can't make it run a little faster.

Specifically, there are a lot of library man pages to generate. Each one takes
a couple seconds. That adds up to a long wait time. I would love to cut that
time in half if possible.

$ time ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/config.fbld \
./fbld/version.fbld ../fbld/man.fbld ../fbld/dc.man.fbld \
./include/fble/FbleEval.3.fbld
real    0m1.750s

Fble based profiling says:
 44% in /Fbld/Parse%.Parse
 36% in /Fbld/Eval%.Eval
 15% in /Core/Stream/IStream%.GetChar

perf based profiling says:
 72% in /Fbld/Parse%.Parse

It's hard to get more clarity than that from the perf profile. I kind of wish
everything didn't go through FbleThreadCall. That really muddies the picture.

Let's focus on the parser. It's tough to tell from the profile, because
everything goes through Do. Once again, it would be nice if there was some way
to eliminate that to avoid muddying the picture.

Ideas from reading the code:
* Do we need a State monad? Can we use just a Reader monad?
  - It's not like we are writing output.
  - I don't think we can use a Reader monad. We need to track unconsumed
    output.
* Compute GetL_ up front as a list of CharL@?
  - To avoid having to redo GetL_ over and over every time we Try_ or Test_
    and fail?

Remember that perf profiling shows most of the time due to allocations. Can we
see who is doing those allocations? Should we increase the cost of
instructions that allocate for prof purposes?

According to perf, there are 909 allocations. They are:

         63       70           FbleNewStructValue_.constprop.7[0025]
         94      122           FbleNewLiteralValue[0005]
        150      159           FbleNewUnionValue[0010]
        266      292           FbleNewFuncValue[001e]
        328      349           FbleNewStructValue[0027]

This is odd. Let's focus on FbleNewFuncValue, which normally I don't expect to
see much of.


         17       17           _2f_Fbld_2f_Result_25__2e_Do_21_.000d[0057]
         23       24           _2f_Fbld_2f_Parse_2f_M_25__2e_GetL_21_.0058[003f]
         25       31           _2f_Core_2f_List_2f_Ord_25__2e_Ord_21_.0009[001d]
         32       33           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21_.002b[0047]
         43       46           _2f_Fbld_2f_Parse_2f_M_25__2e_Return_21_.0023[002
         54       57           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21__21_.002d[003
**      293      319       26  FbleNewFuncValue[001e] **

Do and Return are kind of expected, because they are structured as functions
that return functions to work with bind syntax. What about List Ord?

List Ord takes the ordering function separately. We should be able to factor
that out. That's the recursive call to Ord itself.

---

It's like the whole monadic construct involves allocations everywhere.
Particularly the parser one. We are allocating every time we 'try' and fail.
For example, allocating an error message that we just ignore in the end.

Is there any way to structure the parser monad so that it doesn't allocate all
over like this?

The reason results are turning into object allocations instead of being small
enough to be packed is because of the error message that we often throw away.
In theory we could use different variants on the function if we know we are
going to throw away the error message.

In the case of success, we need to allocate a heap object because we have to
have space to store the 'tail' pointer with the rest of the text to parse.

Could I change the parser monad to be a parser generator monad? Have it
somehow work out all the smarts to generate an efficient function that doesn't
do unnecessary allocation, while preserving the same kind of description?

I think it might be possible. We'll presumably want some restriction that
decisions made during parsing don't depend on the parsed result. They only
depend on whether parsing succeeded or not.

---

Let's start simpler. Say we have two kinds of parser types:
 * M@ is as today - parse and returns a value or error.
 * Parses@ - parse, but only indicates pass/fail, does not return a value or
   an error message.

Assuming Parses@ is much faster implementation wise, we do as much as we can
in Parses@. How much can we do there in practice?

* Try: wants to ignore error message, but not value.
* Test: Can ignore error message, but not value.
* Or: Could probably change to take M@<Unit@> as argument, then ignore both?
* Try_: Can ignore error message and value.
* Test_: Can ignore error message and value.
* EndOfInput - Can probably ignore error message and value.
* String - Can probably ignore error message and value.

I think I can do this incrementally. Slowly move things over to a Parses@ type
and see how far it gets me.

---

I don't know. It doesn't feel like this is going to be significant. It feels
like micro-optimizing. Maybe better to start by inlining common operations?

Let's focus on Or to start, which is only used with String matches right now.

And Do(Test_(...))?

It's like I want a minilanguage:

String :: String@ -> P@
Or :: P@ -> P@ -> P@
DoTest_ :: P@ -> (Bool@ -> M@<A@>) -> M@<A@>

How much time do we spend in Or? In Test_?

From perf profile, best I can tell is it's the regular use of Do that is
causing allocations which is hurting performance. Can we isolate how much of
those calls to Do could be a simplified Do_ that doesn't return value/error
message?

---

Okay, here's what I'm thinking. We want a variant of M@, call it M_@, that
ignores the error message but keeps the return value. For M_@<Unit@>, it
should be able to pack without having to allocate an object.

Let me work with this and see how far I get.

---

What we get to is:
* Return, Do, etc. glue functions want variants for M@ and M_@.
* Things that cannot return an error may as well use M_@. We can wrap in
  M(...) if we ever need to convert.
* Some functions an return errors, but in some cases we may want to ignore
  those error messages. In those cases, provide both M@ and M_@ variants.

Performance numbers: This is about 30% improvement. The number of object
allocations is cut in half.

Hmm... This feels sad. Do I want to take the change?

One thing we could do is generalize Result@ to include a second kind of error:
an unexplained error. That way we don't need two different types. That makes
it simpler in some cases, but we get less help from the compiler in other
cases.

Could we make it part of the reader? Like, have an extra option which says if
we want to return an error message or not. The functions Try_, Test_, etc.
will all say: run without recording the error message. Do this in combination
with expanding Result@ type.

That way, no need to duplicate any logic. No need to deal with messy types.
Everything can work in both contexts efficiently.

I like that idea. Let me save and back out my current changes and try that
alternative approach.

---

Alternative approach works great. Very minimal changes required.

Also note: avoiding return of errors was only maybe half the 30% improvement.
The other half was using GetL for String instead of Get and Loc.

Here's where we are at now:

time ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/config.fbld
./fbld/version.fbld ../fbld/man.fbld ../fbld/dc.man.fbld
./include/fble/FbleEval.3.fbld
real    0m1.129s


I still see a fair bit of time going into allocations.

         15       15           FbleNewLiteralValue[007a]
        107      116           FbleNewUnionValue[0019]
        167      183           FbleNewStructValue[001d]
        220      246           FbleNewFuncValue[0013]
**      516      567        7  FbleNewHeapObject[0014] **

Upping the frequency of collection to max:

         85      114           _2f_Fbld_2f_Parse_2f_M_25__2e_Return_21_.0026[003
        109      129           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21__21_.0032[003
        127      139           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21_.0030[0046]
**      740      937       51  FbleNewFuncValue[0013] **


Do these function allocations make sense? Any way to avoid them? Let's start
with Return.

Yeah. That makes sense. Not sure what I could do about that. M@ is a function.
Return allocates an M@ based on the provided argument. Each time you call
Return, you'll get a new function back.

The only thing I could think is if we do, like Return(Unit) a lot, we could
factor that out.

Get is defined as GetL + Do + Return. We could inline to avoid things there?

Could we improve the garbage collector? Like, manage memory ourselves to avoid
repeated malloc/free?

---

I tried manually removing FbleThreadCall from the perf profile for fbld. The
results are pretty interesting:
 
* 48% of time is in FbleNewHeapObject. We're spending half the time of the
  program allocating new objects.
* 47% of time is in Eval
  - Map Lookup is a decent chunk of this.
  - The rest mostly looks like fbld commands?
* 39% of time is in Parse

It's still pretty hard to figure out where the bulk of the time is going
otherwise. The way we handle recursion makes things hard to tell. I almost
wish, for cycles in the call graph, we bunch together everything in a cycle
into one blob.

That's two different ideas for how to make profiles more useful:
1. Lump recursive cycles into a single block. So it's easier to see how much
is self time through the cycle versus what the leaf times are.
2. Lump function call-throughs like FbleThreadCall or Do with their callers.
To preserve the relationship between caller and eventual callthrough callee.

I don't have enough information to do this from the profiling output info. But
I should have enough info to do it from the input to the profiler.

