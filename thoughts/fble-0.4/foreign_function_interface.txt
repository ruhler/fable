Foreign Function Interface
==========================

Simple: we provide a way to call a foreign (aka C) function from fble. The
compiler generates the logic to call it for you.

The abstract syntax is:
  foreign <type> <name>

This is an expression of given type implemented in C (or whatever foreign
language your compiler is targeting) named <name>.

Any reasonable way to do type checking to make sure what your calling
implements the type it claims it does?

And on the C side, provide a standard header for writing functions that
compute Fble values. Like I already have now, just slightly more abstracted
and formalized to work with different implementations.

---

Foreign function interface on the C side, I think should be, for now:

  FbleValue* <function>(FbleValueHeap* heap, FbleProfile* profile);

But we want this to be called once to set up the profile blocks. It would be
bad if it was called repeatedly and kept adding more and more to the profile.

Oh well. Let's not worry about that for now. To start I'll just make sure I
don't call an ffi expression in the body of a function. They'll all be module
level variable declarations anyway.

I almost want the whole foreign function expression to be a separate module,
to guarantee it's only called once. Or maybe each FFI expression is treated
like a module reference, and loaded as part of module initialization. Yeah,
that sounds good. Doesn't matter where you have it, we arrange for it to be
called once at link time.

This needs to be made clear to the user, in case it's something expect to run
a long time. But it is a static thing, in the sense that you can't pass
arguments at FFI expression time.

The implementation is simple for C and aarch64 backends I think. Add an FFI
instruction? Hmm... If we are doing things at link time, then no instruction
needed, but we would need some metadata to tell the compiler about it.

Maybe the easy way to start is to do things at FFI expression eval time
instead of module link time. Then all we need is a new FFI instruction with
the name of the thing and the destination for the value. C and aarch64
backends are easy: generate code to call the function...

Except we don't have access to the profile then. Just the profile thread? Is
that enough?

Anyway, the point I was trying to get to, is interpreter is harder, because we
have to look up the symbol somehow. One idea I had to get a hacky prototype
working is to pass a map from name to function to the interpreter. When it
sees the instruction, it looks in the map to get the function to call.

That doesn't work unless you have the FFI function built into the interpreter
though, which I don't want. I could get away with it for stdio, because the
interpreter is part of the same library. But that wouldn't let you add any FFI
in other packages to be used with fble-stdio. For example, it wouldn't work
for spec tests, which don't depend on studio, and would want to use contrived
FFI functions I don't want built into any implementation.

So the question is, how should FFI work in general? How do we bundle the FFI
implementation with the module? In particular, how to make it work in the
interpreter case?

The way I think of FFI is that we have two layers of abstraction going on
during execution: the fble layer, and the native layer. What is considered
'native' depends on the backend you are using. If it's C backend, then C code
is your native layer. If it's aarch64 backend, then aarch64 is your native
layer. If it's interpreter... then?

If it's C backend, is C code your native layer, or is it always machine code?
Because for interpreter, I could say it's the native machine code the
interpreter is implemented in.

Regardless, native code of some sort is going to be involved. Precompiled
native code, that is built separately from the interpreter. How can I pass
that to the interpreter? Using an elf file on linux.

Now things start to get tricky with different platforms. Let's not worry about
that right now. Let's focus on linux.

In the compiled case, we already have an elf file associated with the fble
module. That's your package level elf file. Perfect place to store your FFI
implementation. And you can reference it directly from there.

In the interpreted case, today all we have is .fble code. There is no
associated package level elf file. Then what can we do?
A. Provide a list of .so files with symbols to use for FFI to the interpreter.
B. Allow mixing of interpreted and compiled code, and require compiled code
for FFI.

(A) feels weird from a usage point of view? Or I suppose we could have
Foo.fble.so in the search path, and load that anytime we see it with
interpreted code for the sole purpose of looking up FFI. Yeah, that could
work. But also annoying to have to mix architecturally independent fble code
with architecturally dependent FFI implementation.

(B) seems more natural from a usage and distribution point of view. How would
it look in practice? It's related to modular compilation.

Ooh. (B) is the same mechanism we would want for being able to distribute
binary code without source for some packages. That's a uniform approach to
take. It's something we'll want eventually anyway. Which suggests (B) is the
way to go. Kill two birds with one stone.

How fble package distribution works today:
1. For the interpreter case: you have a package directly with all the .fble
files, each with full source.

2. For the compiled case, you have a .so file for each package that you link
with at compile time.

Today there is no mixed case. Either every package is compiled, or no package
is compiled. That's what I want to add. How would it look?

You are running an interpreter. It isn't pre-linked with all the compiled
packages you need. It needs a way to find and load that code. There are two
things it needs: the implementation of the modules (for execution), and the
types of the modules (for use).

I can imagine two different ways to provide the types:
1. Using .fble files with a special syntax to allow elision of the
implementation.
2. Compiled into whatever .so file we have with the rest of the code.

Option (1) is more geared towards user documentation. So that we can give nice
error messages and clear description of what the type should be. Option (2)
seems safer, in the sense that it's harder to mess with than it would be to
mess with the .fble files. For user sanity, some form of (1) will be needed.

Let's say we go with (1). We could always fall back to (2) later. We could do
both (1) for the user and (2) for the trust. But for now, just (1).

This means, in the package search path, for a package, we'll want to have:

* A tree of .fble files, one for each module.
* A .so file containing the code for the module.

You know what's interesting about this? It looks like it could support FFI
usage mode (A). The .fble files could contain the implementation and the .so
file could contain just the FFI code.

Anyway, what are the semantics? Can we use the same approach for interpreter
and compiler?

Think of the tree of .fble files as the header files. Think of the .so file as
the library file. This is straight forward, right?

When compiling a module against the package, use the .fble files to get the
type.

When linking your compiled code, use the .so file. This just tells you were to
look for that.

When loading the package from the interpreter, we just need a way to figure
out if we should read the .fble files for bytecode to interpret, or we already
have compiled code available. This can be on a per-module basis perhaps. That
sounds reasonable. Maybe you want to ship a mix of interpreted and compiled
code in a package, rather than have all one or another.

Which means, when loading a particular module, first, look for the
FbleGeneratedModule symbol associated with that module. If you can't find
that, then fall back to loading bytecode from the .fble file.

Can we go the other way? Can we link compiled code against interpreted code?
How to deal with dependencies between packages?

I think, maybe the directory structure we want is:

/usr/share/fble/<package>/Foo.fble
/usr/lib/fble/libfble-<package>.so

Anyway, what this means is, for the interpreter case, when we want to load a
module, we look for the .so file first, try to dynamically load it, before
falling back to the .fble file.

---

First step towards foreign function interface: switch from static libraries to
shared libraries in the build system. In preparation for being able to load
those libraries dynamically in the interpreter use case.

Start with linux. Worry about Windows separately.

Today we make .a files, e.g.:

  lib "$::b/lib/libfble.a" $objs

Based on:

proc lib { lib objs } {
  build $lib $objs "rm -f $lib ; ar rcs $lib $objs"
}

Okay, so what are the commands to build a shared library from a bunch of .o
files instead? Sounds like we need to add -fPIC when compiling C files. Then
use:

gcc -o foo.so -shared $objs.

Let's try one step at a time.

One concern is that we use the static libraries by passing them as if they
were object files. Is something else needed now? Seems like maybe not.

I'm getting errors like:

/usr/bin/ld: ./pkgs/satgame/SatGame/Generate.fble.o: relocation
R_AARCH64_ADR_PREL_PG_HI21 against symbol `stderr@@GLIBC_2.17' which may bind
externally can not be used when making a shared object; recompile with -fPIC

Add -pic to as call?

---

Let's go step by step for this .so change, to make sure I understand what's
really needed.

Step 1: Build libfble.so. Done. No issues.
Step 2: Use libfble.so in the bin/ binaries.

* If we directly replace the libfble.a argument with libfble.so:

Works, but only if I call it from the out/ directory. Otherwise we get an
error:

./out/bin/fble-compile: error while loading shared libraries:
./lib/libfble.so: cannot open shared object file: No such file or directory

Same thing after I install to the target location.

https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html

Reading up on shared libraries:
* -rpath option to ld sets a relative path to search for the .so file.
  - For example: -Wl,-rpath,$(DEFAULT_LIB_INSTALL_PATH)
* In theory I should pass -fPIC to gcc when compiling the .o files that go
  into a shared library.
* In theory I should run ldconfig after installing the library.
  - ldconfig -n directory_with_shared_libraries

I just need to figure out the approach for running locally and installed. Use
some existing package for example, that ships its own .so file. The question
is:

* Can I run the binary before it's installed? If so, how does it find the
  library it needs?
* Can I run the binary after it's installed to my custom directory? If so, how
  does it find the library it needs?

Then try to mimic that approach.

---

Looking at freetype as an example.

Unfortunately, I don't think freetype builds any binaries. Maybe sdl instead?

How about sudo? It looks like that builds .so and binary.

Notes:
* `objdump -x <foo> | grep PATH` is supposed to show the rpath of an
  executable. I see none in fble-compile. I wonder how it finds libfble.so
  then?
* readelf -d <foo> shows the path of the library needed. In my case, it shows
  ./lib/libfble.so. That explains what it's looking for anyway. 
* ldd shows not found if the library wouldn't be found at runtime.
* If I do '-l fble' when compiling fble-compile instead of 'lib/libfble.so' as
  an object file, then it shows up as 'libfble.so' in the executable.
* ./sudo doesn't work in the build directory. It says it can't find
  libsudo_util.so. It has 'libsudo_util.so.1' listed as the library from
  readelf.

---

This shared library stuff sounds tedious and tricky.

Anyway, my goals are:
* Need to be able to build from tools before installing.
* Would like to be able to run tools before installing without passing special
  arguments or environment variables.
* Need to be able to run tools after installing without passing special
  arguments or environment variables.

Static libraries easily satisfy all these requirements. How should we handle
it for dynamic libraries?

The things we can manipulate:
* The absolute/relative path to the library when building the executable.
  e.g. gcc -o foo foo.o ../../libbar.so
* The LD_LIBRARY_PATH
* The DT_RUNPATH attribute of the in the readelf -d section.
* ldconfig cache.
* The default search path.

It sounds like the path to the directory and DT_RUNPATH are two separate
things. Let me confirm experimentally.

Yes. For example:

 0x0000000000000001 (NEEDED)             Shared library: [libfoo.so]
 0x000000000000001d (RUNPATH)            Library runpath: [/home/richard/scratch/so]

NEEDED comes from how I pass the .so to gcc
 - Either as a path to libbar.so, or using -lbar
RUNPATH comes from -Wl,-rpath,<path> option to gcc.
 - And can be specified multiple times.

Every installed binary I see on my system sets NEEDED without an / in it, so
using the -lbar option to gcc. It depends on the library being found in the 

There is potentially an option to modify the executable when installing. Let
me see if I can figure out how to do that.

Yes, via: chrpath or patchelf. But I don't have those installed on my system.

Or, we could create two separate versions of the binary, the 'local' one, and
the 'to be installed' one. Though that feels like a pain.

How does it work if I install a package and executable to a local directory?
Do I have to set LD_LIBRARY_PATH? I feel like I normally wouldn't? Let's try
that.

Note that when calling dlopen, I can pass an absolute path to the .so file
to open, so I can use my own search logic for that.

sqlite is interesting. It distributes a library and binary, but the installed
binary is statically linked with the library rather than dynamically linked.
Can I take that approach? How would that look? What needs to be dynamic vs.
shared in fble?

Libraries to consider:
* libfble, libfbletest, for lib for each package.

How about we statically link with libfble and libfbletest. We can make .so
files out of the package libraries and use explicit dlopen for all of them?
They can assume the binary has been statically linked with libfble, so we
avoid duplicates of that in process. It just means duplicate across binaries,
but I'm okay with that?

* libfble.a is 1.8M
* libfble.so is 1M
* fble-deps, fble-disassemble, etc are around 25K each when dynamically
  linked. They are around 200K to 600K when statically linked.

It would be really nice if we could dynamically link, to save on space.

If I'm willing to be hacky, how about build them with:
* rpath $ORIGIN/../lib

That way they always work, no need for LD_LIBRARY_PATH. The user can't specify
separate bindir and libdir anyway at the moment, so should be okay, right?

Yeah, that works. Just tedious to escape the mix of tcl, ninja, and shell:

  -Wl,-rpath,\\\$\$ORIGIN/../lib

\$ to escape tcl
$$ to escape ninja
\$ to escape shell

All combined: \\\$\$

Next step on the journey: get rid of libfble.a entirely. Switch everyone over
to libfble.so. Just so I don't have to worry about things anymore with regards
to that.

Trouble: pkgs/fbld/bin/fbld is installed to bin/fbld. Aside from that being
wrong in general (we should be installing the C implementation there), it's
problematic because $ORIGIN/../lib isn't the right value to use there. Should
be the same for any of the binaries built as part of fble packages.

---

Let's start with two relative runpaths: relative for build and relative for
installed. I don't think it matters what order we do them in.

---

Next issue: 

I'm getting an error at link time when linking a .o file created from my
aarch64 backend. I don't see any -pic option we can pass to the assembler. I
think I have to generate different assembly, specifically for 'Adr': loading a
label into a register.

Maybe we want something different for local vs. global labels?

Anyway, let's test out on gcc. What does gcc change one way or the other? I
can try on FbleGenericTypeValue.

	adrp	x0, :got:FbleGenericTypeValue
	ldr	x0, [x0, #:got_lo12:FbleGenericTypeValue]

What I'm currently generating:

  adrp x0, FbleGenericTypeValue
  add x0, x0, :lo12:FbleGenericTypeValue

Cool. That works. Now I just need to get the library rpath right, because the
binaries and the libraries they depend on are in a subdirectory.

---

Any way to fix rpath settings in general?

For a binary $bin and library $lib, we want:
* -L the directory to the library. That's still needed, right? Yes.
* -l<foo> for the library.
* -rpath from bin to library directory, for running at build time.
* -rpath from bin to ../lib, for running at install time
  Assuming we install all bins to bindir and libs to libdir.
* Take a dependency on the library.

How about: add a libs option to bin which takes the .../libfoo.so. For locally
built libraries. It can compute the necessary rpaths and everyone is happy.

---

Okay, it looks like it works. One annoying problem now: we have to know to
link against SDL for any stdio programs that depend on the 'app' package.
That's pretty annoying?

It's not just annoying. It's a real problem. I explicitly want people to be
able to run these unit tests without having SDL installed on their system.

It used to work. How?

We conditionally compile app.o into libfble-app.so based on enable_fble_app.
The stdio function adds the library as the list of objects to the test binary.
I guess before it would follow references from main, not see anything from
app.o, so not require anything from SDL. Now it sees libfble-app.so, we get a
false dependency, so it complains about the missing (false) dependency on SDL.

The person linking against libfble-app.so ought not to have to know what
libraries it depends on, don't you think? Except, they need to know if they
have to install that library or make sure it is installed.

That's pretty annoying.

Brainstorm:
* Any options we can pass to the linker to ignore the error, and everything
  goes away nicely in this case? We just end up with potential mysterious
  errors when you really do want to link against SDL.
* Use package config for each library to know what additional things to link
  against. Or something similar built into my build system.
* Give up on shared libraries approach.

Let's explore the options at least. Note this is going to be a general issue
with foreign function interface. You will write fble code that depends on
other libraries. Users will want to reference some but not all modules from
your package. They shouldn't have to know what external libraries they need to
link with to make their particular program work, and they shouldn't have to
install and link against libraries they don't need. But they should be
informed when they do need to link against a particular library.

Note that static libraries apparently work fine in that sense. You get an
error at static link time unless you specify the set of library dependencies
you need.

First step: understanding linker options. I want some way to say: ignore this
unmet dependency, because I know it won't be an issue at runtime.

-Wl,--unresolved-symbols=ignore-in-shared-libs
--[no-]allow-shlib-undefined 

Let's try --allow-shlib-undefined. Where would I add this? It doesn't really
make sense to put it anywhere specifically. The only reason I can safely do
this, and the only reason I will be able to safely do it in the end dream, is
because I know the test apps happen to not depend on any of the code where the
library in question is needed. The build system doesn't know that in general.

That means I need to put it on the call to the specific binary I'm building,
which means passing it through to stdio.

Yeah. Works, but not pretty at all.

---

Now the build system is using shared libraries. Summary of things I don't
like:
* rpath is annoying
* need for --allow-shlib-undefined for app test binaries.

Next steps:
* do shared libraries work on windows?
* remind myself again why I want shared libraries.

Reminding myself of how we got here:
* Foreign function interfacing doesn't scale today to build all possible
  foreign functions into the interpreter binary and the type of the fble main
  function.
* To make it scale better:
  - Have a generic FFI monad type that you can implicitly extend by
    implementing your own ffi operations.
  - Allow fble packages to ship the code needed for their own ffi operations.
* Compiled fble packages can already ship code needed for their own ffi
  operations. Interpreted fble packages can't.
* So the first primary goal is to make it possible for interpreted fble
  packages to ship their own ffi operations.
* The way it works in practice today, is each package (core, app) ships its
  own interpreter (fble-stdio, fble-app), which declares the set of permitted
  ffi operations. It's not possible, for example, to write an fble-app that
  makes use of an FFI operation fble-app doesn't know about.
* Thus, the proof will be my ability to write an fble-app that makes use of an
  FFI operation fble-app doesn't know about, and run it interpreted.

Let's come up with something a little more specific, because I'm sold on the
issue yet.

We are writing an fble-app. So we need SDL.
Say I want to write my own web browser as an fble-app. I need sockets, which
is a foreign function interface. That would be part of a 'sockets' package. So
we have the following packages:

* stdio, app
* sockets
* browser

The 'sockets' package has to ship binary code, like stdio and app packages.
Because they provide ffi implementations.

The 'browser' package should have the option of being entirely interpreted.
But it's also the only package that knows it needs the ffi code from both app
and sockets.

The best option we have today for this kind of thing is for browser to provide
its own interpreter, fble-browser. fble-browser is a binary linked with both
app and sockets to get the ffi calls it needs.

If I didn't care about interpreted code at all, this wouldn't be an issue.

Even if I care about interpreted code, as long as I don't mind shipping
fble-browser as a custom interpreter, it's not an issue. What's the issue
with that?

Certainly it would be convenient if there was a single fble interpreter binary
we could run that finds whatever ffi code it needs and loads it dynamically.

Note, we could separate out ffi code from module code if we want. For example,
maybe the 'app' and 'socket' packages are entirely interpreted, but they
include libapp-ffi.so and libsocket-ffi.so libraries with the native bindings.
You also need the linker flags for SDL, sockets, etc.

Then the question is: can I write a single fble interpreter binary that knows
how to dynamically load libapp-ffi.so and libsocket-ffi.so passed on the
command line, and then lets you run your interpreted code?

Another reason to support .so files is because it seems like a more standard
thing to do than .a files, and I ought to try and get there eventually, right?

For example, we can compare the output size of the fble install before and
after switching to .so files.

Before: 42M: 22M bin, 18M lib
After: 12M: 292K bin, 8.3M lib

That's pretty significant.

Summary of options:
1. Avoid use of .so files.
 - have to provide custom interpreter for different combinations of ffi
   packages.
 - deal with larger file sizes.
2. Use of .so files without dlopen.
 - have to provide custom interpreter for different combinations of ffi
   packages.
 - does it work on windows?
 - smaller file sizes, potentially more standard, but also lots of pitfalls to
   worry about there.
3. Use of .so files with dlopen
 - full flexibility
 - does it work on linux?
 - does it work on windows?

Interesting that the language changes for FFI and the restructuring of how we
do stdio and app ffi typing doesn't depend on any of these options.

Anyway, key things to de-risk:
* building with shared libraries on windows (no dlopen yet)
* dlopen on linux
* dlopen on windows

Is there a modular compilation aspect to this? There is, right?
* I want to be able to ship compiled code for packages that doesn't include
  the fble source code.
* It's a similar composition problem. But work-aroundable. Namely: you
  implement your own custom interpreter pre-linked against whatever packages
  you need.

I have three things to derisk. Let's get going on those.

Next step: shared libraries on windows.
