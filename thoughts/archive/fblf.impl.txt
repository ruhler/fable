Implementing Fblf so that it runs fast.

For a true potential for speed, we've got to compile to C code:
* translate fblf struct, enum, and union types to corresponding C types.
* define equality functions for all struct and union types.
 - struct is field by field.
 - union is whichever field is the most number of bits.
* translate literal assignment to appropriate sequence of assignment statements.
* translate ref assignment to normal assignment in C.
* translate if statements to if statements.
* translate loops to loops.

* translate a 'par' statement for threads 'a' and 'b' by defining separate
  functions for 'a' and 'b'. The function takes the global state as input and
  returns a PC, which is a number specific to that program which identifies
  the current program counter. Wrap calls to both functions in a loop:
    state->pc_a = 0, state->pc_b = 0;
    do {
      func_a(state);
      func_b(state);
    } while (state.pc_a != DONE || state.pc_b != DONE);
   
  where the functions func_a and func_b update the pc state before they
  return, and read the state, jumping to the right place in the code to start
  using a switch statement that is interleaved with the rest of the code of
  the function.

  I guess another option would be to assign program fragments to threads
  statically, then define one function per thread and run them all with a
  single top level while loop in round robin fashion. That sounds good. That's
  maybe better. Scales better to more threads? Depends actually. Maybe doesn't
  scale better.

---

Use of bitfields to improve memory overheads looks suspect to me. gcc isn't
doing things I expect. If I want things to be packed, I should probably
implement that explicitly myself with bitwise operations. Should be possible,
if annoying, as a future optimization (if it actually turns out to be an
improvement).

---

Manually packing things into bits has its own overheads. If I'm storing a 32 or
64 bit bit pointer for each bit, that gives us the same overheads? Perhaps not
entirely if we allocated things in chunks and use math to compute intermediate
addresses. I'm not sure.

---

If we introduce module compilation, which I think we need in order to have
efficient integer operations by providing primitive C implementations, then a
question comes up about whether a program might yield or not.

If a program could yield, then whenever we execute it we have to put it around
a loop and add another program state to track. If the program can't yield,
then we don't have to do that. Something like a 64 bit addition implemented as
modules shouldn't have to yield at all. It shouldn't have to store program
state at all. But if it's implemented with modules, either it has to be
conservative and do a lot of looping and potentially yielding, or we have to
capture in the type of the module whether the program can yield or not.

One way to capture explicitly if a program can yield or not would be to have
an explicit yield statement. If you use yield or wait, then the program is
interruptible. Otherwise it isn't. I like that, in the sense that as a
programmer I know what loops need to allow concurrency and what loops will
finish all on their own. The trouble is, this would imply that we have to do
all non-yields atomically. And that feels unlike how the real world works, and
the original motivation for introducing concurrency in the language.

In my md5 sketch prototype, I did distinguish between Run and RunI methods.
Explicit yielding could make for more explicit scheduling behavior. Maybe we
say, in a parallel program, we always start with the first, when it yields we
go to the second, when that yields we go back to the first, until all are
done. Any way to control when a parent thread yields as opposed to switching
back to another child thread?

If we had global state that persisted beyond a particular program, maybe we
don't need concurrency in the language? Everything can be simulated?

How would Pad and Compute look in md5?

Compute: you call it, it runs on whatever existing block exists. Then we can
write:

Pad;
Pad;
Pad;
Pad;
...
Compute;
Pad;
...

But then we need to distinguish between instantiating the state for a module,
and instantiating the program for the module. Because we are saying we can
rerun the program for the module repeatedly on the same state.

We sort of already do that in fblf with loops, but this would be a less
restricted form. Imagine, if we can avoid having concurrency built into the
language, it feels like that's the last bit we need to really make it fast.

What would that look like?

Perhaps a program is parameterized by a segment of state.
Perhaps we define state and program separately. You can rerun the same program
on different states?

Well, that's kind of what we already have, except instead of having the
program declare its own internal state, you give it the state it needs to
compute.

Maybe the thing we want to define and compose is some thing with its own state
that you instantiate and repeatedly run. Perhaps that's the module we want to
introduce to the language anyway.

module Foo(args ...) {
  declare state.

  Run = {
  };
};

This is starting to look a lot like a class. Why have only one program we can
run over the state? Why not allow multiple programs to run over the same
internal state?

In other words, we require the user to explicitly code concurrency in their
program when desired. That probably means the top level interface to a program
should be one where someone else is driving the program. The program shouldn't
assume it has full control while something else magically pokes register
values.

