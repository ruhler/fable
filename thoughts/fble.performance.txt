Fble Performance
================
Right now is pretty bad. How can we make things better?

Some things I notice from profiling:
* A lot of GC related things.
* A lot of call/return instruction related things.
* Note that we have profile arcs and no optimization turned on, though that
  seems to only be a 2x speedup.

It's a lot of little things being called a ton, rather than a few big calls.
That's a bit annoying, because it makes me want to do low level optimizations
that aren't very general, and it makes it hard to figure out what to focus on.

Remember the disadvantages we expect versus, say, C:
- no primitive types or operations
- automatic reference vs. value
- immutable data types
- automatic memory management
- closures that capture variables from local scope
- light weight multithreading

Add to that the fact that we are currently running an interpreter instead of
compiled code. The space invaders game looks to be running over 100x slower
than it needs to be. How can I get a factor of 100x improvement?

It's got to come from big, high level changes. I don't think it will come from
lots of little low level hacks. I don't want it to come from lots of little
low level hacks.

Some obvious things to try:
* Compile instead of interpret.
* Inline function calls.
* Have a variation of the language with zero cost abstraction, mutable data
  types, and explicit memory management.
* Try to do lots of tiny little cleanup/optimizations and see if they can add
  up anywhere.

---

Here's a proposal for a different programming language that I believe I could
implement efficiently, without garbage collection or function calls at run
time.

We have struct and union types as in fble, except non-recursive.
We have a reference type T@&, which is a reference type values of type T@.
We have a program.

You have the following operations in the program:

Access:
  Given a reference to a struct and a field, returns a reference to the field.
  S@& -> Field -> F@&

Assign:
  Given two references of the same type, copy the data in one reference to
  another.

  T@& -> T@& -> Prog@
Condition:
  Select a program to run based on the tag of a referenced union value.

  U@& -> Prog@, Prog@, ... Prog@ -> Prog@

Loop:
  Run a program repeatedly as long as the tag of a referenced union value is
  0.

  U@& -> Prog@ -> Prog@

Var:
  Create a reference to a variable of a given type, and use that in a given
  program.

  T@ -> Prog@ -> Prog@

Sequence:
  Given two programs, run one program after the other.

  Prog@ -> Prog@ -> Prog@

Parallel:
  Given two programs, run both programs in parallel and wait for them both to
  finish.

  Prog@ -> Prog@ -> Prog@

For multithreading, we may also want:

Swap:
  Given two references of the same type, swap their values. Possibly needed
  for multithreaded synchronization.

Block:
  Wait for a referenced value to have a given tag before continuing.


Why this can be done efficiently:
* There are no function calls, so no cost of call/return.
* The total amount of memory used by a program is constant. So we can
  pre-allocate all memory and don't need runtime GC.
* Non-recursive and non-sharing values mean we can pack values into bits for
  efficient copying.

We still have to support multithreading, but the number of threads is constant
and the burden of synchronization is on the programmer, so it shouldn't be too
hard I hope. Just round robin execute commands among the currently running
threads.

The key bits are: working with finite, non-shared values. Using constant
memory for the entire program. Anything unbounded has to be done broken up
into bits by time.

Now we have two paths we can pursue: My newly proposed language, or fble with
elaborate.

For fble elaborate to be useful, you practically have to limit yourself to
non-recursive values. I don't have a great story for loops though.

For my newly proposed language to be useful, we need programming level
abstractions (e.g. functions, polymorphic types, program modules).

If I truly want something to go fast, then I'll allow you to define primitive
program modules in different languages (e.g. C), and build apps on top of
those.

---

I am going to retire the efforts on fble elaborate. What I've learned from
fblf is that full inlining of all functions, even if they are finite, is going
to blow up code size far beyond what's practical. For elaborate to be useful,
then, it would have to be up to the implementation to choose how much gets
inlined. That's completely opaque to the developer. Then the question is what
is the spec for elaborate? You call it and it takes an arbitrary amount of
time to return a value that takes an arbitrary different amount of time to run
than the input? How can you do anything with that?

It had a good run, but I no longer think fble elaborate is the answer. I don't
see any reason not to keep the internal changes to the interpreter that we
made for symbolic elaboration, where a type checked expression is a value.
Unless it gets in the way.

---

I now compile fble instructions to c code, to hopefully avoid the overheads of
instruction dispatch. It doesn't look like that's the big bottleneck though.
The big bottleneck appears to be doing a lot of function calls, each of which
requires allocating a stack frame. And for every stack frame allocated we do
incremental gc, which means traversing more objects.

I bet, if we could avoid doing so many allocations for function calls, we
would spend much less time doing gc, which would improve performance a bunch.

---

The profile for fble bench says:
* 107,519,013 new objects allocated, of which 30,928,572, or almost 30% are
  FbleStackValues. That's more than the number of union values we allocate for
  the entire program. So, 108M total allocations, 31M stack values, 31M ref
  values (for the stack result), 26M union values, 16M struct values, and 4M
  function values.
* We call MarkRef 134M times. That's maybe 30% more than once per allocated
  object. MarkRef is the top entry on the profile. AddRef is second, IncrGc is
  third.

The point is, we are spending a lot of time doing GC, because we have a lot of
allocations, the majority of which are not even a visible part to the end
users program. Here's what I propose to fix this:

1. StackValue should not be an FbleValue. It should not have garbage
collection. It's not longer mixed up with thunk values, so I don't see any
issue with this.

2. StackValue's result should be stored as a pointer to something that expects
to receive a strong reference to the value. Not as a RefValue.

Assuming I haven't missed something that prevents (1) and (2) from working,
this will cut down number of gc object allocations by over 50%, which will cut
down gc by over 50%. Which should give, I would estimate a 10% improvement in
performance of the fble benchmark. In other words, from 5m45s to ... about
5m0s? Okay, doesn't feel like a lot. Especially when before the split of
ThunkValue into RefValue and StackValue we were at 5m10s. But on the flip
side, if you think adding 31M StackValue's slowed down the program from 5m10s
by a minute, then removing 31M would speed it up by a minute, leading to a
runtime around 4m10s, which is much better. Anyway, it's worth a shot.

Step 1: Confirm that the references to stack values we need are strong
references, and the only references to stack value results we need are strong
references.

Yeah. Looks like that's the case. So we'll want locals to be an array of
strongly retained values or NULL. We have to set it to NULL when we release
it, if we do. We'll have to release any time we overwrite a value.

Note: this effectively will undo a change we did to make stacks into gc values
in the first place. But I don't think things need to be as complicated as they
were before. We can track live values on the stack using NULL versus non-NULL.
Clean up values when we overwrite a non-NULL and when we free the stack. We
saw a 1 minute increase in fble bench time when we made that switch. I'm
hopeful we can see a 2 minute improvement in fble bench time when we switch
back.

Yes. As expected, we're down to a 4 minute runtime with this change.

--

What's next for performance? It would be nice if I could profile the compiled
code...

The interpreted code shows time in InterpretedRunFunction, Release, Retain,
FrameGet, FbleReleaseValue, MarkRef, IncrGc, CallInstr, FbleFree,
FbleRawAlloc, PopStackFrame, FbleRetainValue, FbleStrictValue, FbleThreadCall,
FbleThreadTailCall, FrameSetBorrowed, FrameGetStrict.

It's hard to say. Clearly GC is still an issue.

---

I managed to get a compiled version going with gprof. Half the runtime is
spent on GC/allocation related things:

  9.85     22.72    22.72 294626402     0.00     0.00  Release
  8.38     42.04    19.33 249096875     0.00     0.00  Retain
  7.88     60.21    18.17 416137357     0.00     0.00  FbleReleaseValue
  5.65     73.25    13.04 45556575     0.00     0.00  IncrGc
  5.59     86.14    12.89 63284158     0.00     0.00  MarkRef
  5.51     98.85    12.72 120139380     0.00     0.00  FbleFree
  5.02    110.43    11.58 120139380     0.00     0.00  FbleRawAlloc
  4.00    119.66     9.23 30928715     0.00     0.00  PopStackFrame
  3.98    128.84     9.18 249096875     0.00     0.00  FbleRetainValue

The vast majority of Release calls is from cleaning up stack frames. Most of
the Retain calls are from stack calls too, split fairly evenly between the
function and the args.

IncrGc and MarkRef is from New object allocation, which is spread pretty
evenly across different .fble code.

FbleFree is mostly stack frames and IncrGc. Most FbleRawAlloc are from
FbleThreadCall and New.

So, in summary, two things stand out: Allocations and Retain/Release from fble
function calls, and GC from object allocation spread across the fble code. I
think figuring out how to deal with the stack more efficiently is the next
thing to focus on.

Interestingly, the code generated by tcc seems to be slightly faster than the
code generated by gcc for the generated C code. Oh. Actually, I measured the
wrong thing. Nevermind. gcc is 3m24s vs tcc 3m30s. And for the fun of it,
clang is 3m26s.

---

For normal function calls, the function and arguments will be retained by the
caller. There is no need to Retain/Release these.

For tail calls, that may not be the case. Normal function calls happen twice
as often as tail calls according to profiling. You almost wish you could have
a single bit stored with the value itself to say whether you need to
retain/release it or not. Like... a reference count? So, you still pay the
cost of checking per object, but you avoid the function call to Retain/Release
by inlining the initial check?

---

Now that we are generating custom aarch64 assembly when we compile, I want to
step back and do another performance analysis.

My intuition tells me we have the following potential performance
opportunities:
* Pack data into words when possible, as discussed previously, to reduce gc
  and allocation effort required.
* Use the native stack instead of a managed stack for multithreading.

Step 1: fble based profiling of fble-bench

Just to get a sense of what fble code seems to take up time. Then perhaps we
could write a micro benchmark to focus performance analysis on. I kind of
suspect we could just increment an integer in a loop to get a good idea.
Something like:
 
for (int i = 0; i < 1000; ++i)
  for (int j = 0; j < 1000; ++j)
    ;
 
Step 2: Let's try using linux perf tool again, so we can test on our generated
assembly.

Goal to start: ideally I can sample periodically over the course of the
benchmark and get a histogram of PC addresses where the samples took place.
I should be able to label which functions the addresses come from and get a
sense if we spend a lot of time, for example, in GC, or allocation, or
generated assembly, or what have you.

I'm kind of guessing it's almost all going to be in allocations. Is it worth
trying to optimize my allocator rather than use the slow debug on top of
malloc based allocation?

It took a shockingly long 32 minutes to run compiled fble-bench with
profiling. That's 10x overhead for profiling?

For the fun of it, let me try running interpreted with profiling.
That only takes 6m15 seconds. Something is very wrong. If it's faster to do
profiling with the interpreter and they give the same results, why support
profiling in compiled code?

Oh. You know what it is? A bug. Compiled code looks to be sampling every cycle
instead of every 1000. That should be fixable.

gcc generates the following code:

    1acc:       94000000        bl      0 <rand>
    1ad0:       12002400        and     w0, w0, #0x3ff
    1ad4:       7100001f        cmp     w0, #0x0
    1ad8:       54000081        b.ne    1ae8 <FbleInterpreterRunFunction+0xa4>

    1adc:       d2800021        mov     x1, #0x1                        // #1
    1ae0:       f94027e0        ldr     x0, [sp, #72]
    1ae4:       94000000        bl      0 <FbleProfileSample>

I generate:

  bl rand
  and w0, w0, #1023
  cbz w0, L._Run_0x181b2258.0.postsample

  mov x0, R_PROFILE
  mov x1, #1
  bl FbleProfileSample

What's wrong? What's broken here?


The condition is flipped? gcc says: if it is not equal to zero, skip sampling.
Ah. That's it. I need cbnz.

Now compiled profile is 4m19.816s. Good.

---

fble profile shows: 

Looks like a lot of time iterating over map and list data structures?

perf shows...

Previous command lines I've used:

       perf record -F 997 -d --call-graph dwarf ./out/bin/fble-test prgms/fble-bench.fble prgms/
       perf report -g folded,address,0.1,0 > perf.report.txt
       #perf report -g folded,address,callee,0.1,0 > perf.report.txt

Let's try:
  perf record -F 997 -d <cmd>

To try and sample virtual addresses at a rate of 997KHz

We get 161130 samples.

Now, to dump the samples, I think: perf report -n.

I'm happy to look at raw sample counts, and then do my own visualization from
there. No stack chains for the moment. I'm not sure my generated assembly
supports them.

Now, how to make sense of what we see? The samples are organized by function,
but some functions are bigger than others, so is that a good way to group
them? Or does it bias towards looking at bigger functions?

The top entries are all allocation / gc related:

    10.01%         16125  fble-bench  fble-bench        [.] FbleReleaseHeapObject
     9.57%         15435  fble-bench  fble-bench        [.] MarkRef
     8.90%         14330  fble-bench  fble-bench        [.] FbleRetainHeapObject
     7.47%         12034  fble-bench  fble-bench        [.] IncrGc
     6.14%          9898  fble-bench  libc-2.28.so      [.] _int_free
     5.93%          9551  fble-bench  libc-2.28.so      [.] _int_malloc
     5.66%          9122  fble-bench  fble-bench        [.] FbleFree
     5.02%          8093  fble-bench  fble-bench        [.] FbleRawAlloc
     3.34%          5375  fble-bench  libc-2.28.so      [.] malloc
     3.32%          5343  fble-bench  libc-2.28.so      [.] cfree@GLIBC_2.17
     3.11%          5006  fble-bench  fble-bench        [.] Refs

How long is the tail though? I'm not sure this is a meaningful way to look at
the data. Can I dump raw addresses?

Looks like: `perf annotate -n` shows things pretty clearly: it shows number of
samples per address, interleaved with source code. That's pretty nice. Still
not sure how I want to think about and interpret the data though.

Looks like samples mostly happen after branches, loads, and stores. Not
surprising. That's when we could stall for a while.

Things that stand out:
* obj->space in MarkRef looks like it must be missing in the cache a bunch?
  Like, accessing all the obj gc headers is slow.

Hmm... So it seems I have lots of good information, but no obvious way to
organize and interpret it.

Is it possible to avoid exporting the local labels in assembly? That way we
could group everything from a _Run_* function together? I think that could
give a better picture.

Looks like I need to use ".L" instead of "L" as the label prefix. I'll try
that.

---

Just to see what it looks like, I want to try running perf with stack traces.

Try: perf record -F 997 -d -g <cmd>

Which should use the frame pointer to unwind. I think that should work with my
generated assembly. I don't see an obvious option for specifying the stack
depth. Let's see what it does.

For the report, something like:
  perf report -n -g folded,0,0,caller,function,count

This groups together chains that have the same function, ignoring the specific
addresses. That probably makes sense to do. I think this is the data
presentation I want. Note that all sorts of other presentations exist that
don't particularly make any sense to me.

This is a nicer picture than the flat array I think.

You know what will help a lot? More descriptive names for the _Run_ functions.

This shows that we are basically dominated by time spent doing allocations of
new objects, which have the fairly big cost of IncrGc, among other things. I
think my intuition is correct. The biggest thing we can do is to pack small
objects and pass by value instead of by object, to cut down on number of
object allocations and GC. Another, smaller magnitude improvement we could
make is using the native stack instead of a managed stack.

Let me see if I can get that IncrGc to show up on the top of a report. I guess
what I want is for each function, the number of events that happen in that
function. Expect to see '_start' and 'main' at the top, but eventually IncrGc.

We get close with:
  perf report -n -g none,0,0,caller,function,count

Just look at the top set of entries. That looks like it has the right
percentages. Too bad it doesn't show total samples.

The simple version:
  perf report -n none

43% FbleNewObject
33% IncrGc

It should also be possible to make an fble profile style presentation of the
data, with 'time' in samples but no 'count' column. I think that's worth
doing. It would be great if I could reuse the C code for profiling. Maybe
treat each individual sample as single count. Then all we need is enter/exit
calls per sample. That should be doable.

It looks like perf has some support for python scripting. See the
perf-script-python man page for more info. So maybe I write a python script to
extract the info I want from perf.data and convert it into some text format I
can easily parse and consume by the C code. The write an fble-profile tool
that takes a trace as input and outputs the profile.

What trace format should we use? Is there a standard one? I basically want a
sequence of: enter <id>, exit, replace <id>, sample <n> calls.


Summary:
  $ perf record -F 997 -d -g <cmd>
  $ perf report -q -g folded,count,0 | fble-perf-profile

Where fble-perf-profile is a C program I wrote that parses the perf report
directly. That's easier than introducing a python script into the mix.

---

Running perf based profiling on invaders game shows:
 The prominent fble functions are:
 /Int/IntP%.2P0
 /Int/Int%.P
 /Int/IntP%.2P1
 /Int/IntS%.IntP

With most time spent in FbleNewUnionValue, in all cases.

The fble profile shows all the time is in /Invaders/Aliens%.Draw:

**     7205    18159  /Invaders/Aliens%.Draw!![051d] **
       7205     5950  /Invaders/Aliens%.Draw!!.alien_x[0527]   
       7205     5453  /Invaders/Aliens%.Draw!!.alien_y[0528]   
       7205     4602  /Invaders/Aliens%.Row![04c7]   
       7205     1096  /Invaders/Aliens%.Sprite![050e]   
       7205      568  /Drawing%.Translate![018c]   

That's: Add, Mul, Div.

I don't think we're trying to do an unreasonable amount of Int operations
here. And it looks like the main performance issue is just the raw number of
Int objects we have to allocate to do the Add, Mul, Div. I don't think
function overhead is that significant. And it looks like most of the cost of
the allocation is actually the time spent doing incremental GC.

Lots of object allocations means lots of GC. As we've said before, I think we
could cut down a lot on the GC needed if we can pack as many objects as we can
into bits that we pass as values instead of as objects. I think this is
probably much more significantly important to do that, for example, using
native stack instead of managed stack. It's got to be the next step for fble
performance.

---

Now on the top of the list is allocating and freeing for every push/pop of the
stack. This should be a straight forward fix. We want to do batch allocation
instead of allocation per push/pop. So, allocate 1MB or 8MB at a time and just
increment a pointer the rest of the time.

---

Implementing a custom stack allocator helped with the cost of stack
allocations. Now we're back to data type allocations showing up as the top
contributor to runtime. Time to do next round of optimizations for packed data
types.

