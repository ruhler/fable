Fble Spec Tests
===============
Question: can we change how we specify fble language tests?

Currently they are specified using tcl with a funny mechanism for specifying
module hierarchy. Question is, could we write .fble files directly, like as a
separate fble library package with no other fble library dependencies? I think
that could be nicer.

We just need a way to indicate the metadata for each test:
 fble-test
 fble-test-compile-error <loc>
 fble-test-runtime-error <loc>
 fble-test-memory-constant
 fble-test-memory-growth

Advantages of what I'm envisioning:
* No need for special tcl syntax for modules.
* We know the name of the top level test module.
* We write .fble files directly, so we get syntax highlighting and direct line
  numbers for errors.
* We avoid an extra extraction step to get .fble files, which should simplify
  the build system notably.
* Hopefully more easily reused by hypothetical other fble implementations.
* We can compile Nat.fble just once and easily reuse it.

All I have to do is figure out a reasonable way to do the metadata.

Seems something like checktest from llvm, but I don't really like that, and I
certainly don't want a dependency on llvm.

Most natural thing to do is add a comment to the code. Some syntax tests will
be out of reach, like the ones involving comments, but that's such a basic
thing I don't think we need to test that using a separate test.

Note that the top level for the test case may be different from where the
error occurs. I suggest putting a metadata line only at the top level of the
test case?

So it's pretty straight forward, right? First line of the file for a test case
should be:

# fble-test-* <loc>

Or, maybe, because we aren't using tcl, we can use different names. Like:

# TEST
# TEST_COMPILE_ERROR <..>
# TEST_RUNTIME_ERROR <..>
# TEST_MEMORY_CONSTANT
# TEST_MEMORY_GROWTH

Or, like, have assertions? I dunno. I don't think it's hard, it's just
arbitrary.

We could do double comment:

## NO_ERROR
## COMPILE_ERROR 8:3
## RUNTIME_ERROR 8:3
## MEMORY_CONSTANT
## MEMORY_GROWTH

If it isn't annotated, it isn't as separate test case.

How will the test runner work? How will it integrate with the build system
where we want to compile?

We could run a single grep command over all modules in the directory. That
gives us a mapping from filename (hence module name) to test case. We can then
do a loop over those lines. Maybe we can generate that using a build rule and
have build.ninja depend on it and build.ninja.tcl read it? With a slight
bootstrapping process to generate it initially I suppose? That's unnecessary
though.

Then we can parse the entry easy enough in tcl.

So, I just need some metadata easy to locate with grep, and easy to parse from
tcl. Let's explore options.

For prefix, we want something that is a comment and is unlikely to occur by
accident. It may be nice if it indicates to readers of the code what it is,
instead of being some magic. If we have text, we don't need magic symbols
aside from the comment character. I like 'test' and 'fble' in the name.

# FBLE-TEST no-error 
# FBLE-TEST compile-error 8:3
# FBLE-TEST runtime-error 8:3
# FBLE-TEST memory-constant
# FBLE-TEST memory-growth

Say it has to be the first line in the file, because why not? Or is that too
picky and too easy to accidentally not be applied? No. I think anywhere should
be okay. Allow any amount of whitespace between tokens. How to prevent
accidental uses of 'FBLE-TEST' in a document? I think we want some special
symbols for it.

Okay, how about this, a line of the form:

# @@fble-test@@ <type> <msg>

<type> is one of:
  no-error
  compile-error
  runtime-error
  memory-constant
  memory-growth

<msg> is an arbitrary string expected to be included in the error message,
primarily used to identify the location of the error.

There should be at most one @@fble-test@@ line per file. I'm not sure who
enforces that though.

Clearly it has some special meaning. Simple to detect with grep. Clearly
related to fble and testing. Simple to parse. I say go with it. No need for a
name, the current module path should be fine?

Good. What do you think? Shall I try doing this cleanup?

---

What are the next steps here? I need to come up with a plan for how to
implement this.

So we have the .fble code already extracted.
We get a list of tests from build.ninja.tcl using glob and/or grep.
We parse the test type from within build.ninja.tcl and call the appropriate
spec-test-compile, fble-test, fble-test-compile-error, etc. functions.

I'll want to migrate slowly to the new scheme. So duplicate and modify in
build.ninja.tcl to start. Start with just a handful of tests, get that
working, and slowly migrate.

What directory structure do I want for the spec tests? Put them in spec
directory maybe? spec/SpecTests/... I would say the tests are part of the
spec. 'lang' versus 'spec'? I think 'spec' is more specific. Do we want an
'fble' prefix? Or is everything okay to be fble based now?

Let's try 'spec/SpecTests', where spec is the -I value and tests go under
/SpecTests/...%

Immediate next steps:
* Port the langs/fble/0... tests to new format.
* Set up build.ninja.tcl infra for running tests in the new format.

---

The easy way to build the new test format is:
* For every file under SpecTests directory, create a spec test rule that calls
  a script to run the file. The script reads the metadata, verifies it is a
  test (trivially passing otherwise), and executes the commands necessary to
  run the test.

That way we don't have to read the contents of any of the spec tests to
generate the build.ninja. The downside is we don't know at build time whether
the test depends on fble-test, fble-compile, fble-mem-test, etc, so we'll end
up adding a dependency on all of those, which could trigger unnecessary test
reruns in some (rare?) cases.

The slightly harder way is to parse the test file in build.tcl and generate
rules depending on the test. Let's start with the easy way. Once we have the
easy way, we can decide if we want to migrate to the hard way, which needs all
the same code, just a bit more and in different places.

First steps for implementing script to execute:
* Pass an include directory and a module path? Or include directory and .fble
  file name. Either way, want the info necessary for the .fble file name, the
  include directory, and the module path.
* Read the file, search for and parse the @@fble-test@@ metadata.

---

Next step, let's consider the case of no-error.
* No need to extract anything.
* We want to compile .fble -> .o all modules the test depends on.
 - How do we know which modules the test depends on?
* We want to compile the main .fble -> .o with --main.
* We want to generate a binary.
 - Can we call ld directly, or do we need gcc flags?
 - How does that interact with code coverage?
* We want to run fble-disassemble, for the fun of it.
* We want to run interpreter with --profile (can ignore result)
* We want to run compiled executable.
* We want to catch any errors and report the test file.

Questions:
* How to know which modules to compile?
  - Maybe run fble-deps to get that info?
  - Can we avoid recompiling, e.g. Nat.o multiple times?
  - Or should we separate compilable from non-compilable, and just compile all
    the compilable modules up front in one big go?
* Where do we put output files?
  - tempdir? out dir?
* How to link compiled executable / how does that work with coverage?

I should be able to answer the question about linking on the existing spectest
infrastructure. Maybe let's start there.

Looks like directly invoking 'ld' doesn't work great, because it's missing
some standard options and libraries that gcc adds when it calls 'ld'. For
example, _start as entry point and whatever library implements 'rand'.

Does that suggest we want to reuse the build options in build.ninja for
building these things? In which case, maybe I really want to parse the spec
test .fble files and generate build rules from those rather than doing a
one-off test run directly from the .fble?

---

Here's the issue:
* Unlike the old approach to spec tests, the new approach does not explicitly
  list the modules that a test depends on.
* Using fble-dep to get that information is feasible, but suggests we can't do
  it as part of generating the build.ninja file, because we need build.ninja
  to build fble-dep first.
* The fact that ld isn't trivial to call directly, and that we ideally would
  like to reuse compilation of Nat.fble, suggests we would prefer to have
  build.ninja in charge of building for tests.

So we are stuck in a knot. It's a question of how I want to break the knot.
Some possibilities:
* Come up with some way to make dependencies explicit in test cases. For
  example, a test /SpecTest/.../Foo.fble depends on exactly Foo.fble and
  everything in the directory Foo/, plus Nat.o for memory tests.
* Don't worry about recompiling Nat.fble or factoring out common code for
  linking fble-based executables.

As far as building with coverage goes, I think the current approach is the
best we can do:
* Run fble-test.cov, fble-compile.cov tools.
* Link tests against non-coverage libfble.a.

Because I assume gcc needs to generate special code to support coverage, and
when we are compiling fble code, we bypass gcc and whatever special code you
need. In other words, compiled fble code does not support code coverage.

I kind of wish there was an alternative to gcov we could use for code
coverage. Something perf based possibly? I feel like perf would require
sampling at every instruction, which is way overkill.

---

Solution for build.ninja dependency on fble-deps: see if I can set up a
subninja file that depends on fble-dep. I feel like ninja should be able to
read the initial build.ninja file, use it to build fble-deps, use that to
generate the subninja file, then reload the subninja file, start over again,
see fble-deps and subninja are up to date, and execute those rules.

Solution for knowing whether we should be able to compile a test .fble or not:
any .fble file that can't compile should be marked with @@fble-test@@
compile-error. Even if it's not the main test entry, for example, if we want
to test reference to a module that doesn't compile. That's a rare case, so
it's fine to have an extra @@fble-test@@ compile error on the referenced
module saying that compilation would fail from the top level module too.

So, I guess my vote is the following:
* Use fble-deps to figure out dependencies we need to compile and link
  together for a test. We want that regardless of whether we build via ninja
  or not.
* Start by using separate test script that duplicates compilations and gcc
  command lines. Generate intermediate files to out/<path>/ directory. Boot
  strap that way.
* If desired, switch to hack for generating subninja dynamically.

Next steps:
1. Add support for interpreted test cases via script.
2. Set up spec/build.ninja to run test cases.
3. Add support for compiled test cases.
  - Figure out how to know what to compile and link using fble-deps.
4. Migrate rest of test cases.
5. Optionally figure out hack for generating subninja dynamically.

---

Note: One thing nice about having the .fble be the source for the test is we
don't have to output a separate error message to the user to direct them to
the .tcl file.

