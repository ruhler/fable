Fble 0.2 Performance
====================
Working on optimizing the sat solver, it's showing 50% time in Lt/Ord
operation on Int@. That .fble code looks about as straight forward as it could
get. Anything worthwhile we can do in the fble implementation to speed things
up here?

A few ideas come to mind:
* Remove profiling logic from generated code?
 - Maybe put it behind a compile time flag?
 - Because it seems silly to slow things down all the time for a rarely used
   use case.
 - It's probably worth at least measuring how significant this is.
* Remove explicit error checking from generated code?
 - Maybe we just segfault and ask users to run a debugger to see the issue?
 - Maybe we can use signals somehow to trap and handle these cases implicitly?
 - Probably worth at least measuring how significant this is for performance.
* Improve value packing algorithm.
 - Though honestly I don't think that will help at all for Lt/Ord, because I
   don't expect any allocations there.
* Something about function call/tail call optimization?

---

Reviewing perf profile for the sat problem in question:

It shows only 25% time spent in Ord, and about as much time in
FbleNewHeapObject. It could be hard to interpret the profile, given how tight
a recursion is going on here.

Self time shows 8.5% in the Ord function. Everything else is runtime code:

    %     self  block
 8.49     4110  _Run.0x22673a48._2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_[0029]
 6.50     3145  MarkRef[0043]
 5.86     2838  FbleRetainHeapObject[0010]
 5.70     2760  FbleThreadTailCall[002e]
 4.86     2350  FbleStrictValue[002d]
 4.64     2248  FbleUnionValueTag[002c]
 4.54     2196  FbleThreadCall[0001]
 3.80     1840  FbleThreadSample[003e]
 3.70     1790  FbleReleaseHeapObject[003f]
 3.48     1682  IncrGc.part.0[001a]
 3.39     1639  Refs[004c]

Reading through the assembly, I feel like we could inline parts of things like
FbleReleaseValue, FbleRetainValue, FbleStrictValue, etc. Avoid the function
call overhead and having to save/restore regs.

My vote is we do some exploration. Don't worry about breaking things or
features. Use a separate branch. Remove profiling and error checking code.
Maybe try inlining things. Focus on aarch64 compiler target. See if we can
flesh out what we can do in terms of generating optimized code and how big a
different it has a potential to make.

After the exploration, we can consider if we want to change anything for
production.

---

First thing, establish a benchmark.

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m37.040s
user    1m36.508s
sys     0m0.469s

Next: Let's strip out all the profiling code. Anything that would be in the
critical path for runtime, including stuff in execute.c and generated code.

Commenting out profiling related code, all except tracking profile block
offset stuff for functions:

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m31.467s
user    1m30.843s
sys     0m0.537s

Attempted again:

So, Maybe 7% performance overhead from having the profiling code in when
profiling is not enabled.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m33.136s
user    1m32.554s
sys     0m0.508s

Next: Let's strip out all the generated error checking code.

$ time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m22.338s
user    1m21.828s
sys     0m0.476s

Next: let's look at the generated assembly code for IntP@ Ord, see how it
looks.

* The jump table looks potentially more expensive than need be?
.L._Run_0x31b06a48.0.pcs:
  .xword .L._Run_0x31b06a48.pc.1
  .xword .L._Run_0x31b06a48.pc.5
  .xword .L._Run_0x31b06a48.pc.15
  ...
  
  bl FbleUnionValueTag
  lsl x0, x0, #3
  adrp x1, .L._Run_0x31b06a48.0.pcs
  add x1, x1, :lo12:.L._Run_0x31b06a48.0.pcs
  add x0, x0, x1
  ldr x0, [x0]
  br x0

  This is a load followed by a branch. Difficult for branch prediction to
  predict anything from I would guess. Don't you think it would be much faster
  to do a binary search based on the tag?

  For example, consider a linear search on the 3 branches in this case:

  bl FbleUnionValueTag
  b.eq .L._Run_0x31b06a48.pc.1 x0 0
  b.eq .L._Run_0x31b06a48.pc.5 x0 1
  b .L._Run_0x31b06a48.pc.15

  No memory load required. We can take full advantage of branch prediction.

  And, if we use this approach, or a binary search, we can take advantage of
  the default option the user provides. So, character equality, for example,
  instead of being a 64*64 split branch, would be a 64 * 2 branch split. That
  sounds like a pretty big deal.
 
* Lots of calls to FbleRetainValue, inlining could help I think. In most cases
  this is either skipped entirely for packed values, or just increment, right?
  
Let's explore the generate of union select first. The right way to do it, I
think, is pass info about default branches down to the bytecode, and use
binary search. That's tricky to do and get right. For a first pass, just to
gauge how sensitive performance is to this, let me switch to a linear approach
like above. It should work fine for small branch counts. Won't work great for
this like character conversion.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m21.529s
user    1m21.114s
sys     0m0.398s

---

Another opportunity to improve performance I just saw: for union access, don't
check the tag value. Assume it's the right value. Hard to productize this,
but would at least be nice to gauge the cost.

time ./out/pkgs/sat/fble-sat < pkgs/sat/bench/easy/rope_0002.shuffled.cnf
unsat

real    1m19.422s
user    1m18.999s
sys     0m0.381s

---

Trouble: my numbers aren't very reliable. I see swings +- 2s in some cases
when running the same thing, and sometimes I forgot to build in the change I
intended.

Looks like error checking code is more costly than I first thought. That
changes things.

I kind of wish I had a more extensive set of benchmarks than just the sat
solver. Can we add md5? Is there an invaders benchmark?

Anyway, I couple ideas I think are worth exploring:
* Changing how we generate union select statements. Even if the numbers above
  don't suggest much difference, it feels like the right thing to do.
* Finding some alternative to error checking.
  For the most part it's NULL checking. We could use memory protection and
  signals to handle that?

---

Examples of what gcc generates for switch statements:

  switch (x) {
    case 0: printf("0\n"); break;
    case 4: printf("4\n"); break;
    case 5: printf("5\n"); break;
    case 6: printf("6\n"); break;
    case 7: printf("7\n"); break;
    case 12: printf("12\n"); break;
    default: printf("?\n");
  }

Turns into the equivalent of:
  if (x == 5) {
    printf("5\n"); break;
  }

  if (x > 5) {
    if (x == 7) {
      printf("7\n"); break;
    }
    if (x < 7) {
      printf("6\n"); break;
    }
    if (x == 12) {
      printf("12\n"); break;
    }
    printf("?\n"); break;
  }

  if (x == 0) {
    printf("0\n"); break;
  }
  if (x == 4) {
    printf("4\n"); break;
  }
  printf("?\n"); break;

It uses 'beq', 'bhi', and 'bcc' for the conditional branches. That is binary
search. 'CC' stands for 'Carry clear', same as less than. 'HI' stands for
'unsigned higher', same as greater than.

---

Migrating to new union select.

Benchmarks for awareness:

time fble-sat < rope_0002.shuffled.cnf: 1m39s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m42s
time yes | head -n 40000 | fble-md5: 0m59s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m41.013s

Initial draft status:
* Need to update union_select_tc everywhere.
* Need to update compilation from union select tc to instr.
* Want to update aarch64 code gen to do binary instead of linear search.

---

First draft done. It uses binary search for interpreter, but linear search for
aarch64 code.

time fble-sat < rope_0002.shuffled.cnf: 1m39s ==> 1m36s
time fble-stdio -m /Sat/Main% < 6cnf20_28000_28000_3.shuffled.cnf: 0m42s ==> 41s
time yes | head -n 40000 | fble-md5: 0m59s ==> 53s
time yes | head -n 20000 | fble-stdio -m /Md5/Main%: 0m41s ==> 0m39s

So, modest improvement in performance, which is nice. Note: these benchmarks
don't capture the pathological case of large number of tags. Probably should
have wrote something up for that. Oh well.

Next step: binary search aarch64.

---

I made an fble-cat program. I think this should be good to check the
pathological case of large tags, because it converts characters to and from
ascii.

Let's see, for example:

time yes '!!!!!!!!!' | head -n 10000 | fble-cat > /dev/null: 6s
time yes '~~~~~~~~~' | head -n 10000 | fble-cat > /dev/null: 8s

Hmm... It's hard to know how much of this is union select as opposed to what
characters happen to be where in the tree.

---

How to implement codegen aarch64 for binary search based union select. There
are a number of puzzle pieces to put together:

1. We want to know the max tag value possible.
Imagine you have

  goto x.?(1: a, 2: b, 3: c, : d);

You know min tag value is 0. You don't know if tag 4 is possible or not.
Binary search starts checking on '2', if greater than '2', there are two
possibilities:
 i. 3 is the max tag: jump direct to c
 ii. 4 is the max tag: need to compare tag to 3.

Thus we want to know max tag. Add that as a field to select_instr.

2. Determine if a tag interval range has one or more targets.
Inputs:
* list of targets with tags in interval range. e.g. [1: a, 2: b, 3: c]
* possible tag values. e.g. [0, 4]
* default target. e.g. d

Outputs:
  Either the single possible branch target, or note that there is no single
  possible branch target.

There are two cases when there is a single possible branch target:
 i. The list of targets with tags is empty. Default is the single possible target.
 ii. There is a single element int targets, and that tag is the only tag in
     the list of possible tag values.

3. The top level list of explicit targets may be empty.

For example:
  @ T@ = +(Unit@ t); 
  T@ x = ...
  x.?(t: ...);

Or:
  Char@ c = ...
  c.?(: ...);   

I'm not sure if the language allows the second example, but it certainly
should allow the first. We should make sure there are spec tests for these two
interesting cases.

4. You must explicitly branch to all targets. You can't assume the next target
is the next pc.

Who knows what changes we'll make to compile.c. From code.h point of view, the
targets could be unrelated to the current pc. So, in the case of (3) for
example, we require an explicit branch instructions to get to the target. We
can't assume the target is the next pc.
 
5. When generated code, you'll find yourself in one of two situations:
 i. You have space for only one instruction here, any more has to be
elsewhere.
 ii. You have space for as many instructions as you need here.


Putting all these things together, there are really four cases depending on
the case for (2) and the case for (5):

  * Space for one instr, need one instr: just write the instruction. Done.
  * Space for one instr, need multiple: write a branch to some target, add
    todo item when you have space available to do the multiple.
  * Space for multiple instrs, need one: just write the instruction. Done.
  * Space for multiple instrs, need multiple: write the multiple.

Pseudocode starting at the top level:
  Add info to todo.
  While more todo:
    F(MULTI, next todo)

Where F(space, info):
  if info needs only one instr:
    Write the instruction and return.
  else if space is single:
    Alloc a new unique id.
    Write branch to new unique id.
    Add todo for multi.
    Return.
  else 
    cmp x0 mid_tag
    b.eq mid_target
    b.lt F(single, low info)
    F(multi, high info)

Which brings out one more piece of the puzzle: for the 'single' case, you have
to provide the label, the branch instruction mnemonic comes from the parent.
    
Maybe split into two separate functions:
  SINGLE - outputs a label, optionally adds to todo.
  MULTI - outputs instructions.
    
In terms of todo list, we need to keep track of a unique id to use for
internal jumps.

It's a lot of info to manage all at once. We're talking about having a queue
and doing recursive function calls. Any way to do only as a queue?

Queue entry has:
 uid, info

Top level:
 Push (NONE, info)

To process:
 output label
 if single target info
   b.eq target; continue
 cmp x0 mid_tag
 b.eq mid_target
 if single target low info
   b.lt target
 else
   alloc id, push low info.
   b.lt id
   push (NONE, high info)

Make it a stack. This should work. All we need is the helper function to find
the single possible target, if any.
   
Or... instead of needing an explicit stack, use function stack?

Top level:
 F(NONE, info)

F(label, info):
 output label
 if single target info
   b.eq target; return;
 cmp x0 mid_tag
 b.eq mid_target
 if single target low info
   b.lt target
   F (NONE, high info)
 else
   b.lt id
   F (NONE, high info)
   F (id, low info)
 
Yeah. Let's go with that. Just need to pass some shared state about intervals,
targets, etc.
 
---

The results:

time fble-sat < rope_0002.shuffled.cnf: 1m36s ==> 1m36s
time yes | head -n 40000 | fble-md5: 53s ==> 54s

Interesting to note that md5 actually does have a pretty big switch statement
in it, of 25 tags at least. Anyway, no major regression, so I'm happy.

