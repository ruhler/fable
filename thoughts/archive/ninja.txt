Switch back to ninja for build system.

Motivations:
* For the fun of it.
* To avoid having to rebuild and retest everything for small changes.
 - Don't rerun spec tests for changes to prgms.
 - Don't rerun all tests for changes to some spec tests, or new spec tests.

The problem is: I'm going to mess up the dependencies, because it's impossible
not to, and it's going to be very frustrating.

Anyway, the build description isn't too bad to start, generating the ninja
file from tcl. Looks pretty much like my original build file.

The big question next is how to do tests. Let's ignore code coverage for now.
I suggest:
* Let's have ninja in charge of running the tests, so we only have to rerun
  tests that have changed instead of always having to run them all or nothing.
* For an individual test, we could either do it all in one go, or split it up
  into paces if we want to reuse, for example, ninja to build intermediate
  artifacts.
* I still want to have a summary of number of tests passed and failed, to help
  double check a new test I ran was actually run.
* I think we don't want build to fail for a test failure, because we want to
  easily see all the tests failing at once, and often I like to check things
  in with test failures. It's fine to fail build for test errors, but probably
  not test failures, if that distinction is clear.

It would be nice to have one common way to describe tests that can be reused
for spec tests and one-off tests. I propose the following interface:

A test is any command that outputs a test result file indicating if the test
passed or failed. The file should include the unique name of the test, whether
it passed or failed, and if it failed, what the error message was.

Maybe encode the test name as the name of the file. Add a '.tr' extension for
'test result'. Contents of the file should be PASSED if the test passes, an
error message otherwise? How about, first line is 'PASSED' or 'FAILED'. The
error message or any other info comes on subsequent lines?

You kind of which there was an easy way to convert the contents to a possibly
empty message and a status code.

How about just: 'PASSED', single line, for success. Anything else is failure.
For command line stuff we could do something like:

  foo x 2> foo.tr && echo 'PASSED' > foo.tr

Let's try it. Then, for a summary, we have a script that is passed the list of
tr files and reads through them looking for failures, summarizing the status
and results, and returning status code 0 or 1 depending.

Alternatively, to make it easy to output stuff when passed, maybe we always
generate two files: .testresult is 'PASSED' or 'FAILED'. .testlog is whatever
output the test produces? Yeah. That's good. Good to separate the status from
the test log, so that we don't accidentally interfere. The script to summarize
takes as input a list of test names? How about a list of .testresult files,
and .testlog is optionally assumed to be adjacent. Reasonable? Let's give it a
shot.

Let's start simple. A build rule to turn a command into a test, two initial
tests: true and false, and a test summarizer.

It's a little annoying to deal with multiple tests. How about log is all
except the last line, last line is status PASSED or FAILED otherwise it's
treated as a test error?

That seems to work fine.

---

Thoughts on testing with and without profiling: it's a little annoying, and
seems redundant to run all the spec tests with and without profiling. It looks
like I originally added with and without profiling to catch a bug that showed
up for spec tests with profiling enabled. Since then, I don't think we've ever
caught a bug in one mode that didn't show up the same in the other mode.

My vote is this:
* Run with profiling on for spec tests.
* Run with profiling off for the Fble tests.

That way we get coverage of profiling in all the various expressions, and
coverage of turning off profiling. We don't want to run profiling on for the
Fble tests anyway because it runs slower.

---

It's a little annoying that we don't get to see error messages from tests as
they are produced. Seems we have to wait until the summary to see them all. I
can probably make that better.

I think there are only two big things before we can switch over entirely to
ninja (with the understanding that I can always rebuild full by removing the
ninja directory and rebuilding if I'm worried about broken dependencies):

1. How to track dependencies for .fble files.
2. How to run code coverage / profiling.

For code coverage, I don't see anything else reasonable available aside from
gcov. gcov will be annoying to integrate into the build system I think. I'm
worried about gcovs ability to support multiple processes trying to update the
database at the same time. And we want to limit the scope of the gcov data to
just the spec tests.

I'm thinking it would be easier to avoid gcov by default. Just have it as a
special mode when needed. So I could run it mostly manually. Maybe we have a
way to build a profiling version of libfble. And then maybe add a script which
is a wrapper around all the spec tests to run and gather code coverage. It can
pass in profiling enabled variants of fble-test and fble-mem-test and reuse
the spec_test.tcl code.

Yeah. Let's do that for coverage:
* Add build rules to generate fble-test.prof and fble-mem-test.prof
  executables. Don't build these by default.
* Add a tcl script to run all of the spec tests using profiling and extract
  the ...?

It's awkward because using code coverage really depends on knowing how things
were compiled. That suggests it would be good to have ninja rules to run gcov
and output the result.

For fble dependencies, maybe we could come up with build rules to generate
some file foo.fble.d that depends on all the .fble files that foo.fble
depends on. We could have the contents of foo.fble.d be a depsfile compatible
thing for ninja to read in. Then we just have to add an implicit dependency on
any target that reads foo.fble: depend on foo.fble.d.

I think we can do one-off things. That is, just generate a .d for each top
level fble program we use. Not for every individual .fble file.

So then I just need to write a program that, given a .fble file and search
path, generates a depsfile for given foo.fble.d that depends on the list of
files loaded for the .fble file. I think add a special function to load.c that
exports this functionality.

Maybe a nice API would be to add an optional pointer to a vector of
FbleStrings listing all of the dependencies. Let me give that a try.

Looks like that will work.

Boy is it annoying not to see the incremental output of my big slow fble tests
test.

---

I cleaned up the build.ninja.tcl file. It looks pretty nice to me now. I think
we're ready to start developing normal fble code with it to see how things go.
There are a few minor issues to resolve, but I think this is a decent step in
the right direction. And the build script is abstract enough that we could
probably replace it with something that runs commands directly if desired.
It's not too tied to ninja.

---

Cleanup for handling of output and tests:
* let's just store PASSED or FAILED in the .tr file, not the test output
* put the test output into the build log instead.
* if we really want the output to be generated and saved, it shouldn't be as
  part of a test, it should be a separate build rule.
* test error messages will go to the build log. No need to save them
  elsewhere.
* tests.tcl just needs to count tests and pass/fail, no need to re-output logs
  from failed tests.

Let's give it a try.

---

One nice benefit of ninja that I didn't anticipate: if I'm working on a
particular piece of code, I can specify a target explicitly to just compile
that rather than have to try compiling everything.

---

I wish I could see the list of all failing tests in the case of memory leaks,
to help identify the source of the leak. But because memory leak turns into a
crash, the build stops before it gets to the test summary.

And it's annoying that once the test has run, we throw away the error messages
and it won't run again because it claims it has already run. Which means I'm
tempted to fail the build for an individual failed test. Then test summary is
just counting the tests and printing them, it can never show failed tests?
