Fbld Improvements
=================

I'm happy with the syntax for fbld. The implementation needs work.

One idea is to try implementing fbld in fble. Partly as an excuse to write
more real world fble programs, partly because it almost feels nicer to use
fble than tcl given things like tracking error locations.

I see two big challenges with using fble for fbld:
1. How to make it easily extensible. Can users write their own front ends and
back ends? Would they have to write fble? Is that too much to ask? How can we
compose with other existing programs like groff?

2. How to deal with bootstrapping dependency of fble on fbld and fbld on fble?
It's a bit annoying, because it's just the help text for fble-stdio. Maybe
have an option to compile fble with and without that usage text and do a mini
bootstrap that way in the build system.

Big things I want to improve for fbld (regardless of what language we use to
implement it):
* Location tracking, so we can point to the exact place in a file where an
  error occurs.

* Composition: so we can easily implement commands directly in terms of other
  commands if we want to, instead of having to convert back into strings and
  re-parsing.

Both those things come down to this challenging aspect of fbld:

The commands have to parse args using fbld, the fbld parser has to execute
commands using the commands. That creates this cycle going back and forth
between the parser and commands where we have to keep track of and pass
location information back and forth. We can't just parse the whole thing all
at once and then execute commands all at once.

---

This should be pretty straight forward I think.

Assume some monad for computation/side effects.

We have generic commands that don't care about the monad. They can be
parametrized by it. The monad lets you call a sequence of commands.

Define fble functions that take typed (post-parsed) arguments.
Define a generic command map that takes non-typed List@<String@> arguments,
parses the arguments appropriately, and forwards that to the typed fble
functions.

The implementation of a command can call either the raw fble function it
wants, or invoke a command via the generic command map.

Typed fble functions for backends can work with specific monads. For example,
maybe we have an HtmlM@ for generating html documents.

We have a block parser and an inline parser that both work solely with generic
monad via the generic command map.

For composition, you create a command map that works for a particular
specialized HtmlM@ monad. You invoke the block parser, which causes all the
commands to be executed recursively in the monad. You can do what you want
with the results of that. Easy.

Note: instead of List@<String@>, we'll want List@<StringL@>, or some variation
that tracks locations for strings. And the block and inline parsers should
operate on StringL@ to maintain file location information.

I think this will be nice, straight forward, very composable. The two issues
I see are:

1. How to get the filename for what we are parsing.
Ideally we can open named files rather than just stdin. Then that's solved. We
want that for fble-md5 too.

2. How to let people write their own extensions to fbld without using fble.
They need some programming language to be flexible enough. For ease of use, it
would be nice to have a programming language with an interpreter. Some options
are: tcl, python, fble, some custom new fbld language. I think, let's not
worry about this now.

---

First step: add support for opening named files. Even before we start on fbld
implementation. We can do this on md5.

Goal is to change the command line interface to md5 by adding an option to
pass one or more file names on the command line (or '-').

Output format is:

<sum>  <filename>

Or, in case of a directory

fble-md5: <name>: Is a directory

Let me see if I can dig up my old thoughts on how to support file names:
* Skip ReadDir to start.
* (String@) { M@<Maybe@<IStream@>>; } Read;

Maybe it's that simple. Just need to implement it.

The return value should be True if all files are found. False if any are not
found.

---

Getting started on fbld, the tricky part is the knot in the middle.
* The parser needs to know how to invoke commands.
* The invoked commands want to call back into the parser.
* Neither parser nor individual command knows what the other commands are.
* You could get an error message at any point while invoking a command or
  running the parser.
* All of parsing and command invocation takes place in some monad M@, which is
  abstract for the parser and some commands, but may be concrete for other
  commands.

Access to the invoke function, possibility of error, and monad M@ are common
between invocation and parser. The string to parse and the state of that is
specific to the parser. I want some monad transformer Fbld@ that stores the
command invocation function and error handling.

Interface from within Fbld@:
  (Loc@, String@) { Fbld@<M@><Unit@>; } error - reports an error.
  (StringL@, List@<StringL@>) { Fbld@<M@><Unit@>; } invoke - invokes a command.
  <@ A@>(M@<A@>) { Fbld@<M@><A@>; } lift - execute an M@.

Is there anything simpler we could do? Like, commands take invoke as their
first argument and return an M@<Maybe@<String@>>? And parser takes invoke as
their first argument? No need for an Fbld@ transformer monad?

Result@ = +(Unit@ ok, String@ err);
Invoke@ = <<@>@ M@>(Invoke@<M@>, StringL@, List@<StringL@>){ M@<Result@>; };

<<@>@ M@>(Invoke@<M@>, StringL@) { M@<Result@>; } Block;
<<@>@ M@>(Invoke@<M@>, StringL@) { M@<Result@>; } Inline;

Cool. Easy.

---

Ugh. Parsing with locations is tedious. Can I make a little Parser@ monad to
help? I want to do things like:

* Get the next character from the input stream, have location advance
  automatically based on the character in question.
* Run a Parser@ computation to get some value at the beginning of the input,
  returning the rest of the input.

(Parser@<A@>, String@) { *(A@ x, String@ tail); } Run

And maybe it can do errors too. And this can be fully separate from M@, right?

Might be nice if we can have a running state though. So:

(Parser@<A@>, S@) { *(A@ x, S@ s); }

Where S@ stores info about the string, the location, errors, etc.

Things to write:
 * Parse a character.
 * Parse plain text before next @... command.
 * Parse to end of matching ']' character.
   - Could result in an error if unmatched.
 * Parse command name.
  
---

I bet I can entirely split the parser off from M@. Write code to parse inline
text into a sequence of commands and block text into a sequence of commands.
No need for M@ at all there.

---

I need to update the parse to pass around the current indent level and allow
parsing of indented strings. Otherwise we'll mess up column numbers when
parsing nested block structured text.

This brings up an interesting question. What should be the location for next
line arg in terms of column number? What if it's an empty line?

Loc@ needs an indent field, which defines what the column at the start of the
next line is. That's a separate issue.

Next line argument should be, for example, column 2. If it's a blank next
line, it should be... doesn't matter I don't think.

I need to come up with a test case to capture locations properly. Maybe, parse
a command. Extract the next line argument. Parse that, verify the locations
are set properly. Yeah.

---

Inline and Block parsing is done. What's next?

Let's review all the current use cases of fbld. I'll want to migrate them over
one by one to fble based fbld, I would think.

* build.lib.tcl - @config, @BuildStamp
* @doc --> html
* @tutorial --> html
* @usage --> man (1)
* @usage --> c header
* doc comment --> man  (3)
  - Extract doc comments from file.
  - Convert doc comments to man.
* Core tags syntax check.
* Core tags html back end.
* Core tags man back end.
* Core tags roff back end.
* @tutorial front end.
* @usage --> help front end.
* @usage --> man front end.
* @usage library

Let's start with Core. Core is an interface. In other words, a struct type
parameterized by abstract monadic type M@.

And we have an instance of Core@ for html, man, text, and optionally 'check'.

We don't currently use 'check' anywhere. So why bother with it?

* Define /Fbld/Core% with Core@, some way to convert typed functions to
  generic fble command invocation.
* Define /Fbld/Core/Html% instance of Core@ on an abstract monad with
  OStream@.
* Try and use fble fbld to generate fbld.html.
 - Will need a solution for @FbleVersion, @BuildStamp

Cool. That sounds like a good next target.

What type do we use for the raw types of functions? Before we said
'post-parsed' arguments.

Anywhere we expect ESCAPED, we should accept /Core/String%.String@;

Anywhere we expect INLINE...
  List@<Command@>?
  M@<Unit@>?

Because we can go:
  String@ --> List@<Command@> --> M@<Unit@>

From fble code, M@<Unit@> is most generic.

String@ --> List@<Command@> is either Inline or Block parse.
List@<Command@> --> Core@.* is either Inline or Block invoke.
Core@.* --> M@ comes from the back end.

Where do we put the logic to know if we should inline or block parse/invoke?

We'll want two different Invoke@ functions: one for inline, one for block. So
we can distinguish between tags depending on whether it's an inline or block
context.

We'll have a function from Core@ to *(Invoke@, Invoke@) that defines how to
sub-parse and sub-invoke each argument.

---

Type inference isn't working:

  <@ A@>(M@<Result@<A@>>)<@ B@>((A@) { M@<Result@<B@>>; }) { M@<Result@<B@>>; }
  DoR = <@ A@>(M@<Result@<A@>> ma)<@ B@>((A@) { M@<Result@<B@>>; } f) {
    Result@<A@> ra <- m.do(ma);
    ra.?(err: m.return(Err<B@>(ra.err)));
    f(ra.ok);
  };

      (Command@ cmd, M@<Result@<Unit@>> mx) {
        Unit@ _ <- DoR(mx);
        inline(inline, block, cmd);
      });

In the application of 'DoR':

../pkgs/fbld/Fbld/Invoke.fble|30 col 24| error: expected type M@<Result@<A@>>, but found M@<Result@<Unit@>>
|| Inferred types:
||   A@: ???

Even if I explicitly give the type of A@ as Unit@. Is this expected?

M@ is fully abstract. There's no way to expand that. Do we know that M@<X@>
equals M@<Y@> if X@ = Y@? Or do we not take advantage of that fact?

Yes. we say poly equal and arg equal.

I should be able to step through this in gdb and trace what's going wrong.
It's just a little tedious to get to the right call in gdb.

Because we should say Result@<A@> equals Result@<Unit@>
Which then should give us A@ equals Unit@
Which should infer things just swell, right? Or, I suppose Result isn't
abstract, so we will say:

+(Unit@ ok, String@ err) versus +(A@ ok, String@ err).

And that should work just fine.

---

Found the issue. I have two different abstract types M@ defined at different
locations. Type error is working as intended aside from not making it clear
the difference between two different types with the same name.

---

How should I implement @FbleVersion and @BuildStamp tags?

The way it's implemented in tcl:
* The build script creates a version.fbld.tcl file to define
  inline_FbleVersion, pulling $::version as the value.
* build.lib.tcl defines inline-BuildStamp which executes the buildstamp
  program whever it is invoked, ensuring we have the latest build stamp
  whenever we run the program.

Let's start with @FbleVersion, which seems slightly simpler.

Options:
A. Define /Fbld/FbleVersion% that hard codes the value of @FbleVersion.
 - Remember to update it any time the version is updated.
B. Define /Fbld/FbleVersion% that reads the raw value from
  /Fbld/FbleVersion/Value%, or some such. Have the build script auto-generate
  that /Fbld/FbleVersion/Value% file as, /Core/String%.Str|'...';
C. Hard code the version, but add a test that it matches what's defined
  elsewhere.

(B) sounds easy enough to me. I can't think of any other sane options. The
only trick with (B) will be figuring out how to add the generated code to the
include path.

Let's think about @Buildstamp next, which is trickier. The goal is to get the
latest value of buildstamp every time you run, e.g. fbld-html-doc.

Maybe that suggests we should read the buildstamp value from a file when
fbld-html-doc starts? Or... should we add an 'exec' option to Stdio@ for
executing other programs?

How do we want generic 'fbld-html-doc' and fble build specific @FbleVersion
and @Buildstamp to work? It's like, how do we add custom tags after the fact?
Should I worry about that for now, or don't care and worry about it another
day?

---

Another challenge: implementing the @code tag. Can we reuse source-highlight
from within fble? How?

I think we need exec support to be able to compose with other programs. That
will let us reuse source-highlight (assuming we can find the path to it in some reasonable way?) and buildstamp (again, assuming we can find the path to it).

I'm thinking, for exec, also have a 'fifo' function to return IStream/OStream
pairs, then pass stdin and stdout streams as an argument to exec along with
command line arguments. It forks off the process to execute in the background.
Maybe we return an M@<Int@> to be able to block on and fetch the exit status of
the process.

Another approach would be to re-implement source-highlight equivalent in fble.
Sounds like an interesting program to try and write. The trouble is, there are
a lot of languages to have to support.

---

I've finished fleshing out the core tags. Going to have to decide what's next
to work on. A few options:

* Add a markdown backend so we can generate README.md.
* Implement mapping from foo.fbld to foo.html for @fbld html backend.
* Figure out how to specify libraries like spec/fble.fbld.tcl
  Where to put the .fble file?
  Do we want a separate binary for this other than fbld-html-doc?
* Figure out how to implement @FbleVersion
* Figure out how to implement @BuildStamp
* Figure out how to do source highlighting for @code

---

We want users to be able to supply their own front ends and own back ends, and
have different front ends and back ends be combined together.

I claim this cannot be done today in fble with a compiled binary.

To pass a new front end or back end to an existing compiled binary, you need
some way to convert the .fble code into its corresponding fble value at
runtime, dynamically. Options:
* Put the .fble code into the compiled binary, pass a name as a reference to
  the code. But this doesn't allow new code, it only allows code in the
  binary.
* Come up with a different syntax for implementing tags that can be parsed
  and loaded from fble. But then we are not using fble to implement tags.
* Implement a parser for fble that can turn it into a value at runtime. But...
  we would need an interpreter in fble?

The point is, unless we have some magic way to take a String describing a
module path or .fble filename and dynamically load it, we can't pass new fble
modules on the command line as arguments.

The other route is for the user to provide the main function. Then they can
pass whatever code they want. They can run the interpreter or compile it if
they so choose.

---

What is a front end? It would be nice to have a specific type for frontends so
you can easily combine them.

Front ends are built on top of Core@, and possibly other front ends. There are
two ways we may want to use a front end:
1. Directly.
2. As markup.

To use a front end directly requires knowing the specific type of the front
end. To use it as markup just needs generic Markup@ for Inline and Block.

Core@ is special because it is an interface, not an implementation. Other
markups can pass around their implementation directly as functions to be
called.

So, I expect a front end is an fble module that takes Core@ as an argument and
defines a function for each tag supported by the front end, and Inline and
Block Markup@.

Having modules be functions is starting to be problematic. Consider a front
end B that depends on front end A, and both depend on core.

B wants to access A directly. We'll end up invoking the A module function
twice: once when defining B, and once at the top level. That does not scale
well.

An alternative is to provide A as an argument to B. But that is like exposing
internal implementation details. Whoever is using B shouldn't have to care
about whether B uses A internally. They do need to care that B uses Core
internally, because the user has to provide the backend implementation of
Core.

Well, it all happens during setup, and these functions aren't called that
much, so maybe don't worry about it for now.

A related thing is having false dependencies. For example, Core module takes
M@ and Monad@<M@>. It exports Core@, which doesn't depend on Monad@<M@>.
That's a false dependency introduced. If the user only uses Core@, it
shouldn't need to provide Monad@<M@>.

We could address this by splitting up Core into the type and the
implementation. Either as separate modules, or within the same module as, e.g.

@(Core@, (Monad@<M@> m) { ... @(InlineMarkup, BlockMarkup); } Impl);

Again, let's not worry about it for the time being.

Next issue: it's slightly annoying to pass around everything as inline or
block. Can we unify somehow? Ideas:

Don't distinguish between inline and block markup. Use a single markup for
everything. This means you can't have a tag interpreted differently for inline
versus block markup. The only place we do this right now is the default inline
and block tags, both called "". But we can give those separate names if we
want, like "_inline@" and "_block@".

That way you could use any markup in either context. Umm... that's a little
concerning. What does it mean to use section@ in an inline context? And @l in
a block context? For it to make sense, we would have to do different
processing for tags in each. No, I prefer to type a tag based on inline or
block context.

The other idea is define a type Markups@, or some such, that has two fields,
inline, block. Then we can more easily name, pass around, and combine things.
We could do the same for Invoke too perhaps. invoke.inline and invoke.block.

---

Say we want to pass around a single invoke function instead of two. How would
we represent that?

@ Context@ = +(Unit@ inline, Unit@ block);
@ I@ = (I@, Context@, Command@) { M@<Result@<Unit@>>; }

Invoke.Inline becomes: invoke(i, Inline, cmd);
Invoke.Block becomes: invoke(i, Block, cmd);

Now markup can become an abstract type where you can insert entries as Inline
or Block.

The reason this is better than what we have today is because we can pass
around a single Markup@ object that stores both inline and block tags. I think
it's worth trying.

Does it make sense to do other cleanup while I'm at it?
* Separate different types out of /Fbld/Types%?
* Make a better monad transformer for M@<Result@<T@>>?
* Get rid of R@ helper type.
* Use a name other than String@ for Fbld String@, so we can using String@ for
  /Core/String%?
  Like, StringL@. That's not bad. Let's do that.

---

I'm thinking we want two different types. I'm not sure what to call them:
1. A function that takes global context and a command and executes the
command.
2. A global context, which is a pair of invocation functions, one for inline,
one for block.

Brainstorm of names:
* Invoke@, Context@.

Anything better than Context@ for (2)?
* Env@ is better.

So, what do define where?
* Command@
* Invoke@, Env@
* Invoke.Inline, Invoke.Block - that parse and execute structured text.
* Is Markup@ the right term to use? Or would that more often refer to the text
  itself rather than the tag definitions?

My vote:
* Command@ should be separate from Invoke@, because Parse doesn't need
  Invoke@.
* Invoke@ and Env@ must be defined together, because they depend on each
  other. I think Invoke% is fine for this.
* Maybe Interpret.Inline, Interpret.Block? Exec? Eval? Impl? Run?
  I like Exec or Run. Let's say Run, because it is a complete word.
* "markup" means the marked up text, not the tags. What other name could we
  use? Markup Language? Language?
  - This brings an interesting idea: should StringL@ be renamed to Markup@?
    No. I don't think so.
  How about Schema@? Tags@? Honestly, I like Markup@... Maybe think of it as
  the definition of what markup elements can be used in the doc. Yeah. Let's
  go with that.

We have our answers. Get to work.

---

The markdown backend makes things difficult:
* You can only define labels on section headers?
* It just feels like a ton of work to do properly.

Is it worth trying to do properly for real? In case it reveals issues with the
general structure of how fbld is implemented? Like, I fear I'll need to track
some other internal state for text wrapping and indentation and nested lists.

Anyway, the generated README.md looks fine to me except for @FbleVersion and
@BuildStamp not being implemented yet. Maybe let's focus on those first. If I
can generate README.md, then set up the template to have it checked in and
tested against the generated version, that will be great.

Actually, I should remove @BuildStamp, because we don't want it changing every
time. I don't think @BuildStamp is so important for that. That means we just
need to figure out @FbleVersion.

---

@FbleVersion is taken care of now. Let's revisit @BuildStamp.

@BuildStamp is intended to capture the version of the documentation. If we
left it in, we want it to execute buildstamp at the time the document is
produced, not at the time we build fbld-html-doc or whatever.

There are two ways to do that:
1. Pass it as a command line argument when we run.
2. Have fbld-html-doc be able to execute buildstamp at runtime.

For README.md, I'm thinking of checking it into source code. It's pretty
annoying if we keep modifying it for every buildstamp.

The fact is, we don't expect README.md to change for every git sha.

Could we update buildstamp and only check it in when README.md changes
otherwise? That way the buildstamp would be accurate, we would have a build
stamp.

On the other hand, buildstamp of index.html kind of represents the buildstamp
of the entire set of documentation, which certainly could have changed. And I
like having the build stamp in the website documentation. It's really nice
that it has the date along with the documentation.

What if, instead of saying README.md is a source file and we check it against
a generated file, instead we generate README.md from ninja directly to source?

But that means the build process dirties the source, which could mess up
buildstamp for other docs.

I could be really hacky and not implement buildstamp especially for markdown
backend. Because README.md is only expected to be read from github when you
can easily check what the git sha is.

I'm going to want to implement @BuildStamp eventually. How about this:
* Pass @BuildStamp as an argument to the main function. We can easily call it
  as part of the build command. For README.md we can pass "" as the build
  stamp, or HEAD, or something like that.

Sounds good.

---

Looks like my @FbleVersion implementation is breaking ninja. Ninja is going
into a loop and gives up after 100 tries. For the time being, let's use the
same approach for @FbleVersion we use for @Buildstamp, passing it on the
command line.

---

The biggest blocker for using fble based fbld right now is @code syntax
highlighting. Specifically for 'fble' and 'fbld' languages. If I could support
those two languages, that would be enough. Aside from that, txt, tcl, sh, not
so important I don't think.

Both fble and fbld should be pretty straight forward to do syntax highlighting
for. How would that look?

We want to separate two parts:
1. text to highlight regions (backend independent)
2. implementation of highlight regions (backend dependent)

We can use fbld markup to annotate the highlight regions in a backend
independent way. So the syntax highlighter could convert StringL@ to MRU@,
where that MRU@ is inline text.

We would want the following kind of markup:

@comment[INLINE]
@label[INLINE]
@string[INLINE]
@type[INLINE]
@symbol[INLINE]
@special[INLINE]
@include[INLINE]
@identifier[INLINE]

Maybe we want an intermediate implementation that maps all those to colors,
and then we just need:

@highlight[style][INLINE text]

Where style is color, or maybe we want bold, underline, italic, etc.

Note: this brings up an issue that's been in the back of my mind for fbld:
namespace control. How do we make sure people don't pick conflicting tag
names? For example, @label means something else to Core fbld.

Anyway, the part to figure out is how to convert text to marked up highlighted
text.

Start with fbld:
* @[a-zA-Z0-9_]\+ for a tag
* [, ] for bracket
* \[, \], \@, \\ for escape

No need for regex for that. We can code a lexer very easily. In fact, now that
you mention it, we could code a full parser if we wanted. I'm not sure it
makes a difference in this case. Maybe for fble.

How about fble? I don't think it would be too hard.

Let's start with fbld, implement it, wire it up, see how it looks.

Maybe we use a single tag, like

@hi[class][INLINE text]

---

Working on tutorial front end. I need to think more about how the helper
functions should work and the fble functions.

For example: BuildStamp function. No arguments. But it wants a Loc@ perhaps to
form a location for whatever text it calls. But that depends on the
implementation of BuildStamp.

Consider tutorial's Tutorial function. This wants to reuse BuildStamp and
Version. Should it take those as arguments, or take Env@ assuming those?

Take A2. Some functions might want access to the command location. Most I
would say. What about Env@? It needs that because some wrapper functions want
to convert structured text.

---

Possible next steps:
* Work on syntax highlighting for 'fbld' language.
* Add usage, usage.help, and usage.man front ends.
* Doc Comments frontend.
* Add man backend.
* Add txt backend.

---

For syntax highlighting, I vote we start simple.

Interface is String@ to List@<*(Class@ class, String@ text)>. Assume to start
that we don't have nested highlighting items. Start with fbld, then go from
there.

---

The man backend doesn't implement @doc, it replaces it with a different typed
@man tag. How am I supposed to handle that? I takes extra 'section' and
'source' arguments.

Maybe we implement @doc, but give an error message saying man backend does not
support @doc tag, use @man tag instead? And export both Core and Man top
level values from the backend?

The implementation of fbld in fble is feeling pretty messy right now. Things
like that, and:
* Having to write lots of invoke utility functions.
* Overheads of passing M@ and m everywhere.
* Inability to do dynamic front ends or back ends.
* Not knowing when to pass Loc@ to functions.

---

Let's imagine a different world for fbld.

Most front end tags could be described as simple transformations. For example:

@define[AbstractSyntax][content]
 @definition[Abstract Syntax][@code[txt][@content]]

Okay, so that one doesn't work great, because we wouldn't substitute @content
inside an @code block. Unless it's a special thing for @define.

Try again:

@define[tutorial][name][content]
 @doc[@name]
   @FbleVersion (@BuildStamp)

  @content

That doesn't say how we would implement @FbleVersion or @BuildStamp. Maybe
define those with auto-generated code:

@define[FbleVersion][fble-0.2]
@define[BuildStamp][asdfasefase]

Assume we come up with a reasonable approach for this, now we have an easy way
to define new front ends dynamically. You just write @defines into your docs.
And maybe have an @include too, to help arrange things.

Imagine we have sufficient type information about each tag that we can know
how many arguments it expects, and whether those arguments are 'RAW',
'INLINE', or 'BLOCK' structured text. Then we could parse the entire document
in memory as something like:

@ Fble@ = +(StringL@ raw, Command@ command, List@<Fble@> list),
@ Command@ = *(StringL@ name, List@<Fble@> args);

Front end tags are reductions we can apply. Apply all possible reductions.

At this point, hopefully you are left with only tags that your backend knows
how to handle. Pass it off to the backend. We could enumerate all the tags
left over and call that the overall 'type' of the document, and find a backend
that supports that type. Or just have the backend fail if it sees a tag it
doesn't expect.

There are a few corner cases to double check:
* @fbld tag and similar which can take one or two arguments.
  Have support for optional or default arguments?
  Or come up with different names for the different overloadings of the tags
  like this? @i versus @ii, for example. @fbld[...] versus
  @link[fbld][label][...]? Or @fbld[...][...] versus @fbld[...][]?
* usage man page front end generates @man back end tag instead of @doc.
  I don't see any problems here.
* doc comment processor processes tags twice with different interpretation.
  It's not a problem if it's backend, because we can do custom processing
  there. Maybe we want a way to do more custom front end tag processing
  though.

To simplify the problem to start, let's imagine front end tags can be defined
different ways. Via @define. Via custom fble code. Via custom tcl code. Via
custom process. Whatever.

Could we start by just revamping the current implementation of fbld in fble,
without changing the fbld spec? Instead of passing monads around everywhere,
pass around the 'value' of the doc?

We have two ways of processing a document:
1. Front end: apply reductions until you can't anymore.
No monads required. Stays in Fbld@ -> Fble@ land.

2. Back end: convert to backend specific types.
Maybe we want a monad. Maybe more specific typing?

Alternatively, if we think of a backend as always producing text of some form,
and we say transformations can go Fbld@ to raw String@, then backend need not
be special?

All my current backends are straight up string translations. I've structured
them so they don't need any intermediate state. I don't know if that's what we
can expect for things like markdown or txt, where we want text wrapping and
need to know state of nesting and other things like that.

What I'm saying is, it would be nice to unify front end and back end
transformations. But that gets you back, I think, towards where we are today.

Let's explore more. Say a document is one of:

1. Raw string: there is nothing you can do to simplify further.
2. Command: You can convert the command to another document. 
3. List of ???: 
  You can keep as a list, or concatenate into a raw string?

---

After much thought, I've ended up pretty close to where we are today. Some
minor differences:

* Maybe we want to add implicit @[] commands for block and inline text that
  wraps the list of comments in a sequence.
* We should pass OStream to all parts of commands, and allow subinvocations to
  provide alternative OStream implementation. This will allow us, for example,
  to reflow or trim text produced by subcommands.
* Consider defining a Markup backend that has a tag like
  @define[command][arg1][arg2]...[body]
  As a syntax for describing user defined commands. Not as part of the doc
  itself, but as an input to the fbld processor to describe markup to add to
  the processor.
* Maybe separate OStream and CharOutputStream, rather than assume we always
  want to write to byte streams instead of directly to char streams.

This way a front end and back end are kind of the same thing. We could
implement a backend as a direct string translation just like front end.

If we had the ability for tags to modify the environment, we could potentially
define things like @define to use within docs, but don't worry about that for
now. Having an @define syntax for describing markup would allow us to have
dynamic front ends and back ends. It's limited to whatever we provide for
string manipulations in the language. You could imagine a full programming
language, or something tcl like that lets you do full string manipulation. Or,
for the time being, just allow simple substitutions of text.

Let's keep forging ahead as is. Keep in mind the above changes, but wait until
there's a clear use case to make those changes.

---

I changed Core.doc to take a Loc@, which is needed for the man backend to
report the error message for that. This suggests every Core method should take
a loc.

If every method takes a Loc@, what is the value of having Core@ type? Why not
just construct a command directly by name?

The idea was calling directly would give better type info: number and type of
args. But type of args is just StringL@ or MRU@. It doesn't give that much
info.

I suppose we like being able to pass a monadic computation as an MRU@
argument.

Imagine if we referenced, e.g. Core@ only from Env@. Is that better or worse?
No need to depend on Core, but on the other hand, easier to make type errors
in terms of typos or wrong number/type of arguments.

A third option: we could provide an abstract instance of Core@ that takes from
the environment instead of being the ultimate backend?

---

On further thought, we don't want front ends to be able to write strings
directly to the output stream, right? We only want them to write things via
Text backend tags so that backends have full control over what's being written
to the output stream. ?

---

For usage.lib front end, I implemented it as direct fbld text parsing. Some
interesting things came out:
* No need to depend on Core or other things within fble code, which is nice.
* Ran into @config missing error, which is the other side of the coin for
  that.
* The location tracking doesn't work, because I'm giving the location for
  where the tag is called, not the location for where the tag is being
  defined.

---

How to handle @config?

Even if I had a way to read tags using @define, that doesn't give me a way to
implement @config, which wants to switch on the argument value.

It's getting a little old to pass everything like this on the command line,
but I suppose that would be the natural continuation. Pass a config.txt or
some such file to fble-fbld, it can read it and define the @config tag that
way.

Options are:
A. Pass config.tcl or equivalent to fble-fbld.
B. Figure out build issue to support generated fble source files.
C. Come up with a sophisticated enough language for defining tags in fbld that
we can generate and describe the @config tag that way.

For the fun of it, let me spend some time thinking about (C).

---

Say we want a full fledged fbld based programming language. Sufficient to
define almost any tag you would want. It would be something like tcl: string
based arguments. What do we need to support?

* Variable definitions.
* Tag definitions.
* Conditionals
 - Presumably based on string comparison.
* String operations
 - Concatenation, search/substring to support structures.
* Loops and/or recursion.
 - Integer arithmetic to support loops and/or recursion?

Say @foo[...] returns a string, and can modify the environment of
tags/variables. The top level document computes the value of the top level tag
and returns it. A sequence of tags results in a concatenation operation (maybe
some implicit tag we can redefine).

We need some primitive tags to support lets, conditions, string operations,
etc. We probably need namespace support in practice: allow some special
characters in tag names. An @include tag would likely be useful to organize
code into different files.

The overall vision here is to define a programming language, instead of a
markup language. Using the syntax of fbld. You run that program to convert the
document.

It's not bad. I don't see any major downsides. I think the syntax should work
fine for a command/string-based programming language.

A variable is the same as a zero-argument command.

Function definition:

@define[name][arg1][arg2]...[body]

Adds a tag @name to scope.

Order of evaluation? Do we pass the arguments into the body first before
evaluating, or evaluate first? Or do we just define a tag that's visible in
the body? This matters for things like:

@define[ConcreteSyntax][content]
 @definition[Concrete Syntax]
  @code[txt][@content]

1. If we just define a tag inside the body, @content will never be expanded.
2. If we inline the contents before evaluation, then nothing inside the
   content will be expanded.
3. If we evaluate the contents before inlining, then all the contents will be
   expanded.

What if we have argument types:
* raw: arg not evaluated before inlining.
* inline: arg parsed as inline structure before inlining.
* block: arg parsed as block structure before inlining.

If the second argument to code is raw, how can we possibly define substitution
in there?

What if the result of inline or block structured text is structured text?
Somehow that sounds not reasonable. Like the result should always be in raw
text form.

Let's execute:

    @ConcreteSyntax[@foo[bar]]

Do a substitution:

 @definition[Concrete Syntax]
  @code[txt][@foo[bar]]

Evaluate the body (still as part of @define/application)

We cannot execute the argument before knowing how to interpret it, either as
raw or structured text.

Okay. So what trick could we use to have @foo[bar] expanded before becoming
code?

Tcl distinguishes between {...} and "...". Maybe that's the idea: when passing
an argument, where you do application, you can specify if you want to evaluate
the argument first as block, inline, or raw text. That probably needs a
language syntax change, but it could be useful. For example, maybe use [] for
raw, [[]] for inline, [[[]]] for block? Now you have full control?

Or, maybe we have a separate tag for that. Like:

@apply[@code[txt]][@foo[bar]]

Turns into: @code[txt][expanded_foo_bar]

No new syntax needed. But maybe a bit clunky to specify what we want.

---

What would variable scoping look like? Because if we don't interpret an
argument until it's expand into a function, then we need the caller's scope to
be visible from that function.

We have three different expression syntax:
1. raw string.
2. inline structure.
3. block structure.

What we really want is a way to specify which syntax you are using when you
pass an argument. We've already seen examples where, for the same tag, you may
want different syntax used.

In the current fble, the syntax to use is implicit, based on the command it's
being passed to. Implicit is nice, because it doesn't clutter the source text.

How do we reconcile implicit nice versus wanting more control? We saw one
approach above: @apply. Maybe there are different variants on it we could use.

In practice, do I ever put block structured text in an argument that is not
a next line argument? Well, I put raw text in next line argument.

Could we distinguish between different ways of invoking commands? For example,
we have @foo, which passes a command, which will be interpreted wherever it
finally lands. What if we had $foo, which says you interpret the command
before passing it down?

Let's go back to the example in question:

 @ConcreteSyntax[@foo[bar]]

 @definition[Concrete Syntax]
  @code[txt][@foo[bar]]

Assume second arg of @code expects raw text.
Assume arg of @ConcreteSyntax expects raw text. Then it works fine, but we
have a new problem.

How can we pass substituted text where raw text is expected?

Simplify the example:

  @l[@foo]

In one case, I want this to refer to the literal string "@foo". In another, I
want this to refer to the result of interpreting @foo as inline text.

  @inline[@foo][@l[$1]]
  @let[x][@foo][@l[$x]]
  @inline[x=@foo][@l[$x]]
  @l[$foo]
 
  @let[x][@inline[@foo]][@l[$x]]

  @l[:@foo]
  @l[[@foo]]


  @a[...][...] ...
   ...

By default args are raw text. We want some way to override individual args
separately. Some syntax we can sneak in there.

  @a[$ ...][$ ...] $ ...
   $
   ...

Idea here: if the text of the argument starts with $<space>, it is first
interpreted as inline structured text. Maybe !<space> for block structured, or
something.

Now, if we do this, commands are no longer side effecting things, they are
value returning things? Otherwise it's not clear where the side effects of
those commands should take place.

  @inline[x][@foo][@l[$x]]
  @block[x][@foo][@l[$x]]

  @inline[cfg][@config[hello]][@l[$cfg]]

I feel like this kind of thing is the cleanest approach. Not very readable,
but get's the job done, no special syntax required. Arguments continue to be
interpreted by the command running them.

If we wanted to, we could factor this out:

 @define[ll][text][@inline[x][@text][@l[$x]]]

And now use: @ll[@foo] or @l[@foo] depending on which we want.
I guess, similarly, we could do:

 @code[...][RAW ...]

or @code_i[...][INLINE ...]
or @code_b[...][BLOCK ...]

Define wrapper functions like this. Less flexible, but entirely capable.

---

Taking a step back to summarize:
* We have syntax for block and inline structured text.
* I'm very happy with writing docs in this syntax.
* The current question is how to implement tags.
* Proposal is to reuse the same syntax for writing docs as defining tags.
* We've established that arguments are not parsed before passing them to
  commands. Commands interpret those arguments as they see fit.
* Next step is to come up with a collection of primitives that let us define
  the kinds of commands we want.
* Two classes to consider: front ends, back ends.

If I want to go this route, a reasonable next step would be to sketch the
implementation of all our current front ends. That will help iron out details.
After that, sketch the implementation of a back end like html, to see what
that entails.

Top level user flow: let's assume you pass a list of .fbld files, which are
conceptually concatenated together in order and read as block structured text.
That way we can mix and match back ends with documents. For example, you might
have:

  fbld html.fbld spec.lib.fbld spec.fbld

We could have spec.fbld directly import spec.lib.fbld. That makes sense. But
it doesn't make sense for spec.fbld to directly import html.fbld, because we
may want different backends.

---

fble.fbld.tcl:

@define[AbstractSyntax][content]
 @definition[Abstract Syntax]
  @code[txt][$content]

@define[ConcreteSyntax][content]
 @definition[Concrete Syntax]
  @code[txt][$content]

@define[Example][content]
 @definition[Example]
  @code[fble][$content]

This requires our first primitive, @define.
  
  @define[name][arg1][arg2]...[body]

Defines a new command @name that expects the given number of arguments.

Execution of that new command substitutes arg values into the body based on
$name, then executes the result as block structured text.

Questions:
* This defines a block command. How do we define an inline command? Presumably
  have different names. Maybe @define_b, @define_i, or something weird like
  that. It's not too important right now.
* What if we want to use $... somewhere in the body without matching an arg?
  - Pick a different name for the arg so it doesn't conflict.
  - Say only matched $name are substituted, others are left as is?

---

tutorial.tcl:

@define[tutorial][name][content]
 @doc[$name]
   @FbleVersion (@Buildstamp)

   $content

@define[exercise][content]
 $content
  
Works great. Nothing more needed.

---

usage.lib.tcl:

@define[GenericProgramInfo]
 @subsection[Generic Program Information]
  @opt[@l[-h], @l[--help]]
   display this help text and exit

  @opt[@l[-v], @l[--version]]
   display version information and exit

@define[ModuleInput]
 @subsection[Module Input]
  @opt[@l[-I] @a[DIR]]
   add @a[DIR] to the module search path

  @opt[@l[-p], @l[--package] @a[PACKAGE]]
   add @a[PACKAGE] to the module search path

  @opt[@l[-m], @l[--module] @a[MODULE_PATH]]
   the module path of the input module

  Packages are searched for in the package search path specified by the
  @l[FBLE_PACKAGE_PATH] environment variable followed by the system default
  package search path. Modules are searched for in the module search path.

  For example, if @l[FBLE_PACKAGE_PATH] is @l[/fble/pkgs] and you provide
  command line options @l[-p core -m /Core/Unit%], the @l[-p core] option adds
  @l[/fble/pkgs/core] to the module search path, and the @l[-m /Core/Unit%]
  option will look for the module at @l[/fble/pkgs/core/Core/Unit.fble].

  The system default package search path is @config[datadir]/fble.


Works great.

---

How to define interfaces? Don't worry about it. Programs are expected to be
short and fast. It's fine to give an error 'tag not defined' in that case. Or
define a null backend that supports the tags you expect to have and check the
doc that way.

---

@define_i[FbleVersion]
 fble-0.2

So yes, we want different ways to define for block and inline commands.

I think I want inline define to be a block tag, not an inline tag. Any better
name ideas?

@defblock[...]
@definline[...]

@defb[...]
@defi[...]

Yeah, I like that. Unlikely to conflict with a user chosen tag, explicit about
block versus inline.

---

usagel.help.tcl

@defb[usage][name][brief][content]
 @doc[$name][$name - $brief]
  $content

How do we 'trim' $brief? Why is that there right now?

We pass this as a same line argument to a block command. There should be no
need to trim that. But, let's imagine. For example, imagine we want to convert
the brief to uppercase? How would we do it? And imagine we have an inline
command @upper.

@defb[usage][name][brief][content]
 @doc[$name][$name - @upper[$brief]]
  $content

That's not hard.

---

build.lib.tcl

Here's an interesting one. @config[name] is supposed to return the value. How
would we define that?

@defi[config][name]
 @if[@eq[bindir][$name]][/usr/local/bin]

But that doesn't scale up to a bunch of options. It would be one, huge, long
line? Well, we could sneak some newlines in, right?
 @if[@eq[bindir][$name]
   ][/usr/local/bin][
 @if[@eq[mandir][$name]
  ...

That's a little tedious. If we use nested if like this, we have lots of braces
to close at the end. How could we do elseif?

The way tcl does it is accept multiple var args to if:

 @if[a
   ][va][
  elseif][b
   ][vb][
  elseif][c
   ][vc][
  else
   ][???]

The important thing here is that I'm putting newlines in the conditions and
places where we could ignore whitespace, not in the actual values themselves.

Could we somehow define a list of pairs instead and implement a function to
search that list?

Or, maybe we have a command that takes block structured text, evaluates it,
and then interprets it as inline structured text? But @defi already does that.
So we should be able to say:

 @if[a] va
  @if[b] vb
   @if[c] vc
    ???

No worry about braces, just a bunch of indent.

Or maybe we define, like, an @concat command that turns block structured text
into inline text?

 @concat
  @if[a]
   va
  @if[b]
   vb
  @if[c]
   vc
 
Which works if a, b, and c are mutually exclusive, but doesn't give us a final
else branch.

If we had primitives for working with dictionaries, we could do:
  @defi[configs]
   @dict[bindir][/usr/local/bin][mandir][...]
  @lookup[@configs][$name]
  
But, again, this annoying thing of not being able to split up a single inline
command easily across multiple lines. If we had that ability, it would be
great.

---

I want to start with implementing @defb and @defi, and use it to replace all
of the current front ends. Let's go incrementally.

1. Implement a defb/defi helper functions in fble. It takes a list of argument
names and a string body, and returns an Invoke function capable of doing the
desired substitution. We can use this to replace the implementation of all our
front end markup.

2. Once switched over to defb/defi helper functions, we should no longer need
to pass around interfaces like Core@, Usage@, Man@. Everything relies on the
environment to invoke sub commands. So we can clean up the code based on that.

3. I'd like to transition away from abstract monad M@, and towards concrete
String@ result. That way we can define a concrete monadic type that allows
state update to Env@.

4. Add defb/defi markup commands.

5. Move frontends from .fble to .fbld.

We can worry about the rest later. That will be a good start.

Remember: defi and defb are both block commands, but defi interprets the
result as inline and defb interprets the result as block.

---

One thing that's tricky is keeping track of locations after substitution.
Should we not worry about it for now? Make up a useless location?

---

One challenge with defb/defi: when we substitute into a block context, we may
need to indent. For example:

@code
 $content

If $content spans multiple lines, the appropriate way to substitute in would
be to make sure all those lines are indented.

How do we know when it's safe to do that though? Because if we wrote instead

@code[$content]

Then we shouldn't be indenting all the lines. Maybe we ask the user to provide
that by passing '$$' for indent substitution and '$' for direct substitution?

A better example might be:

@code[
 $content
]

Do we want to indent here? I actually don't know.

Let's ask the user to do it for now. That's not to onerous I don't think.

---

A little bit of weirdness about $ versus $$: you can't use $ if it has
multiple lines and you have indented at all. For example:

@foo
 @code[txt][$content]

Does not work, because it wraps to column 0 instead of column 1, so part of
the content is read as the command after @foo instead of the body of @foo.

I hope we don't get into situations where neither of $ nor $$ are good enough.
Maybe we could always indent by whatever amount of whitespace is on that same
line? Let's see what issues we run into first and consider cleaning this up
later.

---

I would really like to pull @defi and @defb out of fble into fbld directly.

To do that requires we make the environment modifiable.

Which means I want to define an Fbld@ monad and get off of this abstract M@
monad.

The Fbld@ monad should have the following capabilities:
* Return error messages.
* Return computed String@ values. Except I think we'll want some smarter
  string type that supports fast append, concat, and location tracking.
* Access and update the environment.

Invoke@ would then become: (Command@) { Fbld@<FbldString@>; }

Let me define the better string type first. We can start with a direct wrapper
around String@ and improve efficiency later. This should replace StringL@. I
wish I had a better name.

Once we have that, do that major shift to new Fbld@ monadic type. Then we can
implement @defi and @defb commands, and pull out all of the frontends and
backends, hopefully.

---

I have defined a Text@ type. There's plenty of work to do to clean that up,
but can we move on to an Fbld@ monad?

It means having all of the documents loading into memory at the same time, but
we kind of do that currently anyway.

Let's give it a try. I'm hoping this will simplify the code a lot.

Fbld@ is a function from Env@ to Result@<(Env@, A@)>.
Env@ is a pair of Invoke@.
Invoke@ is a function from Env@, Command@ to Result@<(Env@, Text@)>
 aka. A function from Command@ to Fbld@<Text@>;

Interesting question: can we actually define this type in fble? I'm pretty
sure yes, but we might have to start by defining Fbld@<Text@> as a separate
type.

---

Okay, so made the switch to a concrete Fbld@<Text@> monad. It was a big
change. I haven't bothered about location tracking yet. The code takes a long
time to compile. This should put is in a good place to define an @defi, @defb
commands and start pulling things out of the implementation and into separate
.fbld files.

---

In order to support dynamic insertion of commands into the environment, can we
have the environment be Markup@? Is there any reason we wouldn't want to use a
map?

One reason is because that would make Fbld@ even more tied up into a knot.
Maybe I can not worry about it for now. Just do a linear chain search.

---

Time to see if it works out. Let's remove one of the easier front ends from
fble, port it to fbld. Let's say tutorial? Except, that doesn't seem to work
at the moment, and I don't have any location info to help. Hmm...

README works. Can we start with that? Or should I just do some manual things?

With a simple manual test... it totally works. That's cool.

---

Indent issue with defb:

@defb[synopsis][text]
 @par[Usage: $text]

Will that work? Maybe?

I think that's okay. It's the nested version that wouldn't work:

@defb[...][text]
 @foo
  @par[Usage: $text]

---

Looking into performance of fbld. It takes a couple minutes to convert
fble.fbld to fble.html. It should take only a couple of seconds.

Profiling shows, as expected, most of the time is in Text Append. In this
case, mostly from calling HtmlText.

Let's update Text@ to support fast append.

---

Next step is to figure out @config tag.

I want a switch. How about something like:

@defb[config][arg]
 @switch[$arg]
  @case[bindir] /usr/local/bin
  @case[builddir] .
  @case[datadir] /usr/local/share
  ...
  @default
   @error Unknown Config Option: $arg
   
That's a very nice syntax for string switch. It suggests the body of @switch
is interpreted as block structured text with @case and @default tags defined
and nothing else. Anything wrong with that?

It uses a different 'concat' operator. Instead of Concat, it takes the first
nonempty string value. Importantly, the @default is only taken if no other
branch is taken.

I say we go for it.

Can we do this as pure string manipulation? All except @default handling,
where it needs some context about what happened before to know if it should be
included or not.

How about as one long @switch command?

 @switch[$arg]
  [bindir] /usr/local/bin
  [builddir] .
  [datadir] /usr/local/share
  [@error Unknown Config Option: $arg]

Here I'm using a new syntax that says we can start arg sequences by using [ at
the start of the next line. Maybe we want \ character or something? Not sure.

I like the version with @case much better from a read/write point of view.

Imagine we had a tag @first that takes block structured text and returns the
first non-empty element. And we had an @ifeq primitive:
  @ifeq[a][b][body]

It returns body if a and b are equal, empty otherwise. Then we could write:

@defi[config][arg]
 @first
  @ifeq[$arg][bindir] /usr/local/bin
  @ifeq[$arg][builddir] .
  ...
  @error Unknown config option: $arg

That's a simpler set of primitives, don't you think? If I knew how to
implement it under the hood.

@error is easy to implement.
@ifeq is easy to implement.

@first is difficult, especially if @error has an immediate side effect instead
of just returning text.

You almost want Run.Block and Run.Invoke to return List@<Fbld@<Text@>>. Then
we could handle things pretty easily. Go one at a time until we get non-empty
(or error).

---

Let's go back to the notion of a command having side effects.  Say it has side
effects and returns a string value. The challenge with if/else is mainly
syntactic. It doesn't naturally fit into 'list of commands' syntax because it
has two separate bodies.

We saw in fble how to get around that: make the next thing after the if be the
else branch. That's if you are using more of a functional style. The other
approach would be to allow commands to influence control flow: @return, for
example.

Because everything is a string, and I want to write the string directly
instead of writing @return[...], let's go with the functional approach. How
would this look? Can it be meaningfully specified?

@if[p] then body
 else body

Would lead to:
@if[p] blah
 @if[q] blah
  @if[z] blah
   @blah

As we've already mentioned. I don't know a way to say @if[p] blah returns
right away, breaking out of the body of what's being executed. But what's to
stop us from that?

Then we consider:

@par[foo] returns foo and continues.
@return[foo] returns foo and does not continue.

Or, @par[foo] adds foo to the current list of results and continues.
@return[foo] adds foo to the current list of results and does not continue.
@defb[...] does not add anything to the current list of results and continues.

So, when we interpret block structured text (or inline structured text?), the
result is a list of strings. Whichever command interprets that list of strings
can combine them however they want. By default, they are concatenated.

Any reason to do anything other than concatenate?

And, again, we have the problem of not know exactly which context a command
takes effect in.

---

How to move forward? I don't want to get stuck trying to design a perfect new
programming language. I want to make progress.

Question: Should we implement minimum primitives to support a general purpose
language  and build everything else as a library, or should we implement high
level primitives that do what I need?

The fear of minimum primitives is performance will be terrible.

Answer: I want support for general purpose programming in .fbld language.
Let's start with that. If performance turns out to be a problem, we can add
some higher level primitives built in.

So, focusing on the minimum primitives to start, what I want to be able to do:
* general purpose programming. If I can figure that out.
* @config[...] implementation.
* html escape implementation.

Really it's the last two that matter for making forward progress. The first
one is something to keep in mind.

When coming up with primitives, there are two things to figure out:
1. What primitives we want.
2. The class of behaviors that a primitive can have.

For (2), we currently have:
* Return a string.
* Add a new tag to the environment.
* @defb/@defi do argument substitution before evaluation.

The things we'll need for general purpose programming:
* struct literal, struct access
* union literal, union access, conditional execution
* functions
* recursion: types and functions.

We have a single built in, recursive, struct/literal type: string. That's
general purpose enough from the type point of view. What operations we need on
strings:

* string literal
* string concat (provides "struct literal")
* string head/tail (provides "struct access")
* string switch (provides conditional execution)

Maybe I also want variables, or equivalent. Like a @let tag.

Arguments to primitives can be arbitrary strings interpreted in arbitrary
ways. For consistency, it would be good not to have arbitrary different syntax
for different primitives. Try to reuse inline and block structured syntax as
much as we can.

For @defi,@defb, we've also introduced variable syntax $... Is that necessary?
I think it's useful to have. Distinguish between '$' substitution, which
happens when @defi,@defb is applied, versus '@' substitution, which happens
when the text in question is eventually, finally, evaluated.

There's still the open question of dealing with indentation for $
substitution.

Let's come up with some straw proposals. Really the big ones are switch and
head/tail.

What I want to implement, in practice:
* switch for config@.
* map for htmlescape.

The map@ command wants list argument. So list is something that would be nice
to have.

We already have a natural syntax for lists: block structured text (newline
separated), and inline structured text (command separated). Imagine then:

@list
 First item.

 Second item.

 Third item.

Or: 
@list
 @item First item.
 @item Second item.
 @item Third item.

Or:
@list[@item[First item.]@item[Second item]@item[Third item]]

Or:
@list[First item][Second item][Third item]
 
The list we are interested in is a list of pairs. Using tcl's approach for
mappings, it's a single flat list.

@list[&][&amp;][<][&lt;][>][&gt;]

@defb[htmlescape][text]
 @let[mapping] @list[&][&amp;][<][&lt;][>][&gt;]
  @map[$mapping][$text]

Or, perhaps:
 @split[ ][& &amp; < &lt; > &gt;]

I guess @split would translate that to the string:
 @item &
 @item &amp;
 @item <
 @item &lt;
 @item >
 @item &gt;

How to implement map@?

@defb[map][mapping][body]
 @if[@startswith[@first[$mapping]][$body]]
  @second[$mapping]...

I kind of want pattern matched based head/tail. So, either: head and tail,
given a pattern to match as counting as the head. Or head and tail given a
pattern to split on. I think the first is better.

So, say we have a notion of a pattern.

@head[pattern][string]
 
Returns as much of the beginning of the given string that matches the given
pattern.

@tail[pattern][string]

Returns the rest of the string not matched by @head[pattern][string].

Maybe pattern can be glob:
  ?, *, [chars], \x

Glob is generally useful syntax for dealing with strings. What if we make that
the core? Instead of ifeq, we have a glob match for if. head/tail take a glob
pattern to try and match against.

For example:
  @head[?][$x]

Returns the first character of $x.

If we want to match everything up until the first space, something like:

@defi[first][pattern][str]
 @let[head] @head[$pattern][$str]
   @if[$head] $head
    @first[$pattern][$tail[?][$str]]

Proposed primitives:
* @let[var][def][body]
  Returns the value of body with "$var" replaced with the value of def
* @if[condition][then][else]
  If condition is non-empty, returns value of then, else returns value of
  else.
* @head[pattern][str]
  Returns as much of the beginning of str as matches pattern.
* @tail[pattern][str]
  Strips as much of the beginning of str as matches pattern.

The syntax for @if nesting isn't entirely idea, but should be workable for the
time being.

Thus, we have:

@config[x]
 @if[@tail[

Uh, it's harder to match an entire string this way. I want something like:
 @if[@match[bindir][$x]] /usr/local/bin
  @if[@match[builddir][$x]] .
   @if[@match[datadir][$x]] /usr/local/share

And so on.

Why not make it much simpler? Skip the glob. All we need is:

* @let[var][def][body]
* @if[condition][then][else]
* @head[str] - returns first character of the string, or empty.
* @tail[str] - returns all but first character of the string.

We can implement @eq or @match ourselves with this.

@defi[eq][a][b]
 @if[@head[$a]][
  @if[@head[...

No. We loose character equality test that way, which is annoying.

How to match an entire string is easy: if @head matches and @tail is empty.

@defi[match][pattern][str]
 @and[@head[pattern][str]][@not[@tail[pattern][str]]]
  
---

After some more thought:
* Don't worry about fancy matching support.
* Provide exact string equals (which is as simple as character equals)
* Provide @head and @tail that return first character of a string and rest of
  a string respectively.

Another key idea: let's expand the syntax of fbld to provide better support
for multiple block arguments. The real challenge with @if is we don't have a
nice syntax for multiple block arguments.

Proposed new syntax:

A single '@' on the line following a block command extends the command. You
can put another same line argument on the same line, and a next line argument
on the next line.

For example:

@if @eq[$a][$b]
 This is what to do if condition holds.
@
 This is what to do if condition doesn't hold.

We could do a list of single line args:

@foo
@ Single line arg
@ Single line arg
@ Single line arg...

We could do a list of multi line args:

@foo
@
 First of 
 a multi line arg.
@
 Yet another multi
 line arg.
@
 And another.

Second proposed syntax: '@@' on the line following a block command causes the
rest of the text to be passed as another argument to the block command.

For example, instead of:

@if @eq[$a][$b]
 This is what to do if condition holds.
@
 This is what to do if condition doesn't hold.

We could write equivalent:

@if @eq[$a][$b]
 This is what to do if condition holds.
@@

This is what to do if condition doesn't hold.
 
Perhaps more useful example would be @let or @defi

@let[var][def][body]

Instead of:

@let[x][@foo]
 The body with $x.

We can write:

@let[x][@foo]
@@

The body with $x.

Now we have full control over glue logic, like <- in fble, conditional
statements, let, etc.

I like this idea syntax wise. No more need for crazy primitives that have
intermediate state if we just have this more complete syntax.

If we wanted, we could write a switch tag:

@switch[$x]
@ bindir
 /usr/local/bin
@ builddir
 .
@ datadir
 /usr/local/share


I guess it makes me want to also be able to write:
@switch[$x]
@[bindir] /usr/local/bin
@[builddir] .
@[datadir] /usr/local/share

So maybe @ resets to what you could write after the initial @switch tag.

If we have more widespread use of multi args like this, we'll want a decent
way to define vararg tags. Most likely they'll want to take a list argument,
like how tcl args works.

With this syntax, no need for tags to modify a global environment. They can
set up a local environment for their arguments. It's more functional that way.

The other open question is about multi-line variable substitution. Instead of
'$' and '$$', can we just have one that behaves exactly as you almost always
want?

@foo
 @bar
  Foo $x

It indents to the same indent as the line it's on. So, if x is "abc\def",
substitution becomes:

Embedding in inline text:

@foo
 @bar
  Foo abc
  def

Embedding in block text:

@foo
 @bar
  abc
  def

The only weird thing is if you want a different indent for the start compared
to the end.

@foo
 @bar
   abc
  def

When would you ever want that? Let's assume you wouldn't and see if I ever run
into such a case.

In summary, updates to syntax for fbld:
* @ on a line after block command extends the command's args.
* @@ on a line after block command adds one last block command arg.
* $ is only substitution supported, it indents to same indentation as current
  line.

Proposed primitives:
* @eq[a][b] - returns "true" if equal, empty string otherwise.
* @if[p][a][b] - returns a if p non-empty, b otherwise.
* @head[str] - returns first char of str, empty if str is empty.
* @tail[str] - returns all but first char of str.

Question: should @let be different from @defi, @defb? Probably.

* @let[var][def][body]

For backends
* @raw[str]
  Evaluates str as block text, returns the result directly.

There is still some confusion about what happens in a block context versus an
inline context. For example, @if is a block tag, then a and b are interpreted
as block. But maybe @if is also an inline tag, so a and b are interpreted as
inline. For example, in block body of @defi, maybe we want an if statement in
block form that returns inline text? Maybe we just need:

* @block[str]
  Inline tag that interprets str as block text.
* @inline[str]
  Block tag that interprets str as inline text.

In that case, we have the following cross products to specify when defining a
tag:
1. Is the tag being defined as an inline tag or block tag?
2. Does the tag interpret its arguments/result as raw, inline, block?

For (2), we can just use @raw, @inline, @block to specify. Assume the body of
a definition is always specified using block structured text.

That suggests, instead of @defi, @defb, how about a single @def that takes
inline or block as one of it's arguments.  Maybe i, b, or ib both?

@def[i][FbleVersion]
 @inline[fble-0.2]

Not sure. I'm not convinced I see much advantage to that.

Summary of proposed tags:

Block:
* @let[var][def][body]
* @defi[tag][args...][body]
* @defb[tag][args...][body]
* @raw[str]
* @inline[str]
* @block[str]
* @if[p][a][b]

Inline:
* @eq[a][b]
* @if[p][a][b]
* @head[str]
* @tail[str]
* @raw[str]
* @inline[str]
* @block[str]

Where @let, @defi, and @defb all use $ substitution with indentation as
described above.

Now:

@defi[config][x]
 @if[@eq[$x][bindir]] /usr/local/bin
 @@
 @if[@eq[$x][builddir]] .
 @@
 @if[@eq[$x][datadir]] /usr/local/share
 ...

Alternatively, instead of @if and @eq, we could have @switch?

@defi[eq][a][b]
 @switch[$a][$b][true][]

@defi[if][p][a][b]
 @switch[$p][][$a][$b]

Yeah. Even better.
  
Block:
* @let[var][def][body]
* @defi[tag][args...][body]
* @defb[tag][args...][body]
* @raw[str]
* @inline[str]
* @block[str]
* @switch[arg][branches...][default]
* @error[msg]

Inline:
* @head[str]
* @tail[str]
* @raw[str]
* @inline[str]
* @block[str]
* @switch[arg][branches...][default]
* @error[msg]


@defi[config][x]
 @switch[$x]
 @[bindir] /usr/local/bin
 @[builddir] .
 @[datadir] /usr/local/share
 @ @error[No such config: $x]

@defi[html][str]
 @raw
  @let[t][@html[@tail[$str]]]
  @@
  @switch[@head[$str]]
  @[][]
  @[&] &amp;$t
  @[<] &lt;$t
  @[>] &gt;$t
  @ $t

Now we get into trouble again with substitution. In the above example, assume
$t has newlines in it, which it will. $t doesn't get indented appropriately.
Unless we put it on a separate line?

@defi[html][str]
 @raw
  @let[t][@html[@tail[$str]]]
  @@
  @switch[@head[$str]]
  @[][]
  @[&]
   &amp;$t
  @[<]
   &lt;$t
  @[>]
   &gt;$t
  @
   $t
 
You almost want newlines in substituted values to not be treated as newlines
for the sake of parsing whatever was substituted into? Like, imagine we have
an escape for newline. \n. Then part of substitution is to change newlines
into \n? Is that what we really want though? In some cases $t might be a
sequence of commands we definitely want to be able to parse as such.

@defi[html][str]
 @raw
  @let[t][@html[@tail[$str]]]
  @@
  @switch[@head[$str]]
  @[][]
  @[&][&amp;$t]
  @[<][&lt;$t]
  @[>][&gt;$t]
  @[$t]

That works, I think. Just needs a bit of care.

Okay? We have a plan to move forward.

---

How to parse new syntax? I think I need more than 1 character lookahead. It's
not enough just to see '@'. I have to see what comes after '@' as well to know
if I should keep parsing args from the same command or switch to the next
command.

Alternatively, I could parse '@' as if it was the next command, and if the
parsed name is '', then append that to the previous command?

But then, how would I parse '@@'? And how would I detect, for example, for
inline parsed commands, that the name is empty?

I could Peek2. That's not hard to implement. Peek2, check if it equals "@@" or
"@ ". Let's go with that.

---

One implication of @@ is we'll want a single command to span across multiple
input files. So we need to concatenate all input files first and process them
in one go, rather than how we currently process them one at a time and then
combine the results.

Another note is we would need a Peek3 to match '@@\n', because it's always
supposed to have the newline after it, right?

---

Current uses of peek:
* IsEnd
* Is '\', if so, consume
* Is '@', if so, consume
* GetChar, etc.

There's probably a nicer way to structure the functions I'm using. Hmm...

IsEnd, Char, Is(String@)

Could be factored into:
  End, Char, String
  Is(...), Try(...)

Where End matches end of input or fails. Char matches not end of input,
returns the character. String matches exact given string. Is returns boolean
if parse succeeds or not, without consuming input. Try returns Maybe, if
match, it consumes, if no match, it doesn't consume.

Would we ever use Is instead of Try? More like I want a helper to try and
consume if matched but return a boolean instead of a Maybe@<Unit@>. Call that
Try_, with an underscore to discard the result?

Maybe I should implement these, then switch over and see what I like.

---

In theory the syntax is now implemented as desired. Next step is to start
implementing primitives. Or updating primitives for new syntax?

---

I drafted code for @config. It depends on primitives @switch and @error that I
haven't defined yet. Let me wait until I need to define those.

Next step is to start converting backends to .fbld. We have:
* html, markdown, man, text.

Let's start with the easiest?

markdown looks straight forward and is used for README.md, so it's realistic.
The only challenge I see is implementing @indent, which can indent text by a
certain amount. text backend is just as hard: it wants @indent too. Let's go
with markdown then.

How will this look?

I need to define the following:
 @doc, @section, 
 @url, @def, @fbld, @code

@defb[doc][title][body]
 @raw
  # $title
  $body
@@

So, I want to define @raw. This is a block command that takes a single
argument and outputs that argument directly without trying to interpret it.

Actually, indent is pretty easy because we get it naturally as a side effect
of substitution. So markdown.fbld is drafted. We just need to implement @raw.
Let's give it a try.

Oops. @raw isn't what I want, because I want to interpret the body as block
text first. I want something like:

@let[x] @block[$body]
 @raw
  # $title
  $x
  
Using variable substitution instead of command substitution.

Instead of @raw, how about generalizing it to @subst?

@subst[key][value][key][value]...[body]

I suppose we still want @raw so you can say things like... uh, not sure how
this would work. We could say @subst returns text directly. Or we could decide
if we want to interpret it as block or inline.

---

I propose the following:

* @let[key][value]...[body]
  Each value is interpreted as block command. The key is set to the result and
  substituted into all subsequent values and eventually the body.
* @inline[text]
  Interprets text as inline structured text and returns the result.
* @raw[text]
  Returns the raw text.

Then we would say:

@defb[doc][title][body]
 @let
 @[t] @inline[$title]
 @[b] $body
  @raw
   # $t
   $b
@@

I also want to rename @defb and @defi to something nicer, but worry about that
later.

---

Performance is really bad. While I try to get markdown working, let me also
work on performance at the same time.

Profiling shows all the time is in the Parser 'Get' function. Most of it is
calling Tail on the text. The rest calling Head.

I think the key is going to be avoiding allocations in Head and Tail. But how
can we do that?

Our operations are:
* Text, StringOf, LocOf, Empty, Head, Tail, Append, Concat, StartsWith, Drop.

We want to track locations all along the way and support fast append. Key, I
think, is for Text, Empty, Head, Tail, Append to all be really fast. But
really, we want Head and Tail to be really fast, and Append to be constant
time (doesn't need to be as fast).

There are two things Text@ adds on top of String@:
1. Tracking locations.
2. Fast append.

---

Here's what I think happens. Subst basically does Append(str, [char]) for the
entire string. That leaves us with a text structure like:

Append(Append(Append(Append(a, b), c), d), e)

Then we parse that string by asking for Head and Tail over and over again.
Let's look at tail.

It traverses down O(N) to get to the string 'a' and turn that into ''.
It asks if 'a' is empty, returns b. Then goes up the chain O(N) allocating
nodes. That means Tail becomes O(N). No surprise it's slow.

Some ideas:
* Flatten the Text@ just before parsing. For example, in the Parser Run
  method. Overall traversal would then drop from O(N^2) to O(N).
* There are only a few places we need fast append:
  - subst
  - main function to concatenate input from different files together.
  - Run inline and parse to concatenate command results together.
  Maybe we arrange to handle that better.

The pattern of use is fairly clear. We concatenate a bunch of strings
together into a string, then we parse the string to separate it into a bunch
of separate parts. We do this back and forth and back and forth. Then we want
to force concatenation exactly at the time after full concatenation is done
before parsing.

Thus the clear answer is this: flatten the Text@ just before parsing. That
should solve the problem.

What does it mean to flatten? If it was String@ we were talking about, that's
obvious. Remove all Appends, replace them with Cons. With Text@ locations,
that's less obvious.

What makes sense as a way to describe a sequence of characters that have, for
the most part, sequential locations, but occasionally jump around?

A. A list of (Loc@, Char@). We store the location at each place. There
duplicate information. For example, N characters takes
N*sizeof(Loc)*sizeof(Char) space.

B. Loc@ plus a sequence of Char@ or Command@. The command is used to jump to a
non-sequential location. May as well have that be Loc@. We save space now for
long runs of sequential text.

(B) is clearly an optimization. Let's start with (A), and save later?

---

Flattening solved the issue. What used to take a minute to run now just takes
a few seconds.

Next issues:
* Should defi 'def' be interpreted as inline text or block text?
 - If block text, why does that add an extra newline to generated README.md?
* Location tracking is broken somehow. My bad call to @url in README.md is
  being reported at the wrong place.
* How to implement @url that can optionally take a single argument?
  Short term workaround could be define @lurl, @lfbld, @lfile for example, for
  'labelled foo'. But in general, do we want to support varags? Seems like we
  should given so many of the builtins use varargs.

---

defi def should interpreted as block text. We can always use @inline if we
want it inline. We should figure out where that extra newline is coming from.

What I expect for parsing of arguments:
* [] args preserve all whitespace.
* same line arg does not include initial space or terminating newline.
* next line arg starts at next line and includes the newline on the last line
  of the next line arg.

Let's see if my test cases agree. Yes, they do.

Question to consider: is there any value in distinguishing between inline and
block commands? Wouldn't it be easier if a command can be used in either
context?

The benefits of having a command be used in either context are clear: don't
have to define it twice, don't have to distinguish between defi and defb or
remember to use the right one. No 'command not defined' if you are
accidentally using it in the wrong context. Difference between inline and
block becomes solely a parser distinction.

Why might we not want a command to be available and behave the same as both
block and inline?
* Define block command and default inline command should behave differently.
  Today they are both named ''. Solution is easy: give them different names.

* Consider @section. It doesn't make sense to use that in an inline context.
  That's meaningless. But it also doesn't make sense to use that in a
  subsection context or a list context. So I claim block versus inline isn't
  the right solution to this problem in the first place, so not a good reason
  to distinguish between block and inline in that way.

* Any issues with how we parse? Like, block command arguments for next line
  arg would have a newline at the end, which may interfere with how you want
  to interpret the argument. It's hard for me to believe this is a real
  problem though. You could always use [] arguments if you need in the
  presumably rare case this happens?

* What did I say before? Let's find previous discussion about this. I think I
  remember having it at some point. See around line 455 of this document. The
  main concern was my second bullet point above.

Let's do it. Let's say any command can be used in either inline or block
context. The only difference is how they are parsed. It's up to the user to
make sure (as they already have to today) that they use commands in places
appropriate for those commands to be used.

From a practical standpoint, the most important question is what to rename the
default block and default inline commands to.

How about, .inline and .block? The . making it impossible to explicitly invoke
them, so there is no accidental. That sounds reasonable to me.

Let's do this.

---

Note: the reason parsing the body of @defi would add an extra newline to
README.md was because it switches from using default inline to using default
block for the unadorned version text. That's easily fixed by explicitly doing
@inline[...] around the text.

---

Next question: why is location tracking not working as expected?

Here's a minimized test case:

@define[doc][body]
 @let[b][@block[$body]][@raw[$b]]
@@
@doc
 @url

The expectation is...
* The body argument passed to @doc has location 5:2
* After substitution, we get:
 @let[b][@block[@url
 ]][@raw[$b]]
 Where, in this case location of @url is 5:2.
* The second argument to @let is @block[@url], where @url is still 5:2.
* The argument to @block is 5:2.
* We invoke @url as a command with location 5:2.

Hmm... Maybe it's from the parser's choice of where command is? I'm not sure.
I need to debug this. If only the debugger wasn't so hard to use... Seems like
/Core/Debug%.Trace may be the best bet. Print arguments to all commands
invoked, using a print that includes location information for the entire
Text@, not just the start. That should make it clear at what point we lost the
location information.

---

I took a debug trace of all the commands as they are invoked. Let's see.

Initially url is r.fbld:5:3:0, $body has body at 2:18:1. It's when we
substitute the argument [5:2:1: @url] into the body of the define, it looks
like we loose the location associated with the body.

It must be happening somewhere as part of Subst.

Next steps in debug:
* Print Subst before and after.
* Print Indent before and after.

The fble code looks okay to me. It will be interesting to see where the
location information is getting messed up.

Indent is fine and returns the correct location: r.fbld:5:3:1: u

The bug is in the parser. The input has @url at the right place. The output
has @url at the wrong location. I should be able to add a test case for this.
Let's see if I can find the bug first?

The string to parse is:
@let[b][@block[@url]][@raw[$b]]

Where @url is at a different location.

Yeah, the problem is that parsing of arguments flattens the argument into a
single location string. We need to preserve the locations.

---

I fixed parsing of args, but looks like we have more location issues to track
down. Maybe start by cleaning up all the inline parsing stuff.

Next issue appears to be the text for default block commands? Or some
combination of default block/inline text and next line argument.

---

It takes a long time and a lot of memory to run one of the fbld conversions.
I think it's worth looking into to see if we can save iteration time.

It's generation of pkgs/fbld/fble.html. If all the time is in the backend,
then we actually don't want to worry too much about this. Maybe just remove it
from the build.tcl file for the time being.

Looks like it's in the final StringOf call in the main function. Let's see if
calling Flatten for StringOf helps.

Yeah, that helps a lot. There's still a lot of memory use, and it takes longer
time than I would like, but it's a bunch faster now.
faster now

---

Question: do we need to keep track of indent for Loc@ anymore? Now that we, in
theory, properly preserve locations across parsing?

The original reason we needed it was to be able to specify a location of a
start of a substring and infer the locations of all characters in the
substring, where the substring could be from and indented block. I feel like
we don't need that anymore. As long as we only infer locations from the start
of a file, column 0, we should be okay.

---

Now that location tracking is fixed, I've temporally(?) reduced @url to take a
single argument. We can translate README.md using the markdown.fbld backend.
It has some issues with whitespace. Let's see if I can track them down.

First issue: two blank lines before the Overview section instead of one.

This is while interpreting the body of the @doc command. Between @.block and
the next @section command. What do I expect?

The @.block command outputs "\n$body".

What we are doing: The @.block command adds a newline. Okay. That's what we
see. Should be easy to fix. Done.

I can't figure out why @url is adding a newline after the url label (but not
the url link). The only difference is @escape versus @inline. Does that mean
@inline introduces a newline somewhere? Looks like it's happening for titles
too. @inline is adding a newline.

Found it. In my definition of @.inline we were adding an extra newline. I
guess the moral of the story is something like:

@define[...]
 @foo
  $x

Adds a newline to the $x.

I still have a few extra blank lines to track down. Done. This is a little
tricky. Something good to be aware of.

---

The last backend to move out of .fble into .fbld is html. The only hard part
should be escaping html characters. The proposal had been to do that with
Head, Tail, and Switch, right?

Should be real easy. I can put them all in /Fbld/Builtin/String%. Remember:
 @head[str] - returns first character as a string, or empty.
 @tail[str] - returns tail of string.
 @switch[arg][k1][v1][k2][v2]...[def]
    Switch on arg, selects value of key equal to arg. If no match uses
    default. A default branch is not required.

---

It's pretty annoying trying to keep track of what variables are evaluated
versus not yet evaluated. Any way to make that nicer?

I tend to automatically think of an argument value as already evaluated. But
we can't do that, because we wouldn't have said how to evaluate it.

---

Trouble: my @code has a backslash in the body. Because of how substitution
works for html escape, eventually we substitute that single character into the
body of a let as:

@switch[\]
@[][@raw[]]
@@
...

That leads to an unterminated argument error. This is a problem.

We have some other problems with substitution and back end descriptions too:
* Hard to keep track of what text has been 'evaluated' or not.
* Have to come up with variable names for each intermediate expression.
* Bad things will happen if some variable names are a prefix of others.
* It's hard to keep track of what commands execute their results or not before
  returning.

Let's rethink things. A key idea is to parse before substituting. But let's
rethink from the top and see where we get.

Arguments to commands should be interpreted by the command itself. @code needs
to know it takes raw text. @section title should be inline. Different back
ends may escape text differently. Let's run with this.

The immediate consequence of this is nesting of commands has issues:

  @foo[@bar[xxx]]

In this case, do we want to pass the literal text '@bar[xxx]' as an argument
to @foo, or do we want to pass the result of running @bar[xxx] to @foo? Having
some syntax to distinguish between those from the caller might be nice.

We've said commands should know how to interpret what's passed to them. But
that doesn't mean we have to pass to them literal strings all the time. We
could pass to them computed strings.

This suggests we want some text transformation step between parsing from the
document and passing to the argument. In between [] for an argument (or for
same line and next line arguments). It should be intrusive syntax. By default
the string should pass as is. Only if you add the intrusive syntax are things
interpreted some different way.

Let's brainstorm. What if we had two different kinds of ways to pass args to
commands: @foo[...] versus @foo{...} for example. One can take a literal
string, the other can do processing first.

  @foo[@bar[xxx]]
  @foo{@bar[xxx]}

It's like "" versus {} in tcl.

Because it's part of the args syntax, it's not likely to conflict with literal
text, which is nice. But it's also not clear how to apply that to same and
next line args.

@foo[@bar[xxx]] vs @foo[$@bar[xxx]]
@foo @bar[xxx] vs @foo $@bar[xxx]
@foo
 @bar[xxx]
@foo
 $@bar[xxx]

How do we know if we want to interpret it as block or inline text?

A different approach would be to always do some substitution, but have
different syntax. Like, maybe $<name>[...] turns into a substitution
regardless, so we could say:

@foo[@bar[xxx]] vs. @foo[$bar[xxx]]

Another question: when do we really want to pass pre-evaluated commands
instead of post-evaluated commands?

@code: We want to pass raw text, which is to say, there are no commands. So
pre vs. post evaluation doesn't actually matter.

@section: Honestly it doesn't matter pre or post evaluation, except we don't
know how to interpret the commands for pre evaluation. For example:

@section Title
 @def[hello] there

Could go just as well to:

@section Title
 <dl><li>hello</li><dd>there</dd>

And then substitute in. We just need to know that it should be interpreted as
block structured text.

Is there any time we want to pass uninterpreted commands that are later
interpreted? The only thing I can think is if the context of the commands
changes. Maybe @doc for html versus markdown backend causes @section to be
interpreted differently, for example? In practice we define @section and @doc
differently based on what's included.

It's more like, would you ever want to have, in the same document, something
like:

@htmldoc
 @section ...

@mddoc
 @section ...

Where the @section commands are interpreted differently because they are in a
different context. I feel like, in general... that makes sense to me. The
commands @htmldoc and @mddoc can decide how they want to interpret their
arguments, which means they could define tags to behave differently, which
means we want to pass that structure to them.

Let's go down the path of having a new kind of preprocessing syntax. We say
all arguments are processed uniformly in this syntax before being passed to a
command. It's a syntax for constructing strings. We could do something like
tcl. We'll want literal text plus some optional substitution. Substitution
could be: $name word delimited. or ${name} to do direct variable substitution.
And $name[...] with inline argument syntax for calling a command. We could
handle escape sequences here for \[, \] and \$, strip those out automatically
before passing to commands for processing? Not sure.

How would this help things?

@define[EscHtml][str]
 @let
 @[h][@head[$str]]
 @@
 @switch[$h]
 @[][@raw[]]
 @@
 @let
 @[nh][@EscHtmlChar[$h]]
 @[t][@tail[$str]]
 @[nt][@EscHtml[$t]]
 @[@raw[$nh$nt]]
@@

Can now be:
@define[EscHtml][str]
 $switch[$head[$str]]
 $[][]
 $ $EscHtmlChar[$head[$str]]$EscHtml[$tail[$str]]
@@

Which brings an interesting twist on syntax: we want to have block structured
command calls.

It would be really nice if we could do this kind of syntax. We don't need @raw
everywhere, because it's all done as preprocessing instead of post processing.

But, in that case, why have a new syntax? We can use @ syntax for commands.
Maybe we just need a way to say substitute in a chunk of inline structured
text, or substitute a chunk of block structured text?

But if we have to wrap everything, then the nested arguments become tedious
again.

Needs more thought.

---

Two directions to take:
1. Evaluate after passing to commands.
2. Evaluate before passing to commands.

For evaluating before passing commands:
* inline arg, same line args treat as inline structured text.
* next line args treat as block structured text.
* need special syntax for 'raw' text.

Brainstrom special syntax for 'raw' text. For inline, we could use @[....].
For next line?

Let's think about @code, which is where we would use a next line arg literal
text. Maybe one of the rare places?

@code[fble]
 ...

We don't want it to be in brackets. We want it to be indent based. So, either
I put something on the first line, or... no, really it should be something on
the first line, indented.

@code[fble]
 %
 ...
 
A single % character, means the rest is raw? Kind of annoying to have to put
that everywhere.

Maybe we could indicate as some part of @code? Or, probably better would be,
similar to @[...] for inline:
 
@code[fble]
 @
  ...

Again, kind of annoying to have the extra level of indent everywhere.

I would argue we probably want a way to pass non-literal inputs to @code. Like
as part of a substitution. That suggest the caller has to indicate somehow. It
shouldn't be a property of the @code command itself. Unless we have multiple
variations of the @code command?

What if we had something like: where an argument is expected, you can put '%'
to indicate it should be interpreted as a literal string?

@code%[fble] %
 ...

Should there be any difference between literal text and default inline text?
For example, in html I would expect one to do html escaping and the other not
to.

Maybe only inline args can be literal?

@code[@[fble]][@[
...
]]

That's annoying syntax...

Let's push on the other direction. Say a command is responsible for parsing
its arguments still. Can we make the hard things easier?

@define[EscHtml][str]
 @let
 @[h][@head[$str]]
 @@
 @switch[$h]
 @[][@raw[]]
 @@
 @let
 @[nh][@EscHtmlChar[$h]]
 @[t][@tail[$str]]
 @[nt][@EscHtml[$t]]
 @[@raw[$nh$nt]]
@@

Let's say @head evaluates its argument as inline structured text first, then
takes the head? And say switch evaluates its argument as inline structured
text first, then does the switch. Then, transatively, say EscHtml evaluates
its argument as inline structured text first:

@define[EscHtml][str]
 @switch[@head[$str]]
 @[][@raw[]]
 @@
 @let
 @[nh][@EscHtmlChar[@head[$str]]]
 @[nt][@EscHtml[@tail[$str]]]
 @[@raw[$nh$nt]]
@@

If we assume default inline does concatenation, or we have a function for
that:

@define[EscHtml][str]
 @switch[@head[$str]]
 @[][@raw[]]
 @[@EscHtmlChar[@head[$str]]@EscHtml[@tail[$str]]]
@@

That's not too bad.

And, if we do this, we could turn arguments into locally defined commands:

@define[EscHtml][str]
 @switch[@head[@str]]
 @[][@raw[]]
 @[@EscHtmlChar[@head[@str]]@EscHtml[@tail[@str]]]
@@

This way we have full control over how things are parsed. We can always wrap
in @raw[], @inline[], @block[], @foo[] to explicitly switch modes to any
possibly user defined mode.

And, if we use commands instead of $ substitution, we could use define instead
of let?

@define[EscHtml][str]
 @define[h] @head[@str]
 @@
 @switch[@h]
 @[][@raw[]]
 @[@EscHtmlChar[@h]@EscHtml[@tail[@str]]]
@@

It's just a little annoying to need an extra line for @@. Any way we could
continue with a marker at the end of the same line? Like, end a line with @@?

@define[EscHtml][str]
 @define[h] @head[@str] @@
 @switch[@h]
 @[][@raw[]]
 @[@EscHtmlChar[@h]@EscHtml[@tail[@str]]]
@@

And if we do that, maybe we could phase out varargs entirely? Current use of
varargs are for:
* @let - but we can do as separate lines now.
* @define - we could pass arg names as a list: @define[foo][a b c][def][body]
* @switch - we could use ifelse chain with @@

Does this solve the problem with '\'? Because we no longer do things based on
substitution, but rather based on commands, maybe yes?

What does this mean for how $ substitution is indented?

More to think about, but this approach is appealing to me.

---

We'll still want a way to pass formatted text to @code. We can do that by
defining a wrapper function. Actually, @code would be the wrapper passing to
and underlying more general function:

@define[code][lang][text]
 @code_[@lang][@text]

Uh... This suggests we can't pass text through in non-raw format? I don't
know. I'm confused.

I feel like I should reset, go back from scratch, and think about what would
make sense for a string based programming language.

Remember how tcl works:
* {} for literal text, eval, subst functions, [...] to invoke a command within
  string text, $ to do substitution within string text.

There are two places transformations occur: at the caller to control what goes
to the command, and at the callee to control how that is interpreted.

Commands are a natural way to control things on the caller side. It's more
awkward to re-interpret at the callee side, because commands would have to be
escaped. We see this with tcl too. It's annoying to dynamically construct
scripts, because you have to escape the square brackets.

I like how my .fbld documents are described right now. Which could be
interpreted either as evaluating arguments before or after the call, except
for raw arguments to things like @code or @url.

@define[EscHtml][str]
 @define[h] @head[@str] @@
 @switch[@h]
 @[][@raw[]]
 @[@EscHtmlChar[@h]@EscHtml[@tail[@str]]]
@@

When I look at something like this, with @head[@str], I think of this is
replacing @str with its value and passing that to @head. I don't think of it
as passing '@str' to @head and letting @head do whatever it wants with that.

That's the dilemma here. On the one hand, I feel like @code and misc literal
arguments to things require we pass raw text everywhere. On the other, I want
to think about describing strings to pass to things.

What if we had a magic syntax for literal strings. Would that solve
everything? It would, wouldn't it?

That magic syntax could be just escape anything that would otherwise be
treated as a command? Or @[...]. Or @.

Here's my proposed magic syntax: Single '@' at the end of the line means
following next line arg is interpreted as literal text.

@code[fble] @
 @ Unit@ ... = ...

I suppose for inline we could say "@ " at the start:

@code[@ fble] @
 ...

That's a little verbose though.

Would we have trouble mixing '@@' and '@'? How to say interpret everything
that follows as literal last argument? Use "@@@" for this rare case?

How does @.inline and @.block fit into all of this?

Maybe we get rid of those entirely? Something like @doc or @section will get
preprocessed text, it has to insert paragraphs itself? Or keep @.block and
skip @.inline?

Instead of thinking of inline structured text as a list of inline commands, we
think of it as describing a string of characters, with substitution performed
where @ appears.

Maybe okay for inline text. That same line of thinking doesn't feel right to
me for block structured text, like html text. We would get something like

  Here is a paragraph

  Another paragraph

  <table>...</table>

  Another paragraph.

Some mix of already formatted html and plain text.

So, proposal is to get rid of @.inline. We always convert inline text directly
to substituted text. But keep @.block.

Okay? Get the overall proposal? Let's call it a straw proposal:
* inline args and same line args parsed as inline structured text.
* next line args parsed as block structured text.
* Remove all use of varargs.
* Use @[...] for literal inline text. Or escape all the @ characters.
* End a line with @ to say the following next line arg is passed as literal
  text without being parsed as block structured text.
* End a line with @@ to say the following is final arg.
* End a line with @@@ to say the following is literal final arg.
* Remove $ substitution. Commands define @... for arguments.

The only change required for documents should be add use of @ for literal next
line args.

Trying out with html.fbld, this is actually pretty nice:
* Next line arg is @inline by default if it doesn't start with a ....
  Oh, wait. Maybe that's wrong. We don't want it to insert <p>.

It would be nice for backends if we didn't have @.block being automatically
inserted. Or if we could have it behave differently for different commands.

Anyway, this is much nicer than dealing with substitutions and nested lets.

Should we get rid of @.block? It would make sense to me. Then we have
basically a template language for describing strings. The backend would be
responsible for parsing those strings again (e.g. for blank lines) and
inserting <p>, </p>. Just like it already has to parse the strings again for
html escaping.

Think about that more. That would certainly simplify fbld conceptually.

---

I don't like the idea of the backend having to parse its own output.
Everywhere else the backend is only parsing input and transforming it to
output. It makes sense for plain text to be parsed as input in the same way.
Which means both inline and block structured text make sense to process the
plain text.

Where it gets annoying is we don't have one global kind of processing. Each
command does its own kind of processing. The language argument to @code should
be raw processing, for example. We shouldn't do html processing on that.

We see the problem in my draft of the new html.fbld. We want to use @inline to
do string template substitution, but we certainly don't want the html
processing to be done to the html tags we put there.

Take, for example:

@define[section][title body]
 @inline[<h2>@title</h2>
 @body
 ]
@@

Consider what's in @body. Some plain text. Some already transformed tags and
inline html. But that also means some already transformed plain text, right?
Is it, like, the first time we parse it that we process the html escapes and
paragraphs. All the rest of the times we are dealing with raw strings, so it
doesn't matter?

We still need some way to turn off html escaping in the backend when
outputting html tags.

---

Let's try again.

@doc is a command. It takes as input a title and body content. The title
should be inline structured text. The body should be block structured text.
It's the job of the @doc command to parse the title and to parse the body as
text, with appropriate commands provided at parse time.

For example, we can say @doc can include @section, @.block, @.inline, etc. We
parse those commands explicitly, say.

After @doc has processed the title and body, the result is escaped html text.
Now @doc performs substitution to create raw text that is the result of @doc.

This suggests we want the following kinds of things:
* @inline, possibly with explicit supported inline commands.
* @block, possibly with explicit supported inline commands, and explicit
  @.block command.
* @subst, for doing substitutions in raw text.

What's the difference between structured text and subsitutions?

Whether the command is defined or not?

@foo[@bar[@x, @y]]

I'm pretty sure I know what I want to execute before as opposed to after. So
why don't we say, anywhere you use '@' is substitution for after. Anywhere you
use '$' is substitution for before?

@define[doc][title body]
 $inline
  <head><title>$html[$title]</title></head><body>
  <h1>$html[$title]</h1>
  $html[$body]
  </body>
@@

The trouble is nesting of things?

Or, the issue is distinguishing between code (where substitution happens
immediately) versus data (where substitution will only happen if explicitly
asked for)?

It feels like we should be able to say: substitute directly, or treat as data.
If treated as data, it will only be substituted when explicitly asked. There
is no mixing of immediate substitution and explicit substitution.

---

Say there are four kinds of things I want to do:
1. Process structured text. Invoke inline and blocked structured @ commands.
  e.g. @doc takes structured text as an argument and wants to invoke
  the commands on that.
2. Process unstructured text. 
  e.g. @HtmlEsc wants to transform a string to another string.
3. Substitute to construct strings.
  e.g. @doc wants to substitute processed body and title into string
<head>...</body>
4. Transform structured text.
  e.g. @tutorial defined as @doc[$name][ @FbleVersion (@BuildStamp)][$content]

Say we want to use different syntax for these different kinds of things. Note
that for the most part, docs would write structured text. All these four
things are things done by backends.

To process structured text is easy. Have some function to call on structured
text, provide necessary information, the result is the string after
processing. For example: $process[$arg]

To process unstructured text, we want a function language where arguments are
evaluated before being passed. For example: $foo[$bar[$sludge]], evaluates
$sludge first, then $bar[...], then $foo[...].

To substitute into strings, we could either have functions to compose strings,
or allow processing within a string of text. For example:
  '$concat[hello, ][$foo[...]][there]', or 'hello, $foo[...] there'.

Could I do multi-line let statements with $? Inline structured text with $ is
fine assuming no $.inline processing takes place. How about block structured
text with $?

$let[x][$foo[$bar]] $$
...

I think that's okay too, with no $.block processing taking place.

Let's try this then. We say @ is deferred processing, $ is immediate
processing. Otherwise same syntax. As soon as you get inside @, the $ is
deferred?

Let's look at some examples.

@define[EscHtmlChar][char]
 $ifeq[$char][&] &amp; @@
 $ifeq[$char][<] &lt; @@
 $ifeq[$char][<] &gt; @@
 $inline[$char]
@@

@define[section][title body]
 <h2>$HtmlEsc[$title]</h2>
 $ProcessBody[$body]
@@

@define[tutorial][name, content]
 @doc[$name]
   @FbleVersion (@BuildStamp)

  $content

Now this last one is interesting, because we want the $content underneath the
@doc. How do we know to process it instead of defer it?

Could we have some other syntax for creating structured text? Like an explicit
function call?

@define[tutorial][name, content]
 $block_struct[doc, $name, @FbleVersion (@BuildStamp)

  $content]

It seems like everything is reasonable and easy to do so far except
transformations on structured text, where we want to produce structured text
programattically.

But wait. In this case, can't I directly call the command?


@define[tutorial][name, content]
 $doc[$name]
   $FbleVersion ($BuildStamp)

  $content

Maybe, but once we start nesting, we run into the same problem:

@define[tutorial][name, content]
 $doc[$name]
   $FbleVersion ($BuildStamp)

   @section Hello
    $content

Here I want the @section passed to $doc, not interpreted in place.

Let's tweak things slightly then. Mix @ and $. So that $ processing is done
on arguments to commands before passing those args. And it could either be
block structured or inline structured $?

Think about how that would work.

---

Processing an inline or same line argument:
* We don't know if the command will process the text as raw, inline, or block
  structured text.
* We want to embed $ substitutions.
* So, scan the string, any time you see a $ sign, perform the appropriate $
  substitution. It can work just like how $ substitution works now, but
  generalized to support calling commands with arguments.
* For the purposes of this, ignore '@' characters?

How about processing of next line arguments? The idea is, in some cases I want
an equivalent of block based $ substitution. Let's say next line arg and final
args are scanned using block based $ substitution. Same syntax as block,
except with '$' instead of '@'.

Now, if you see '@' somewhere in that?

@foo[$bar]
 $ifeq[...]; $$
 @blah[$x]
  Blurg.

To me, intuitively, that should substitute $bar, $ifeq, and $x to result in:

@foo[...]
 @blah[...]
  Blurg.

So, maybe we can have block based $ substitution in inline text too?

@x[
 @foo[...]
  @blah[...]
   Blurg.
]

We can detect it based on newline in the text?

That sort of begs the question, is there any need to distinguish between
inline structured text and block structured text in the first place with '@'?

So here's my proposal: we have $ processing of text, which supports inline and
block syntax simultaneously, and completely ignores '@' occurrences. And then
we have some function that can process @ structured text explicitly.

You need to use @ when you want to pass a command to a subcommand to be
interpreted. Otherwise prefer use of $.

Can $ substitution interfere with syntax of @? I want to say no if possible.

For example:

let x = "]@foo"
in
@bar[$x]

This should turn into @bar[...] where the argument is literal "]@foo". It
should not turn into the malformed syntax @bar[]@foo]. 

This suggests we parse a string for '@' while doing substitution, even if
eventually we aren't going to '@' process the string.

I have a question then. If '@' is only for deferring evaluation, how about we
instead do that based on whether or not something is defined? For example:

$foo[$bar]
 $ifeq[...]; $$
 $blah[$x]
  Blurg.

Say $foo and $blah are undefined. Then after substitution this becomes:

$foo[...]
 $blah[...]
  Blurg.

It means it's less obvious what we are passing, and harder to catch mistakes
about what is undefined. But conceptually we could treat $ and @ as
interchangeable in terms of parsing.

This makes me wonder: should/can we parse every string all at once up front as
structured text? Assuming we have a way to easily enough describe literal
(unprocessed) segments of text?

---

A new line of thinking. Let's lean the other direction, again, where
everything is evaluated eagerly. The problem with that is, for example, how to
do html escaping. The problem with that is sometimes you want implicit @.block
and @.inline and sometimes not. Generalizing, the problem is sometimes you
want a command to have a global definition, like @head, and sometimes you want
it to depend on what command you are in. For example, maybe you want @item to
mean something different inside @list compared to @definition.

Could we have some form of namespacing to distinguish between things?

For example, if I define an @section command with arguments title and body, I
want implicit paragraphs to apply to the body. So maybe we define a command:

@define[section.body.block][txt]
 $par[$txt]


Which is applicable if you are parsing the 'body' argument to the 'section'
command.

Now we can more clearly control what tags appear under which commands, and
maybe we can always do eager evaluation, parse everything only once. What do
you think?

Maybe we could define somewhere whether an argument is supposed to be
interpreted as raw, inline, block, or other structured text using a similar
approach?

What we really want to do, essentially, is define a type for each argument.
The type describes how the argument is parsed (some user provided function)
and the environment of commands available when parsing.

---

More on combining block and inline structured text into a single syntax:

If you are in 'block' mode, it's pretty easy. Inline and same line args turn
you to inline mode. Next line and final args keep you in block mode.

The more interesting question is, if you are in inline mode, can you switch
to block mode? When would you want to?

We could say you can't. But maybe it would be nice to use block mode to define
some substitution into inline mode?

  Hello, I would like $block[
  $let[x] abc $$
  $x$x$x
  ] repeated even more.

I would rather pull the block mode out to the top:

  $let[x] abc $$
  Hello, I would like $x$x$x repeated even more.

Let's say you can't go into block mode from inline mode. But now we have a
single unified parsing mode. You start in block mode. Depending on what you
are parsing, maybe you go to inline mode. Just like we already have today.

---

Are you ready? The image in my mind is now:

In block mode, start parsing. When you get to a command, read the command
name. For each argument to the command, look up in the environment how to
parse it. So parsing happens in an environment. Read the command raw, apply
the environment function for that argument to parse it in its own environment.
After all args have been parsed this way, pass them to the body of the command
which performs some substitution to produce a raw string. And maybe we can say
there is a post processing step for each command as well, read from the
environment.

In other words, a command has:
* A name.
* A list of named arguments.
* For each argument, a processing function.
* A body.

The body is interpreted as block structured text, where each argument is
defined as a command that returns the arguments string value directly.

Why do we need a processing function for each argument? We could define that
locally in the body of the function. Which brings us back full circle to
passing raw strings as arguments to functions. I can't seem to get out of this
loop.

Let me try to state very clearly here the pressure to pass raw strings:
* We want to interpret arguments differently depending on the command they are
  being passed to. So it makes sense to pass the argument in raw form to the
  command and have the command process it however it likes.

What's the pressure to eagerly evaluate arguments?

I think it's cases where we want the caller to do substitution on the argument
before it is passed to the callee to process. So both caller and callee have a
chance to influence the argument.

How about this then, we always perform $ substitution on arguments before
passing them to commands. $ substitution is an inline substitution for
arguments passed as 'inline', regardless of how they are interpreted by the
callee, and $ substitution is a block substitution for arguments passed as
'block', regardless of how they are interpreted by the callee.

That means, all we need is to figure out what it means to do $ substitution in
the presence of @ commands. I already know that though: ignore the @.

And maybe we say $ substitution of raw strings with @, [, ] etc. automatically
escape those characters so they will not be treated as syntax by the given
command?

Here's the key thing, I think, to avoid swinging back to everything eager:
* In some cases, we want the caller to be able to manipulate the string. In
  some cases we want the callee to be able to manipulate the string, and
  differently depending on the callee. So we should support both modes.

Summary of changes to make:
* Generalize $ substitution to support commands with arguments.
* Add block based $ substitution syntax.
* Set it up so that $ substitution cannot impact how text would be parsed as
  inline or block structured text. (this could be tricky?)
* Consider removing all use of varargs.

Which brings up the other point that pressures us towards eager evaluation.
How to do $ substitution without impacting how text is parsed, when you don't
know the syntax that will be used to parse the text at the time of $
substitution?

Let's do a deep dive of escape characters next. That's often been a point of
fuzziness in fbld.

---

New proposal: pass around structured strings, not raw strings. Do parsing of a
file once at first read, then never again.

Have substitution preserve structure, avoid flattening things.

Now, I claim this means we can process escapes when we first parse, and never
have to worry about them again.

What this means is:
* We parse all args as if they were inline or block structured text, based on
  the syntax used for the arg: inline, same line, next line, final.
* To pass raw strings, you pass them as inline structured text with a single
  @.inline tag.
* @.inline and @.block are added by the parser when first parsing.
* Functions like @head expect an @.inline tag and go inside of it.
* We still distinguish between $ and @ commands. @ commands need to be
  explicitly processed by the callee. $ commands are processed before passing
  the structured text to the callee.

Consider for example:

$let[x] \]\@foo $$
@bar[$x]

The argument to @bar will be "]@foo", which was unescaped at parse time.

First thing we do is parse the entire thing into structured text. We can
process $ substitution as we go. The result is structured text.

Then we process it as blocked structured text. The result is structured text.

Last thing we do is concatenate the structured text back to raw text and
output that.

The way html escaping works is: initial parsing adds @.inline and @.block.
Because they are @ commands and not $ commands, they are deferred until @doc
interprets them for the body. In the body it will be replaced with
@.inline[$HtmlEscape[...]] and @.block[$par[...]], and the final top level
@.inline and @.block become straight concatenation.

If you want to pass raw code, for example, you need to escape. Or we can use
our 'raw string' syntax already brainstormed above. I think indent-based raw
string syntax will make it easy to describe any kind of raw string.

This sounds promising to me. How do we figure out if it will work in reality?

Do I expect to have to change any of my existing .fbld docs? Maybe to add
extra escapes where raw strings were expected. Otherwise no assuming the top
level is implicitly processed as block structured text.

Perhaps it's worth going back to the basics:

1. Rewrite the .fbld spec with full details of the new proposal.
2. Implement the new proposal with fble based test cases.
   Don't worry about back ends right now. Expect fble based fbld translation
   to break on existing docs.
3. Port the back ends over, slowly bring back the fble based fbld translation
   on existing docs.

Could we do this incrementally instead? For example, something like:

1. Process escapes at parse time, in both tcl and fble impls.
...

I'm not sure. Sounds complicated.


Question: how do we support @define with this syntax? Because in theory the
body will use '$' in it, but we don't want to process those until the function
is called. Do we need special syntax for defining commands? Or we'll end up
using raw string syntax for that?

Maybe it's a question of when we parse something versus when we execute
something. Maybe we should parse first and do $ substitution as a separate
path?

---

Do we need $ substitution at all? Can we using @ substitution for everything,
only this time do @ substitution on explicit tags at a time instead of all at
once?

Imagine we parse the entire document into structured text first, no
substitution at parse time.

The next step is to 'evaluate' the structured text. All we need is a clear
evaluation strategy that works as desired.

Recall our built in tags. For now let's just use the current ones:

define, let, switch, head, tail, raw, escape, block, inline, @.block,
@.inline.

Top level evaluation:
* Parse doc.
* Evaluate each command in turn.
* Concatenate the results as text.

Evaluation of a command returns structured text.

@raw - no longer needed.
  Because we use inline structured text for these instead.
@escape - no longer needed.
  Because escapes are processed once at initial parse time.
@block - no longer needed. (?)
  Because you can't switch to block text from inline mode.

@head, @tail
 The argument should be inline structured text.
 If @.inline, goes inside and takes the head.
 If @<command>[...], throws an error?

@switch
 The argument should be @.inline[...]
 Replaces with the matching body. Then continues evaluation of the
 replacement.

@define[name][args][def][body]
 Evaluates body in the context where @name is defined.
 Evaluation of @name evaluates def ... in the context where @args are defined?

Now we need to be careful about the difference between evaluating in the
context where something is defined versus doing a deep string replacement.

What I want is:
* Evaluate body in the context where @name is defined. Don't eagerly apply
  things in body, because we want to be lazy in that sense.
* When it comes time to evaluate @name, substitute args into def, then
  evaluate the result.

Ideally we don't evaluate or expand parts of def before substituting the
arguments.

let can be replaced with @define.
 
I'll want an explicit eval@ kind of thing though, right? How do we say: define
some commands to be applied to a pre-existing body?
 
@define[doc][title body]
 @define[ProcessBody][body]
  @define[@.inline][txt][@HtmlEsc[@txt]]
  @define[@.block][txt][@par[@txt]]
  @define[@section][title body]
   ...
  @@
  @body
 @@
 <head>...
 <title>@HtmlEsc[@title]</title>
 </head><body>
 <h1>@HtmlEsc[@title]</title>
 @ProcessBody[@body]</body>
@@
 
Do we need @eval, or is it automatic? If you are evaluating @eval, then you
should have evaluated @body anyway. 

I think this could work.

Okay. Enough thinking. Let's make it happen for real. I have a vision in mind.
Let's get there, hopefully incrementally. I'll learn and document here
whatever issues come up. Ditch $ substitution for now. That will be much nicer
if we can get away with it.

Step 1:
* Update the parser to parse the entire text into structured text.

We'll want a new datatype for structured text. Change the parser to return
that. Rewrite whatever code we need to to get things to compile now that
commands take structured text as arguments instead of Text@.

Once that compiles, run it over existing fbld docs and update them as
necessary to meet the new syntax requirements.

Maybe we only need to keep track of locations of commands and args now?

Structured text is:
* Inline, Loc@, String@
* Or List of commands.

Command is:
* Command name, Loc@, list of args.

Repurpose Text@ type for this purpose?

---

Starting pkgs/fbld2 for the rewrite of fbld.

First steps: defined structured text, implement a parser.

The parser will take as input a list of files, where each file has a path and
string text. State will be that list of files, current location. It returns a
value or an error message.

---

Now is probably a good time to clean up fbld syntax, sad as I am to defer
implementation work even more.

Specifically, there's no way currently to have a zero argument command
followed by plain text characters. For example, @foo followed by 'bar' would
be: @foobar, but that means something different.

Let's be conventional: @foo and @{foo} can both be used. Then we have:

@{foo}bar versus @foobar. Hopefully it's a rare case. But you could also use
it for more interesting command names.

I see no need to escape [ and ] in plain text. As long as [ doesn't follow @
and they are nested, it's clear from context.

Let me try an abstract syntax:

command_name ::= 
    name
  | '{' text '}'

inline_command ::= '@' command_name '[' inline ']' [...]

inline_element ::=
    plain
  | inline_command

inline ::= inline_element [...]

Hmm... It's a little tedious. I feel like it's easier to describe in words and
context.

Parsing of inline text:
  Until end of stream or closing ']':
    if '@':
      Parse command name as name chars or '{' ... '}',
          where ... can be anything with nesting of '{' '}'.
      While '['
        Parse inline text to closing ']'
      continue
    Parse plain text as sequence of characters until '@', with nesting of '['
    and ']', and character escape \x --> x.

Notes:
* Whitespace is preserved as normal characters.
* Name chars are [a-zA-Z_0-9]
    
Idea: instead of {}, maybe just use string escape? @foo\bar? I kind of like
that. It should always work since \ is not allowed? Except, it turns \ into
both escape and syntax denotation, which is maybe a little awkward?

@{foo}bar
@foo\bar 

Let's try it with \ to break up instead of introducing {}. It should be rare
anyway.

Parsing of inline text:
  Until end of line or closing ']' depending on initiating context:
    if '@':
      Parse command as sequence of non-empty name chars.
      While '['
        Parse inline text to closing ']'
      continue
    Parse plain text as sequence of characters until '@' or closing, with
    nesting of '[' and ']', and character escape \x --> x.

Notes:
* Whitespace is preserved as normal characters.
* Name chars are [a-zA-Z_0-9]
* \ is a delimiter for name chars, so, for example: @foo\bar is the command
  '@foo' followed by plain text 'bar'.
* Inline text can start at '[', which must close with ']', or it can start
  from a same line arg, which must close with end of line.

---

There are three different contexts in which inline structured text can occur:
1. Between '[' and ']',  for inline args.
2. Between ' ' and end of line (or to ' @@\n'?), for same line args.
3. Between '\n' and a blank line, for plain block text.

It's tempting to say we should pull out the block of text first, and then
parse it for inline text. I think that's a bad idea, because if there are a
bunch of nested arguments, we end up with quadratic blowup of the number of
characters parsed. For example: a[b[c[d[e[f[g[h[i[...]]]]]]]]], the characters
in the middle end up being gone over for each level of argument.

Hmm... maybe it's not such a bad idea, because when would something like that
ever really happen in practice? Otherwise we could keep track of the expected
end character and the current level of indent while parsing and read through
things once.

Let me try the read through things once approach and see how complicated it
is. I think that will be better in the long run if it isn't too hard.

How to parse block structured text?

* Skip over blank lines.
* If end of file, done.
* If line doesn't start with '@'
  - Parse as inline text to next blank line.
* Else line starts with '@':
* Parse '@' <name>
* Parse inline args following starting with '['
* If there is a space, parse same line arg as inline text to end of line.
  Or to ' @@\n'?
* If next line starts with '@@', parse final arg. 
* If next line starts with a space, parse next arg as block text with indent
  level increased.
* etc...

Summary of block structured text options:
* @name to start.
* [...] is inline arg.
* text to end of line is same line arg.
* text indented on next line is block structured next line arg.
* @ on next line is continuation for inline arg or same line arg.
* @@ on next line signifies final arg follows at same indent level.
* @@ after same line arg signifies final arg follows at same indent level.

So, same as what we have, but I want to at least add @@ at end of same line
arg.

Do we also want to use @@@ and @[] for raw text?

---

When we parse @foo[bar], should the argument be parsed as plain 'bar', or as
structured @.inline with an argument as plain 'bar'?

I don't like having that kind of ambiguity. Can we get rid of it?

To get rid of it, we would have plain be a command instead, and use that
instead of the implicit inline command. Then markup is always a sequence of
commands. We no longer need a '.inline' tag. In that case, should we do the
same for .block?

Or... Markup@ is either plain, a command, or a list of Markup@? We remove
.inline and .block tags entirely, and instead have list structure that can be
processed as desired?

In theory that would allow for arbitrary nesting of things, instead of
two-level nesting 'block' and 'inline'. A much better tree structure I should
think.

markup ::= 
   plain text
 | tag [args ...]
 | sequence [markup ...]

I like it. And when we parse, never parse into a single element sequence?

If we wanted to enforce that structurally, we could do:

markup ::=
   empty
 | plain <text>
 | tag <args>
 | seq <markup> <markup>

I don't know. That might be overly complicating things.

Notes:
* It is possible to describe an empty markup by having an empty file.
  - Meaning we ought to have a way to represent empty markup.
* The concrete syntax has no way to distinguish between ([a, b], [c, d]) and
  [a,b,c,d].
  - Meaning it probably makes sense to represent a sequence as a list instead
    of a tree.
* The concrete syntax has no way to distinguish between (a) and ([a]).

@foo[bar] vs @foo[bar@x]. They should have parallel structure I think. Same
kind of representation.

So, that all leads us to...

markup is a list of (tag or plain).

So...

Command@ = *(Text@ name, List@<Markup@> args),
Element@ = +(Text@ plain, Command@ command),
Markup@ = List@<Element@>;


[bar] is parsed as a single plain element.
[bar@x] is parsed as a list of a plain element followed by a command.

An implicit block of text can be represented as a Command@ with name @.block.
This is different from plain text, which uses a plain element instead of an
@.inline command, because it's argument cannot contain structure.

---

Details: when parsing default block inline text, how do we know when to stop?

* If start of line does not match current indent, treat it as 'end of input'
  for the purposes of the block inline text.
* 'match current indent' means the line is prefixed with the current indent,
  or it contains only spaces.
* The inline text ends at:
 - 'end of input', with or without a newline just before.
 - a blank line.

I know I want to track current indent and [ nesting level. If we are within
inline arg parsing [ ... ], we still need to deal with indent level and be
aware of 'end of input' from a blank line.

Is it really better to take this approach, instead of extracting the subtext
first and then parsing that subtext as inline text stopped by 'end of input'?

Let's keep going. We should be able to encapsulate 'endofinput' as a thing and
be able to compose it.

---

It's not endofinput that we want to encapsulate. It's the 'Loc' and 'Get'
functions.

Conceptually I want something like:

  (M@<T@>) { M@<T@>; } Indent

Which runs a parser on block indented text. That parser will not see the
indent characters and will get end of input back at the end of the indented
text.

Similarly for SameLine and InlineArg?

I just need to work out the details of how to implement this.

The parser monad should know its current indent level. If you call 'Get', it
does it based on current indent level. The 'Indent' function runs a parser on
an extra level of indent. The end of input is different depending on indent
level. That's not too difficult, right? But it means making the syntax part of
M@ itself?

Inline Parsing:
* [ ... ]
* ' ' ... '\n'    (same line)
* ... '\n\n'  (@.block)

General Parsing:
* block indented

A clear first step is to be able to parse at given indent levels. Say we take
care of that. What comes next?

* Same line args should be allowed to end at end of input.
* @.block args should be allowed to end at end of input.
* Something about plain [ ] being nested as we parse inline input.

Question: Can we have newlines and blank lines inside inline args that are in
@.block context? I would say yes.

Good. The idea is clear. First step should hopefully be one to make progress
on the current test failure.

The current test failure is, for

  @title A title

  Some intro text

  @section[...]
   ...

The plain 'Some intro text' parsed doesn't have the terminating newline
character. Because we look for \n\n for the end marker. I don't know a clean
way to deal with this without storing some extra bit of state somewhere saying
newline was the most recently gotten character.

The way we parse it in fbld1 is to extract all the plain text first, where we
have the context of newline followed by newline.

Options:
* Add a 'start of line' bool state to the parser.

Yeah. That seems like the most reasonable approach. We want to parse inline
text until we see a newline at the start of the line.

---

Easy way to detect blank line: check for the column. If it's a newline at
column 1, it's a blank line. Cool. That fixes parsing of end of line in
@.block text.

Two issues I see next from the test:
1. We are failing to consume the trailing newline in the case there is no same
   line argument.
2. Support for parsing with an indent level.

---

Can we simplify the core parser monad?

The Core API today is:
* Do, Return, Error,
* Loc, Get
* Peek - Could be implemented as Test(Get)?
* Indent
* Try, Test
* Run

The non-core API today is:
* EndOfInput on top of Peek.
* Try_, Test_
* Or, String

That's not too bad. Let's move Peek out. Then the question is would it make
sense to merge Loc and Get, so that Get returns the location?

It all depends on how we want to implement Indent.

The idea is we have a current indent level. Loc and Get are the only functions
impacted by it.

Get: Should return the next character after stripping indent, or end of input.
Loc: Should return the position of the next character to be gotten.

In case of end of input, does Get advance past indent? Let's think.

  ...
 
 .

@foo
 @bar
  blah
 @end

@foo
 @bar
  blah
@end

I think it's fine for get to advance past indent. I think it's fine for Loc to
advance past indent.

So here's my proposal. Have a helper function 'SkipIndent'. The job is to skip
past any indent characters and go to the next real character. Use this for
both Get and Loc functions. Then Loc can return like it does now after running
SkipIndent. Get will need to check for indent level after calling SkipIndent
to see if end of file. Easy, no?

---

The basic fbld2 parser is good to go. Next step, I think, is to try parsing my
existing documents and see how they have to change to work with fbld2 syntax.

This might be a good time to write down the fbld2 syntax reference. Let me
also write a main program that parses fbld2, succeeding on successful parse
and failing on error.

---

Made a main program for fbld2. First couple issues I see:

README.fbld: Parses everything without error. But some \ need escaping.
fble.fbld: Fails to parse @l[Bool@], as I would expect.

What do we want to do next?
* Iron out all the details of how to parse corner cases.
* Implement core evaluation.
* Document syntax reference.

Let me sketch out syntax reference, and then work on evaluation. It would be a
shame to spend a lot of effort on syntax corner cases only to realize the
evaluation strategy doesn't make sense.

We can already parse README.fbld. That's a perfect place to start. Generate
README.md from it.

---

The markdown backend for now just uses text substitution. That's a decent
place to start focus on. We should only need the one builtin command to define
user defined commands. Assume fixed number of arguments for now for
simplicity.

The challenge is coming up with an evaluation strategy that:
* Allows for recursion. In other words: don't be too eager in substituting
  things in to arguments to if statements.
* Makes clear what commands are bound to what definitions.

Let me not worry about recursion for now, because I don't need it for
markdown backend.

Example 1:

  @doc Foo
   @section ...
    ...

We use @section in the argument to @doc. We expect @doc to provide the
definition of the command for @section.

Example 2:

  @def[foo][bar]
   @bar@bar
  @@
  @foo[xyz]

We use @bar in the argument to @def. @def provides the definition of the
command for @bar. It changes depending on the invocation.

Example 3:

  @def[foo][section]
   @doc Foo
    @section ...
     ...
  @@
  @foo[xyz]

Who determines the meaning of @section here, the @def[foo] or the @doc
command? Options:
a. @def[foo] defines @section. Then @doc overrides @section. So @doc determines
   the meaning. Corresponds to what would happen if we made args definitions
   for commands that are lazily evaluated.
b. @def[foo] substitutes for @section. @doc never sees @section. So @def[foo]
   determines the meaning.

Imagine we wanted to use @section in the argument to @foo. I feel like option
(b) makes most sense.

Example 4:

  @let[section] hello @@
  @def[foo][section]
   @doc Foo
    @section ...
     ...
  @@
  @foo[This is my favorite @section!]

Yeah, so... this is confusing.

If this were a normal programming language, I would say...

  @let[section_1] hello @@
  @def[foo][section_2]
   @doc Foo
    @section_2 ...
     ...
  @@
  @foo[This is my favorite @section_1!]

But normal programming languages don't tend to let you pass free variables in
arguments that are bound by the function they are being passed to.

---

To start with, I want to try lexical scoping for everything except where we
explicitly want to pass free variables. Let's go with that mindset.

For starters then, let's have section defined at the top level of
markdown.fbld if we can, rather than as part of the @doc definition.

How to write markdown.fbld?

* par - add a newline in front.
* plain text - stays as is.
* .block - is @par
* doc:
  # $title
  $body
* section:
  ## $title
  $body
* url: [$url]($url)
* fbld: @url
* def: * **$name**: $value
* code: indented $text
    
How do we do doc, for example?

I expect title to be inline markup and body to be block markup. Looks like I
want a way to insert elements into the markup lists. Specifically: cons plain
'# ' to title. Cons that to body. Maybe:

@doc[title body]
 # @title

 @body
@@

Let's say title is "Foo @bar".

Then after substitution we have:
  ["# ", @title] ==> ["# ", ["Foo ", @bar]]

Which differs from ["# ", "Foo ", @bar]. Does that matter?

For inline text, no. For block text, depends on how it gets concatenated.
Let's assume no for now, and that text is all directly concatenated for the
final output.

How do I add a newline? Should we add newline to the list of escape
characters? I think so.

How to define .block without introducing additional .block? One way is to
describe it as inline text.

Keeping track of inline versus block markup worries me. We should get rid of
@.block and replace it with a submarkup thing. We need that anyway to support
substitution.

Now paragraphs are going to get tricky though. We lose track of where
paragraphs go in that case.

Anyway, first draft of markdown.fbld looks reasonable, with following
exceptions:
* Add \n as an allowed escape character.
* Implicit .block I fear is causing problems.
* How to describe indent for code?
  We can't do it with direct substitution anymore like we could before.

---

For code indent, my draft uses:
* @ifeq[a][b][then][else]
* @let[var][def][body]
* @head, @tail
* @inline
* @@ as end of same line arg to indicate final arg.

I definitely want \n as an escape sequence.

What are the next steps here?

The biggest concern I have right now is how we handle and track @.block in the
right places.

Say we get rid of @.block and add a submarkup kind of markup. We parse plain
block structured text as a submarkup. When we execute a command, it returns
markup which we install as a submarkup.

An alternative would be to flatten all submarkup. When we execute a command,
it returns markup which we flatten immediately into the list where the
command was executed. This suggests inline text would be flattened into block
text, which might not be a bad thing.

It doesn't change the fact that sometimes we want to introduce paragraph
markers and sometimes not.

The reason we want implicit paragraph markers is to avoid cluttering the text.
We care about cluttering the text much more for end user docs than libraries.
So what if we avoid all use of @.block in libraries? Use explicit tag instead?

That sounds good to me. Remember, @.block is only introduced for plain block
structured text. Not for explicit block structured text.

This suggests we do want a notion of @.block: some special tag to denote plain
text as distinct from general grouping of submarkup that would arise from
command substitution. Think of @.block as an implicit tag added by the parser,
not part of the fundamental structure of markup. Which is really what it is:
syntactic convenience so we don't have to put @par tags everywhere in the docs.

Now then, for substitution, do we want to flatten everything in a list after
substitution, or do we want to maintain hierarchical structure? I vote for
flattening to start. If we really want to maintain structure, we can do it
with explicit commands. Otherwise I think it will be easier to deal with a
single canonical form for markup.

We have decisions made now for how to try to proceed:
* Keep Markup@ structure as already defined.
* Keep @.block tag as already used.
* Add terminating ' @@' as final arg indicator to parser.
* Implement evaluation with lexical scoping for everything. No support for
  free/unbound variables in this first attempt.

---

I have implemented basic evaluation with @define. Next step: get README.fbld
markdown conversion happy under fbld2.

First issue:
* Use of \ in code. It says unsupported escape sequence, as it should.

I see two approaches here:
1. Manually escape the \ character.
2. Add syntax for raw text.

I think I'll want syntax for raw text eventually, so let's do that first. What
did I propose before for this?

* End a line with @ to say the following next line arg is passed as literal
  text without being parsed as block structured text.
* End a line with @@@ to say the following is literal final arg.

This is going to get confusing... and definitely not intuitive in any way. You
have to know the magic of '@' versus '@@' versus '@@@'.

Start with just '@' for now I guess.

The syntax feels plenty nice to me.

---

Okay, basic issue with evaluation.

    @define[foo][a b] @a,@b @@
    @define[a][] AAAA @@
    @foo[@a][there]

The question is: do we substitute into args before calling commands?

In the above, I expect the third argument to the first @define, which is
'@a,@b', not to substitute into @a or @b. But the first argument to @foo,
which is '@a', I expect to substitute into @a.

How can the evaluator tell the difference between these?

Maybe a better example:

    @define[a][] AAAA @@
    @define[foo][a b] @a,@b @@
    @foo[hello][there]

What should this result in? 'AAAA,there' or 'hello,there'?

I know I want it to result in 'hello,there', because @a should bind to the
parameter to @define[foo]. But again, evaluator doesn't know that. It doesn't
know that @define interprets some args as parameters that capture variables
used in other arguments.

And thus the spiral continues.

I suppose we could say it's the job of the callee to decide when substitution
happens into its arguments. It has the environment it needs to replicate what
the caller would have done, I think. So, in this case:

@define
 - eagerly evaluates name, param list.
 - evaluates def symbolically in context where @a, @b are defined as @a, @b?
 - evaluates body in context where @foo is defined.

@foo
 - eagerly evaluates all arguments?

    @define[a][] AAAA @@
    @define[c][] CCCC @@
    @define[foo][a b] @a,@b,@c @@
    @foo[hello][there]

@define
 name: env
 params: env
 def: env + @a: @a, @b: @b
 body: env + @foo: ...

@foo
 all args: env

Maybe that could work.

---

Next steps:

* Add '@' literal next line arg syntax.
* Define @ifeq, @let, @head, @tail

I'll also need to figure out how to transition from tcl fbld and fble fbld1
over to fbld2, given it has slightly different syntax for the documents.

Let's get README.fbld to work manually to markdown first. Then to html.

At that point I think we can start migrating docs one at a time over to fbld2.

---

README.fbld -> README.md via fbld2 works!

That's awesome.

Issues still to be addressed:
* Need for \\ instead of \ in code.
* Definitions of @FbleVersion and @BuildStamp

Let me work on an html.fbld for fbld2 next.

---

I tried writing html.fbld. It's so much nicer in fbld2 than it was in fbld1.
This is looking real good to me.

---

Challenge with literal next line arg:

@a
 @b @
  literal text here

 Plain text here.

The blank line between literal and plain text... does it count as literal text
or as separator between the @b command and the plain text?
 
From what I wrote, I expect the blank line not to be included in blank text.

What if we don't have the blank line at all, is that valid?

@foo
bar
@sludge

That should be valid, right?

Then what if we want the literal text to end with blank lines? Do we need to
look at the indent level to tell?

---

Next issue: how to easily have @ in inline text. Do I want to use \@
everywhere, or have some nicer way to have literal text?

For example: @l[@], @l[@emph], @l[@code], ... is used a lot in my docs.

Proposal: use {} for literal inline arguments. For example:

@l{@}, @l{@emph}, @l{Bool@}

No need for special same line arg. We can use inline arg instead:

@title{Bool@ Types}

Or:

@title Bool\@ Types

I don't know. Think about it.

---

Next question, as we convert over tutorials. How to handle 1 versus 2 arg
@fbld?

We have 6 variations to consider:

{@url, @file, @fbld} x { simple, labelled }

There are two reasonable approaches to take if we want to support fixed number
of arguments.

A. @url, @file, @fbld all take two arguments, where the label can be empty to
denote use of a default label.

   See the @fbld[Bind.fbld][Bind] tutorial.
   See @fbld[spec/fble.fbld]{}

   At @url[https://ruhler.github.io/fable][GitHub]
   At @url[https://ruhler.github.io/fable]{}

B. Two different names for things. For example, @lfbld.

I already like A much better. Let me go with that.

---

Next issue with using fbld2 for tutorials: some of them seem to take forever
to convert and run out of memory. Is this poor performance, or some kind of
bug?

I should run with perf based profiling perhaps to see? Or prof based profiling
of a smaller example?

Let's start by minimizing the Lists.fbld tutorial.

I see one place where parsing may get confused: @l{[...}]. Let's test that
first. If that's the issue, figure out where in the parser it gets stuck and
add a fix for that.

Looks like the problem is:

   @fbld[../spec/fble.fbld#lists][...]

The use of #lists in the argument to @fbld. Interesting.

Of course. Because we are missing the base case for @fbld where it doesn't end
in .fbld.

Not sure if there's anything useful I can do to report this error message. We
would want an fbld debugger for that kind of thing?

---

Now that tutorials are ported over to fbld2, what's next? Brainstorm:
* Port rest of .html files over to fbld2. Remove fbld_html_doc and html_doc.
* See if we can improve performance of fbld2.
* See if we can remove fbld1 entirely and replace it with fbld2?
* Figure out how to do syntax highlighting.
* Carefully review generated documentation for issues.

Let's hold off on converting the rest of the tcl based fbld over to fbld2 yet.
I want to phase out fbld1 first.

---

fble.html runs out of memory, I think just because it wants to use a little
more memory than I have. It works okay using 70% memory if I remove the @code
block for concrete syntax reference.

Any way to see why it's using so much memory?

Let's start with fble based profiling and see if that gives any hints.

It says all our time is spent in looking up commands in the environment. Odd,
because I don't expect there to be too many entries in the environment. Maybe
just because we are calling so many commands.

I wish I could take a heap snapshot and see what that looks like, to know
where all the memory is from.

---

Some analysis shows it's all related to HtmlEsc. If we don't do that, then we
are okay. And specifically it's all from HtmlEsc in @code blocks.

I bet I can reproduce pretty easily by just having one big code block with
html escaping.

Yes. I can reproduce the memory blowup easily that way. It should be O(N)
transformation, right? Are we somehow holding on to all suffixes of the
string?

perf shows a lot of time in list append and cons. That must be where the
allocations are coming from. Let me look at fble profiling next to see where
the calls to Append and Cons are coming from.

---

It feels very much like a quadratic blowup. As if it has copies of all
suffixes of the string to escape in memory at the same time.

fble profiling shows all the time in list append, fairly evenly divided
between ToPlain and Eval.

ToPlain isn't being called that often, but list append is. That suggests we
are appending a long string on the left hand side. This could happen if I
build up the string backwards?

@EscHtml[...]
==>
@head[...]
@EscHtml[@tail[...]]
==>
@head[...]
@head[@tail[...]]
@head[@head[@tail[...]]]
...

I need to trace through how this gets expanded to see what's in memory where
and what string appends we are doing. That should make clear the problem.

@define[EscHtml][str]
 @ifeq[][@str][] @@
 @head[@str]
 @EscHtml[@tail[@str]]
@@

To execute the @define, we add to the environment @EscHtml. That function,
when executed:
* Evaluates its arguments in the calling environment.
* Evaluates the text of the definition in the lexical environment with args
  added to the environment.

@EscHtml[abcd...]

Evaluate plain text argument abcd... Nothing to do.
Evaluate
  @ifeq[][abcd....][] @@
 Converts abcd... to plain text

Note that conversion from abcd... to plain text appends a very long string to
an empty string. Maybe we want to add a special case for that in Append to
avoid the linear cost of it.

 Next we eval
  @head[abcd...]
  @EscHtml[@tail[abcd...]]

We start with the tail.
  Evaluate the @tail first.
   Converts abcd... to plain text (linear time).
   Creates a new string from that. Note that it does not share memory with the
   first string! Because we did an append to the empty string and created a
   separate copy of the original string.

  @EscHtml[bcd...]
   Repeats.

Thus we see the problem. Conversion from markup to plain text is creating a
copy of the string. At each level we get a separate copy of a suffix of the
string. Thus quadratic memory explosion.

Easiest fix, I think, is to optimize Append to work better if the second
argument is empty.

Another option would be to avoid this 'ToPlain' approach for builtins and
instead operate directly on markdown structured text. Like, you can take head
and tail easily enough without having to look over all the markdown. You could
do if comparison at the markdown level.

Three options:

* Add optimization to Append.
* Add optimization to ToPlain.
* Add optimization to builtins.

The first is the most general. The third is the most significant. I think we
should do the third. Append is already known to be slow for long first
arguments. We shouldn't add some weird special case that makes the performance
difference between empty b and non-empty b so large.

Cool. I bet this helps performance of evaluation a bunch. We'll be back to
limited to performance of the parser I expect.

Yes. That solved the memory blowup issue.

---

Next step: round of performance review/improvements for fbld. I think fble.fbld
to html is the big one. It takes a couple of minutes. Ideally it would be less
than 15 seconds.

After we do a round of performance improvements, we should look at the
generated html in detail and fix any issues. For example, in any places
missing required html escaping.

---

Profiling:

./pkgs/fbld/fbld --profile fbld.prof -- ./fbld/version.fbld ../fbld/nobuildstamp.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > foo.html

Looks like 75% of the runtime is in the list Ord function, primarily from map
lookup calls, from looking up the 'impl' function for a command.

We have about 100K lookup calls. Looks like on average each lookup requires 5
iterations. That suggests, on average, around 32 elements in the map. That's
consistent with the number of entries I have defined in the global scope for
the html backend.

I would guess a lot of the time is spent distinguishing between the @EscHtml
and @EscHtmlChar functions.

There are a number of different places we could try to improve:

* the fble runtime implementation
* the char ord function
* the list ord function
* the map lookup function
* the fbld evaluation strategy
* the html.fbld functions

The most promising, I think, would be to change the fbld evaluation strategy
to do the lookup when we first define a function, instead of repeatedly for
each invocation of a function.

For example:

@define[EscHtml][str]
 @ifeq[][@str][] @@
 @EscHtmlChar[@head[@str]]
 @EscHtml[@tail[@str]]
@@

We can do the lookups for @ifeq, @head, @tail, @EscHtmlChar and @EscHtml once
in the body of the function when @EscHtml is defined. No need to repeat the
lookup on every call to the function. Then we can reduce the size of the
environment for the body of the function to just the arguments.

It means traversing the definition of a function an extra time, but hopefully
function definitions are relatively small?

Practically speaking, the idea would be to update @Command with an extra
field: Maybe@<Impl@>. On initial parse, you have Nothing. In defining a
function, we'll do a 'Resolve' traversal that looks up as many commands as it
can, but does not yet apply them. It can do that on the environment the define
is from. Eval will look to see if a command has already been resolved and
reuse that if so.

This would be slightly awkward because it ties up evaluation and Markup@. I
wonder if eval should have its own data structure ResolvedMarkup@ that it
should use during the evaluation process instead.

Or, I could make this all internal. Add a separate Resolved@ data type. Eval%
provides methods to convert Markup@ to Resolved@, and then later evaluate
Resolved@. No need to change the existing top level Eval interface. We can use
this Resolved@ thing to accelerate the @define builtin specifically.

One thing I notice from linux perf based profiling is a lot of list
allocations. And from fble based prof, a lot of calls to Int| from the Ord
function for Char@. It would be pretty easy to factor those calls out of Ord
so we only do the Int| conversions once, not for every character comparison.

Or, I suppose, we could implement Ord for Char@ precisely, instead of
converting to Ascii to do the comparison?

Looking more closely, Int| shows up as 20% of total runtime from linux perf,
and 60% of total runtime from fble profiling.

Cool. Let's start by fixing that:
* Ascii should not call int helper every time.
* If it makes sense, see if ordering for char can be implemented directly
  instead of via conversion to Int@?

Making that change resulted in a 25% performance improvement in this case.

Now prof still shows calls to Int|, mostly from the builtins checking the
right number of arguments. perf doesn't show this as an issue.

---

I suspect two potential issues contributing to long runtimes of fbld:
1. Cost of repeated lookups of commands in env.
2. Quadratic blowup from appending Markup@ lists in eval.

Both should be fixable. (2) we should absolutely do. (1) feels like we
shouldn't have to do if fble was fast enough.

What does profiling suggest for (1) vs. (2)?

prof:
  Lookup: 35% of runtime
  Append: 32% of runtime

perf:
  Lookup: 10% of runtime
  Append: 52% of runtime

I trust perf more, and suspect object allocation is always the more expensive
thing, which happens with (2). Let's start with (2).

Two approaches we can take:
* Eval takes markup to prepend to.
* Change Markup@ from List to Tree structure.

I claim we want the second approach: make it possible to embed the tree
structure in the Markup@ representation itself. Specifically so we can deal
with arguments to functions. We should be able to pass arguments to functions
in pre-flattened form, and be able to append those wherever they are used in
the function in constant time.

This has the downside that there is more than one way to parse a list. Let's
just declare our canonical approach is: (., (., (. , ...))).

Change done. It's a 60% performance improvement. We're done to about 1 minute
now to process fble.fbld.

Now profiling shows:
prof:
  50% command lookup
perf:
  52% parsing
  37% eval

---

Let's talk html backend correctness. We need to escape html characters. I
claim it's not possible to do correctly the way things are currently set up.

Consider:

@section The @l{Bool&} type.

The argument to @section, after evaluation, assuming @l does proper html
escaping, would be something like:

  The <pre>Bool&amp;</pre> type.

We can't escape this again, because it would print the <pre> tag as a literal
string instead of a tag. We must escape this, in case "The " or " type." need
escaping.

The proposal is to do what we do for plain inline text the same as we do for
plain inline blocks. Bring back @.inline. Use it for all plain text.

For example:

@section The @l{Bool&} type.

Is parsed as:

@section[@.inline[The ]@l[@.inline[Bool&]]@.inline[ type.]]
 
In the implementation of @section and @l, we turn off @.inline.  
 
@define[l][text]
 @define[@.inline][]
 @ <code>@EscHtml[@text]</code>
@@

But this gets tricky, because to turn off @.inline, we need to use plain text.

Could we have some magic for @define where it automatically removes @.inline
and @.block from the environment the definition is executed under?

Or, could we have explicit commands to do @.inline and @.block substitution?
Something like:

  @plain[block][@par] @@
  @plain[inline][@EscHtml] @@
  ...

Which applies the relevant commands at all points in the argument before
evaluating the argument?

Meaning we would end up with @EscHtml for things like @code language and @ref
ids?

Or: have commands like @section explicitly call @plain on their arguments
before evaluating those arguments?

...

If we parsed without @.block or @.inline.

  Hello @there you.

  What is @up now?


[['Hello ', @there, ' you.\n'], 'What is ', @up, ' now?\n']

Plain text is very easy to identify.

Paragraphs are more difficult. Consider:

 Hello there now.

Versus:
 
 Hello 

 there 

 now.

The only difference in how they parse is the presence of newlines

  ['Hello there now.\n']

versus:

  ['Hello \n', 'there \n', 'now.\n']

A better example:

 Hello
 there
 now.

Versus:

 Hello 

 there 

 now.

They would parse identically without @.block, when I clearly intend for them
to parse differently.

Another example to consider, say @foo is defined as a paragraph and @bar is
not:

  A

  @foo

  B
 
  @bar
 
  C

How do we get paragraphs around @foo but not around @bar? Is it meaningful to
say @bar isn't a paragraph?

---

Reminder of how things work in tcl-based html backend:
 - Commands parsed their own arguments.
 - Outputs of commands were never used as inputs again.

What we have now:
 - Arguments are parsed and evaluated before going to commands.
 - The result of commands are used as inputs to other commands.

That's what makes it harder to keep track of when something should get an
implicit @.inline or @.block tag.

The parser can track @.inline/@.block structure. The good(?) thing about that
is it only applies to the initial text, never to evaluated text.

But we would still want a mechanism for commands to choose what to do with
that structure.

So say we have the @.inline/@.block structure captured as part of the Markup@
representation. Consider @section for example. The title argument.

We know the title argument is Markup@. It's a sequence of plain text /
commands. When we evaluate the argument, we want to say: plain text gets
evaluated using some default command @EscHtml, explicit commands get evaluated
as is.

The order of operations is the hard part. Because we may want to do some
evaluation of arguments before passing them to @section, which would lose the
information about plain vs. command.

Unless plain always comes from the parser, and never a command itself (or you
need to do something differently for the result of a command to be considered
plain).

This goes back to the original vision: pass @.inline as a free command in the
arguments to @section.

@section[Hello& @l{Unit&}]

==> Parse injects @.inline:

@section[@.inline[Hello& ]@l{Unit&}]

==> Evaluation of commands in scope of caller: @l

@section[@.inline[Hello& ]<code>Unit&amp;</code>]

==> Evaluation of commands in scope of callee: @.inline

title = Hello&amp; <code>Unit&amp;</code>
...


Is it safe to partially evaluate markup? For example:
  Evaluate X under environment E1
  Evaluate result under environment E2

The only challenge would be if there is some command available in both E1 and
E2. Right now I require E2 be empty. If a command is in both, should it take
from E1 or E2?

It should take from the caller environment, because that's the only one
technically reasonable? Because we may have no idea what a command is when we
first evaluate its arguments. We would have no way of knowing if there was a
conflict.

So, would that work here? Evaluate arguments in the callers environment first,
allow 'free' commands. Have a way to evaluate those free commands explicitly
from the callee's point of view, perhaps with an '@eval[env][markup]' call.

---

Taking a step back from the question of html escaping, because it sounds hard
and I don't want to get stuck on it right now.

What else is there to work on fbld?

* Syntax highlighting
* @usage -> man page
* @usage -> help text header
* doc comment -> man page

Let's start with @usage -> man page? To replace ::man_usage.

The pieces of it:
* @doc -> man page.
* @usage -> (man) @doc
  - @usage, @synopsys, @description, @options, @opt, @exitstatus, @examples, @ex
* usage lib
  - @GenericProgramInfo, @ModuleInput

Eventually we'll call these:
* man.fbld
* usage.lib.fbld
* usage.man.fbld

We can do that now.

I will say, defining the man.fbld backend is really nice syntax wise. This is
a definite improvement in the fbld language over the original tcl based
version, assuming I can figure out the html escaping issue.

The other thing I need is fbld support for config.fbld. How hard is that?

config.fbld isn't hard. I already had it all set up.

@usage -> man conversion was pretty easy. The biggest hold-up is having to
support both the old and new syntax together for things that aren't all
converted to the new syntax. Let's prioritize getting everything over to the
new syntax.

Next: header_usage. I expect it to be as easy to convert as man_usage.

---

Annoying thing is: we have a cycle: fbld depends on fble-compile, for example,
but fble-compile depends on fbld for -h usage text.

I can break the cycle by checking in a golden copy of usage text for all the
relevant binaries. It's a bit of a pain though. Oh well. Let's see if/how much
of it I can automate.

---

The last use of tcl-based fbld is dc.man.tcl. This will be an important test
for the new fbld syntax, because it traverses the structured text twice:

1. Gets a list of args and return values from $content.
2. Generates the description from $content.

We want to interpret @arg and @returns separately depending on case (1) or
(2). That means interpreting it after passing it to the @func command.

For (1), I also want a way to make plain text and other irrelevant commands go
away.

This needs thought.

---

Feels like there's only one obvious way to go?

* Allow free commands. To evaluate them, keep them unchanged.
* Have some way to (re)evaluate an evaluated markup in a different environment
  where previously free commands are now defined.
 - Either with an explicit @eval command, or just as part of @foo in a new
   environment.
* Have some way to define a '@plain' command that is called on all plain text.
  Perhaps by default it acts as the identity.

Now define @func as:

 @man[3][@name][@FbleVersion (@BuildStamp)]
  @section[NAME]
   @name - @brief
  @synopsis[@content]
  @section[DESCRIPTION] @brief\n\n@description[@content]

Where @description is defined as:

  @define[arg][type name desc] @l[@type] @a[@name] @desc @@
  @define[returns][type desc]
   @section[RETURN VALUE]
    @definition[@l[@type]][@desc]
  @content

And @synopsis is defined as, approximately:

  @define[arg][type name desc] 
  @define[returns][type desc]
   @section[RETURN VALUE]
    @definition[@l[@type]][@desc]
  @section[SYNOPSIS]
     @l{#include <fble/fble.h>}

     @define[arg][type name desc][] @@
     @define[returns][type desc] @l[@type] @@
     @define[.plain][text][] @@
     @eval[@content]

      @name@l{(}
     @tail
      @tail
       @define[arg][type name desc][@l[, @type] @a[@name]] @@
       @define[returns][type desc][] @@
       @define[.plain][text][] @@
       @eval[@content]
     
      @name@l{);}
  
With the only awkward bit figuring out how to insert the comma between args.
Hmm... Maybe always but the comma, just strip it out from the front. Let me
backport that to the above code.

Awesome. So, changes needed:
1. Allow free commands, don't error out. I suppose we'll error out at some
other place for actually not defined commands?
2. Add an @eval command that evaluates its argument in the current
environment.
3. To eval plain text, check for @.plain in the environment. If there, call
it, else leave text as plain.

Now, assuming we do these things, does that solve the problem of html escape,
or help in any way?

Maybe, any command that generates html is considered free in the argument to
@doc. The @doc command replaces all plain text with a call to html escape. All
other commands assume the input is html sanitized?

---

Okay, here's everything sorted out:

* Allow unknown commands in eval. If encountered, skip evaluation of them.
* Do not evaluate arguments to unknown commands. It's up to the command itself
  if it wants to evaluate arguments when you run it.
* Have an explicit @eval[...] command to evaluate markup in the current
  environment.
* Allow a @.plain command to be defined that gets applied to plain markup
  during evaluation. Default for @.plain is to leave the plain markup as is.

The solution for HtmlEscape is as follows:

* Continue to define doc commands at the top level, e.g. @section, @a, @l, @code.
* Instead of outputting html directly, like <i>...</i>, output the html tags
  using a free command specifically for that. For example:
    @_html[<i>]...@_html[</i>]
* The argument to @doc will now be a sequence of elements that are either
  plain text, or @_html. Evaluate this in the context where @.plain is
  @HtmlEscape. That gives you back a sequence of elements that are either
  html-escaped plain text or @_html. Note that the arguments to @_html are not
  processed by the @.plain, because evaluation doesn't go into arguments to
  free commands.
* Evaluate the result in a context where @_html is identity function. The
  result is proper html.

---

@.plain is going to cause problems. If we define it using @define, the first
thing it will try to do is evaluate its arguments in scope, which will invoke
@.plain.

Can we come up with a better way for processing plain text? Perhaps a builtin
function specifically for transforming plain text?

@plain[a]
 ...
@@
...

Says substitute all occurrences of plain text using the definition with @a for
the plain text in the body. I think I like that better.

Almost makes you wonder, should we make Block part of the syntax and define an
@block[a][...][...] command too? I kind of like that idea. No need for magic
.block appearing in the parser tests. We make Block more of a first class
syntax component like it should be. By default we can treat it like plain text
in the way it is concatenated, but users can convert where they want to
otherwise.

How would we modify, say, html and man backends for this? I think we get
better control with explicit @block regardless.

---

I ended up changing @plain to something slightly easier to implement:
@plain[f][body], where f is the name of a command to apply to all plain text
in body.

---

Implementing dc.man.fbld... it's so close, but pretty messy. Specifically:

A. The @i command can't be implemented in the new syntax. So the source will
  need to be updated somehow.
B. For some reason putting description of an argument on the next line messes
  things up?
C. Paragraphs in the body of the description aren't being stripped away from
  the synopsys. I don't understand why.
D. Changing the order of man.fbld versus dc.man.fbld makes leads to errors I
  don't expect.

---

E. If we don't evaluate into args, previously bound arguments can turn into
   free arguments which is really bad?

For (D):
  This is expected. dc.man.fbld defines @func which assumes @man is defined.
  @func is executed in the scope where it is defined. If we defined @func
  after @man, we would get the error about @man not defined exactly like we
  do.

Let's think about (A) a little. Here's the example:

  @sideeffects
   @i The returned value must be freed with @l[FbleReleaseValue] when no
    longer in use.
   @i Prints an error message to stderr in case of a runtime error.
   @i Updates profiling information in profile based on the execution of
    the program.

An alternative:

  @sideeffects
   @i
    The returned value must be freed with @l[FbleReleaseValue] when no longer
    in use.
   @i
    Prints an error message to stderr in case of a runtime error.
   @i
    Updates profiling information in profile based on the execution of the
    program.

An alternative:

  @sideeffects
   The returned value must be freed with @l[FbleReleaseValue] when no longer
   in use.
   
   Prints an error message to stderr in case of a runtime error.
   
   Updates profiling information in profile based on the execution of the
   program.

An alternative:

  @sideeffects
   @i[The returned value must be freed with @l[FbleReleaseValue] when no longer
   in use.]
   @i[Prints an error message to stderr in case of a runtime error.]
   @i[Updates profiling information in profile based on the execution of the
   program.]

An alternative:

  @sideeffects
   @i The returned value must be freed with @l[FbleReleaseValue] when no
    longer in use.
   @i[] Prints an error message to stderr in case of a runtime error.
   @i Updates profiling information in profile based on the execution of
    the program.

An alternative:

  @sideeffects
   @i The returned value must be freed with @l[FbleReleaseValue] when no
    longer in use.
   @i Prints an error message to stderr in case of a runtime
    error.
   @i Updates profiling information in profile based on the execution of
    the program.

In general it's not great mixing same line and next line arguments for
different invocations, because that changes assumptions about whether this is
a trailing newline or not.

An alternative:

  @sideeffects
   @i_ The returned value must be freed with @l[FbleReleaseValue] when no
    longer in use.
   @i Prints an error message to stderr in case of a runtime error.
   @i_ Updates profiling information in profile based on the execution of
    the program.

It's kind of like @def versus @definition. Have @item versus @i?

But we would use @item for single arg. What to use for multi-line?

  @sideeffects
   @ii The returned value must be freed with @l[FbleReleaseValue] when no
    longer in use.
   @i Prints an error message to stderr in case of a runtime error.
   @ii Updates profiling information in profile based on the execution of
    the program.

Okay. Not ideal. But acceptable for now. Use @i for single argument, assumed
to be same line. Use @ii for same line + following lines.

---

Interesting consequence of (E): When I was defining @i after @func, it didn't
work as expected. Because @i is being passed as a free command to the argument
of @func, and @i isn't defined in the scope of @func. I had to define @i way
earlier. Not sure what I expected here.

On to (B). Why does putting description on the next line mess things up?
Simple. It's because when put on the next line, it's wrapped with @.block,
which makes it a separate paragraph, which is exactly the 'messed up' we are
seeing.

@arg turns into @def. @def currently assumes a same line argument, not a next
line argument. That means @arg assumes a same line argument.

We could change to @definition. But then that messes up for any same line
arguments we use.

Two big differences today between same line and next line args:
1. Next line is wrapped in @.block
2. Next line ends in a newline.

Consider these three:

  @foo Hello there.

  @foo
   Hello there.

  @foo
   Hello there.
  
   How are you?

Today that turns into:

  @foo[Hello there.]
  @foo[@.block[Hello there.\n]]
  @foo[@.block[Hello there.\n]@.block[How are you?\n]]

Keep that in mind for when I start thinking more about @.block.

---

Now on to (C). Why isn't the description getting disappeared by @synopsis?

The third arg to @func is read as:

  @.block[The program ...\n]
  @arg ...
  @arg ...
  @arg ...
  @returns ...
  @sideeffects ...

When we evaluate @func, it simplifies that based on the environment to:

  .P\nThe program ...\n\n
  @arg ...
  @arg ...
  @arg ...
  @returns ...
  @sideeffects ...

Next we say to ignore plain text. That text should have been ignored. What's
going on here? Let's see what the input to @plain looks like in this case.

I see what happened. We were not evaluating the argument to @plain first. So
we were passing @plain[@content], and @content is not plain, it gets mapped to
@content, we then eval on that.

I should add a builtin test case for this.

We're so close now. The only issue left for the generated man page for
FbleEval.3 is that extra comma in front of the first arg in the synopsis. I
don't know of a clean way to get rid of it.

Let's think about (E) too, because this one scares me.

Here's what I'm thinking...

  @define[f][a]
   @g[x@a]
  @@
  @f[hello]

If @g is free, this returns:
  @g[x@a] to an environment where @a is not defined.

The answer would be to do eval inside arguments to commands that aren't
defined. But I fear evaluation into recursion or any other commands that don't
normally evaluate their arguments first. Is it just @ifeq? I think that should
be okay actually, because we know the @ifeq function and wouldn't end up
evaluating into its arguments unnecessarily. The only recursion functions we
have are ones defined with @define, which always evaluates alls its arguments
first.

Thus the proposal is, when an unknown command is encountered, evaluate its
arguments in the current environment. The only things that will be left are
free commands.

Is there an alternative to free commands? They make things very confusing. For
example, don't we really just want a way to encode lists or other structure in
text?

Imagine I had builtins for example:

@list_cons[head][tail]
@list_empty
@list_head
@list_tail

I'm not sure. Something to think about.



