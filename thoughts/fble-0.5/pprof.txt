Pprof
=====
For discussion of the UI that I want.

Here's a concrete use case I feel like I want:
1. View the list of blocks by percent of time the program spent in or under
   that block.
2. Maybe we could merge together similar blocks in the list so there is less
   to look at.
3. Be able to easily 'remove' a block from that list. Which removes any
   samples that have that block anywhere in it.

So I collect together a list of interesting blocks by removing one after the
other. Now I have a high level summary breakdown of where time is going. Then
I want to be able to focus at one of those blocks at a time. Rinse and repeat?

---

I'm concerned that my perf approach is truncating stacks and not including all
the samples. Is it possible for me to write my own perf that does exactly what
I want? Anyway I can review how perf is implemented?

The implementation looks tough. But maybe I can figure out enough to parse the
output of 'perf script' directly. It looks like it is a sequence of traces. We
clearly have the timestamp. I'm just not sure what 'cycles' means. Looks like
it's a timestamp of some sort rather than a sample count. I think it's safe to
ignore for performance purposes.

Yes, so best opportunity at the moment would appear to be to parse each record
from the output of 'perf script'. Read and process the thing however I
prefer. Don't worry about the 128 frame limit for now.

---

Let's figure out the nicest output we can get from perf script.

Today it looks like:

<exe> <pid> <timestamp>.<nanos>:    <n> cycles:u:
              <addr> <name>+<offset> (<so>)
              ...

Any easy way to simplify timestamp perhaps? Or it doesn't matter? We don't
care about the header line anyway, aside from being able to recognize it.

Any way to get rid of the .so? I don't care about that, right?

I don't see any options to turn things off. Let's just parse what we get by
default. That's simplest, right?

We only care about function names, or do we care about where in the function?
Where in the function sounds nice, doesn't it? Or is that too much detail?
Unless I'm going to look into the address, let's strip it for now.

---

The truncated stacks are bothering me. Let's try:

  --call-graph dwarf,65528 

Option to perf record. See if that does anything interesting.

It's still truncating the stacks. Looks like I'm getting 256 at most.

Oh well. I'll have to find some other way to work around that annoyance.

---

Showing the lists by overall time and by self times is a really useful entry
point.

Here's the start of the overall time list:

   60.43%    15997 FbleCall
   45.77%    12116 TailCall
   45.49%    12040 _2f_Core_2f_List_25__2e_Append_21_.001f
   36.54%     9671 FblePopFrame
   34.69%     9183 GcRealloc
   32.32%     8556 NewGcValueRaw
   23.57%     6239 IncrGc
    9.04%     2393 FbleStructValueField

* FbleCall and TailCall should be ignored here
* /Core/List/Append points to a big chunk of time worth investigating.
* FblePopFrame, GcRealloc, NewGcValueRaw, IncrGc all point to same chunk
  of code being run.
* FbleStructValueField points to an interesting thing to look into.

Here's the start of the self time list:

    9.04%     2392 FbleStructValueField
    8.47%     2242 FbleCall
    6.67%     1766 FbleUnionValueTag
    6.21%     1643 Get
    5.26%     1392 FbleUnionValueField
    5.12%     1355 TailCall
    4.91%     1301 _int_free
    4.36%     1155 _2f_Core_2f_List_25__2e_Append_21_.001f
    4.20%     1112 _int_malloc
    3.60%      952 MoveTo

All of these are worth looking into individually. I wonder what 'Get' is?

Anyway, that's really useful top level info. What do we want next level to be
able to better understand? Let's look at /List/Core/Append% for example.

It would be nice to see:
1. Who is calling this. Is it one big path, a single benchmark? Or multiple? If
   it's multiple, how is the time divided between the callers?
2. If I focus on calls starting here, how does time break down into callees?
   What's the expensive part of this call?

For (1), I can break down by stack trace leading up to the append call, but
it's not very enlightening: all except three start in append, because of the
truncated stacks. I really need to get rid of those somehow. It messes
everything up.

The other three are:
* Fbld Result Do
* FannkuchRedux Flips
* Md5 Bits Lit

With no real indication of how much is contributing to each. We should be able
to tell from an fble profile run of the benchmark, because it doesn't truncate
stacks.

For (2), is it a straight up recursive problem? Filter out samples that
include the append call, drop all entries leading up to the first, then show
overall and by self?

* The overall tells us we spend 54% of the time calling FblePopFrame here.
* The self tells us 12% FbleStructValueField, 10% 'Get',
  etc.

I think it would be nice to count sample stacks and show all those too?
Assuming they don't get too long?

I feel like it would be awesome if I could compact a single trace by
recognizing cycles. For example:
  a(bc)*de*f

To cover any kind of trace like:
  abcbcbcdeeeeef

Then list the traces by count.

Thus I propose a single, useful view that you can then zoom in and filter on:
* by overall breakdown
* by self breakdown
* List of sample traces going into.
* List of sample traces starting from.

We can start by looking at the 'root' page.
If you click on an overall entry, it filters to traces with that entry and
focuses there. Multiple subsequent selections filter down more and more.

I would be interested in trying to compress and report all the samples as a
big list. See if anything interesting comes up. How do we do that?

---

Note: the --call-graph dwarf,65528 let me see inside of inlined functions,
which is kind of nice.

---

Where I'm at right now with pprof.py:
* Overall section is very useful.
  Skip over any obvious things, like start, or FbleCall.
  Anything else that stands out as you go down the list is a good candidate
  for investigation. This starts the investigation.
* Self section is very useful.
  Go down the list. All of these are good candidates for investigation.
* Canonicalization of traces.
  I think it works well. No loss of information, makes things more compact.
  Good.
* Listing traces:
  Hard to navigate, but contains some important information about who is
  responsible for calling a frame of interest that we don't get elsewhere.
  Having chains is useful here, because sometimes just one back doesn't give
  enough context.
* Tree of traces:
  Hard to navigate. I'm not convinced it's useful.

So I know how to identify frames of interest. The difficulty comes from there.
Things I want to know after I have a frame of interest:

* Who calls the frame. Are there different cases to consider? If so, which
  case is the biggest case I should consider first.
 - Note that looking a single frame back is not always enough, but looking too
   far back also makes things difficult. Somehow we want to see just enough
   context for the caller to distinguish between interesting cases.

Let's say I magically solve the context question. Is there anything else I
need? Anything else I want?
* I suspect the ability to recursively investigate:
  We start at the root.
  Then we say we are interested in A.
  Within A, we want to say we are interesting in B, but still show that
  relative to A, not too root.

  This should be easy. We just specify a sequence of frames of interest. Each
  frame in tern filters out samples.
* Do we want a breakdown going out of a frame like we want a breakdown going
  into a frame? Or is Overall / Self focused on a frame sufficient for that?

It seems like there are two different ways to view incoming or outcoming
edges:
* The Overall/Self breakdown, doesn't look at direct edges, but focuses on the
  biggest cuts.
* Context traces looks at specific edges.

If both are useful, presumably both are useful both directions: incoming and
outgoing.

I know that just having 'overall/self' doesn't give me information about who
immediately calls a function, which I think is useful in some cases.

---

Idea: Focus on sequences of frames rather than individual frames.
* Keep canonicalization of samples. I see no loss of information there.
* Count occurrences of every observed subsequence of frames.
  Make sure to count each subsequence at most once per sample.
* We can do Overall, Self based on that if we wanted.
  Though I assume the longer the subsequence, the smaller the overall value.
  So we can skip this if it's too expensive.
* We can do a graph view based on that.
  A mini section for each subsequence.
  It shows frames leading into the subsequence, broken down by frame.
  It shows frames going out of the subsequence, broken down by frame.

One more thing we want is filtering. You can partition any profile by a
subsequence into samples with the subsequence and samples without. So, your
filter is a list of subsequence of samples to keep and a list of subsequences
of samples to reject.

The graph might be expensive. If samples are limited to 128 in length, that's
O(128^2) * N possible subsequences (though we expect much less in practice I
hope).

An implementation approach we can try:
1. For each sample:
 i.   Canonicalize the sample.
 ii.  Compute the set of subsequences of the sample.
 iii. Increment overall count of subsequence ==> count mapping by one for each
      subsequence in the set from the sample.
2. For each subsequence in the overall set:
  incoming[subsequence[1:]][subsequence[0]] = count
  outgoing[subsequence[:-2]][subsequence[-1]] = count

Sort subsequences by overall count.

Now we can show:
* Overall by subsequence. Optionally limiting to length 1 subsequences
  depending on what's useful. Maybe better would be limiting to subsequences
  taking up more than 1% of overall time?
* Self by subsequence (meaning when the subsequence is the tail). Maybe
  limiting to subsequences with self time greater than 1%?
* Graph of subsequences like how we do current profile report.

Basically the same as our current profile report, except:
* We canonicalize samples first.
* We track based on subsequence instead of frame.
* We add the option for filtering based on a set of subsequences to accept or
  not.

To get a feel for how useful it is, no need to add filtering to start.

Now, how I expect this to be used:
* Go down list of overall or self to find a subsequence of interest.
* To see who is calling into that subsequence, search in the graph for the
  subsequence, traverse up it. We have as much context available to use as we
  want.
* To see what the subsequence is doing, search the graph for the subsequence,
  traverse down it. Or, consider filtering on the subsequence first, then
  repeat the flow from the beginning.

I think this will be good. I think it will be expensive. We can limit in
various ways based on needs. For example, choose a subsequence first, then
only show graph for nodes starting or ending in that subsequence.

Note that this removes the need for separate canonicalized sample section:
that will show up in 'overall' and 'self' views with subsequences. I think
it's an improvement over the tree view, because it focuses on a single level
at a time in the graph view, which is what I really care about, and it let's
you view the tree from the root, the leafs, or anywhere in between you may be
interested in.

Next step: draft the pprof.py code for this in a small sample profile and see
how it looks.

---

Apparently it's very expensive to do all these subsequences in python. That's
concerning. Except it's not the sequences that are super expensive, it seems
like it's something else. I'm a little confused, because I don't think it was
this slow before.

Yeah, something is definitely wrong. I was able to do the same thing way
faster before.

I see what I did wrong. I forgot to clear the frames after each sample. Okay.
Let's try again then.

Yes, now performance is okay getting counts of all the subsequences.

Next step is to try producing the graph view.

---

The graph view is going to be good I think. Some requested improvements:
* Need an easier way to link to a specific sequence.
* Figure out a better way to highlight the sequence in the graph entry.
* Consider limiting incoming and outgoing edges with N, % like elsewhere?

Honestly, if I can turn this into a webserver with links that shows a single
graph entry at a time, that would be awesome I think.

---

I see three directions I can take this profiling stuff:
1. pprof.py as a webserver. Iron out the interface to make it useful.
2. pprof implementation in fble. Move towards what we have in pprof.py.
3. profiling in runtime. Collect samples instead of aggregated counts.

(1) is presumably most important, because it tells us exactly what we need.
(2) is interesting because it gives us a more meaningful case study to try (1)
out with. (3) can wait. So let's go in order.

---

Proposal for pprof.py website:
* /overall page - gives the list broken down by overall frame.
  Each frame links to the graph node for that single frame.

* /self page - gives list broken down by self frame.
  Each frame links to the graph node for that single frame.

* /graph&id=... - gives a single graph node
  List the stack trace to that point.
  List the incoming edges. Edge links to trace with that incoming added.
  List the outgoing edges. Edge links to trace with that outgoing added.

Try that. See how useful it is.

Oh, also, I feel like I want a kind of self time for a sequence. That would
be: time spent in the sequence not covered by incoming or outgoing? Not sure
that makes sense. Don't worry about it for now.

Step 1: overall page. How do we do http server in python?

---

I got the http server going now, with Overview, Overall, Self, and graph
views. I think this is good. There are some navigation issues to work out,
where I want to go to a specific subsequence without an easy way, and there
are opportunities to provide more information. But it feels good.

I can pretty easily see the tree view I was hoping for, the breakdown by
benchmark within fble-benchmark. Maintaining the context of traversal is what
makes that work, because we can go through FbleCalls without losing the
context.

Cool. It's in good enough shape now to try using it for something for real
before making more changes.

Two things next:
1. Re-implementing pprof in fble.
2. Updating fble profiling to track and output samples instead of aggregated
report.

(1) should be pretty straight forward. I assume we'll instantly run into
performance issues, but that's fine. That's what we need to figure out how to
fix.

(2) is going to be tricky. I think it's doable. The main challenge is I think
we need to support on-the-fly canonicalization in order to handle potentially
unbounded tail recursion. Single frame canonicalization isn't hard. But
multi-frame loops need some way to track where we came from and where we
should go back to when popping frames.

The good news is, (2) isn't blocked on (1). I can switch over entirely to
pprof.py now. Or, at least, once I've used it enough to verify it's not
missing something major we used to have. But, honestly, it's not, right? It
has all the same information we had before in the graph, just more of it.

So, what say you? Time to rewrite pkgs/pprof?
