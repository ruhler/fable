Foreign Function Interface
==========================

Simple: we provide a way to call a foreign (aka C) function from fble. The
compiler generates the logic to call it for you.

The abstract syntax is:
  foreign <type> <name>

This is an expression of given type implemented in C (or whatever foreign
language your compiler is targeting) named <name>.

Any reasonable way to do type checking to make sure what your calling
implements the type it claims it does?

And on the C side, provide a standard header for writing functions that
compute Fble values. Like I already have now, just slightly more abstracted
and formalized to work with different implementations.

---

Foreign function interface on the C side, I think should be, for now:

  FbleValue* <function>(FbleValueHeap* heap, FbleProfile* profile);

But we want this to be called once to set up the profile blocks. It would be
bad if it was called repeatedly and kept adding more and more to the profile.

Oh well. Let's not worry about that for now. To start I'll just make sure I
don't call an ffi expression in the body of a function. They'll all be module
level variable declarations anyway.

I almost want the whole foreign function expression to be a separate module,
to guarantee it's only called once. Or maybe each FFI expression is treated
like a module reference, and loaded as part of module initialization. Yeah,
that sounds good. Doesn't matter where you have it, we arrange for it to be
called once at link time.

This needs to be made clear to the user, in case it's something expect to run
a long time. But it is a static thing, in the sense that you can't pass
arguments at FFI expression time.

The implementation is simple for C and aarch64 backends I think. Add an FFI
instruction? Hmm... If we are doing things at link time, then no instruction
needed, but we would need some metadata to tell the compiler about it.

Maybe the easy way to start is to do things at FFI expression eval time
instead of module link time. Then all we need is a new FFI instruction with
the name of the thing and the destination for the value. C and aarch64
backends are easy: generate code to call the function...

Except we don't have access to the profile then. Just the profile thread? Is
that enough?

Anyway, the point I was trying to get to, is interpreter is harder, because we
have to look up the symbol somehow. One idea I had to get a hacky prototype
working is to pass a map from name to function to the interpreter. When it
sees the instruction, it looks in the map to get the function to call.

That doesn't work unless you have the FFI function built into the interpreter
though, which I don't want. I could get away with it for stdio, because the
interpreter is part of the same library. But that wouldn't let you add any FFI
in other packages to be used with fble-stdio. For example, it wouldn't work
for spec tests, which don't depend on studio, and would want to use contrived
FFI functions I don't want built into any implementation.

So the question is, how should FFI work in general? How do we bundle the FFI
implementation with the module? In particular, how to make it work in the
interpreter case?

The way I think of FFI is that we have two layers of abstraction going on
during execution: the fble layer, and the native layer. What is considered
'native' depends on the backend you are using. If it's C backend, then C code
is your native layer. If it's aarch64 backend, then aarch64 is your native
layer. If it's interpreter... then?

If it's C backend, is C code your native layer, or is it always machine code?
Because for interpreter, I could say it's the native machine code the
interpreter is implemented in.

Regardless, native code of some sort is going to be involved. Precompiled
native code, that is built separately from the interpreter. How can I pass
that to the interpreter? Using an elf file on linux.

Now things start to get tricky with different platforms. Let's not worry about
that right now. Let's focus on linux.

In the compiled case, we already have an elf file associated with the fble
module. That's your package level elf file. Perfect place to store your FFI
implementation. And you can reference it directly from there.

In the interpreted case, today all we have is .fble code. There is no
associated package level elf file. Then what can we do?
A. Provide a list of .so files with symbols to use for FFI to the interpreter.
B. Allow mixing of interpreted and compiled code, and require compiled code
for FFI.

(A) feels weird from a usage point of view? Or I suppose we could have
Foo.fble.so in the search path, and load that anytime we see it with
interpreted code for the sole purpose of looking up FFI. Yeah, that could
work. But also annoying to have to mix architecturally independent fble code
with architecturally dependent FFI implementation.

(B) seems more natural from a usage and distribution point of view. How would
it look in practice? It's related to modular compilation.

Ooh. (B) is the same mechanism we would want for being able to distribute
binary code without source for some packages. That's a uniform approach to
take. It's something we'll want eventually anyway. Which suggests (B) is the
way to go. Kill two birds with one stone.

How fble package distribution works today:
1. For the interpreter case: you have a package directly with all the .fble
files, each with full source.

2. For the compiled case, you have a .so file for each package that you link
with at compile time.

Today there is no mixed case. Either every package is compiled, or no package
is compiled. That's what I want to add. How would it look?

You are running an interpreter. It isn't pre-linked with all the compiled
packages you need. It needs a way to find and load that code. There are two
things it needs: the implementation of the modules (for execution), and the
types of the modules (for use).

I can imagine two different ways to provide the types:
1. Using .fble files with a special syntax to allow elision of the
implementation.
2. Compiled into whatever .so file we have with the rest of the code.

Option (1) is more geared towards user documentation. So that we can give nice
error messages and clear description of what the type should be. Option (2)
seems safer, in the sense that it's harder to mess with than it would be to
mess with the .fble files. For user sanity, some form of (1) will be needed.

Let's say we go with (1). We could always fall back to (2) later. We could do
both (1) for the user and (2) for the trust. But for now, just (1).

This means, in the package search path, for a package, we'll want to have:

* A tree of .fble files, one for each module.
* A .so file containing the code for the module.

You know what's interesting about this? It looks like it could support FFI
usage mode (A). The .fble files could contain the implementation and the .so
file could contain just the FFI code.

Anyway, what are the semantics? Can we use the same approach for interpreter
and compiler?

Think of the tree of .fble files as the header files. Think of the .so file as
the library file. This is straight forward, right?

When compiling a module against the package, use the .fble files to get the
type.

When linking your compiled code, use the .so file. This just tells you were to
look for that.

When loading the package from the interpreter, we just need a way to figure
out if we should read the .fble files for bytecode to interpret, or we already
have compiled code available. This can be on a per-module basis perhaps. That
sounds reasonable. Maybe you want to ship a mix of interpreted and compiled
code in a package, rather than have all one or another.

Which means, when loading a particular module, first, look for the
FbleGeneratedModule symbol associated with that module. If you can't find
that, then fall back to loading bytecode from the .fble file.

Can we go the other way? Can we link compiled code against interpreted code?
How to deal with dependencies between packages?

I think, maybe the directory structure we want is:

/usr/share/fble/<package>/Foo.fble
/usr/lib/fble/libfble-<package>.so

Anyway, what this means is, for the interpreter case, when we want to load a
module, we look for the .so file first, try to dynamically load it, before
falling back to the .fble file.

---

First step towards foreign function interface: switch from static libraries to
shared libraries in the build system. In preparation for being able to load
those libraries dynamically in the interpreter use case.

Start with linux. Worry about Windows separately.

Today we make .a files, e.g.:

  lib "$::b/lib/libfble.a" $objs

Based on:

proc lib { lib objs } {
  build $lib $objs "rm -f $lib ; ar rcs $lib $objs"
}

Okay, so what are the commands to build a shared library from a bunch of .o
files instead? Sounds like we need to add -fPIC when compiling C files. Then
use:

gcc -o foo.so -shared $objs.

Let's try one step at a time.

One concern is that we use the static libraries by passing them as if they
were object files. Is something else needed now? Seems like maybe not.

I'm getting errors like:

/usr/bin/ld: ./pkgs/satgame/SatGame/Generate.fble.o: relocation
R_AARCH64_ADR_PREL_PG_HI21 against symbol `stderr@@GLIBC_2.17' which may bind
externally can not be used when making a shared object; recompile with -fPIC

Add -pic to as call?

---

Let's go step by step for this .so change, to make sure I understand what's
really needed.

Step 1: Build libfble.so. Done. No issues.
Step 2: Use libfble.so in the bin/ binaries.

* If we directly replace the libfble.a argument with libfble.so:

Works, but only if I call it from the out/ directory. Otherwise we get an
error:

./out/bin/fble-compile: error while loading shared libraries:
./lib/libfble.so: cannot open shared object file: No such file or directory

Same thing after I install to the target location.

https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html

Reading up on shared libraries:
* -rpath option to ld sets a relative path to search for the .so file.
  - For example: -Wl,-rpath,$(DEFAULT_LIB_INSTALL_PATH)
* In theory I should pass -fPIC to gcc when compiling the .o files that go
  into a shared library.
* In theory I should run ldconfig after installing the library.
  - ldconfig -n directory_with_shared_libraries

I just need to figure out the approach for running locally and installed. Use
some existing package for example, that ships its own .so file. The question
is:

* Can I run the binary before it's installed? If so, how does it find the
  library it needs?
* Can I run the binary after it's installed to my custom directory? If so, how
  does it find the library it needs?

Then try to mimic that approach.

---

Looking at freetype as an example.

Unfortunately, I don't think freetype builds any binaries. Maybe sdl instead?

How about sudo? It looks like that builds .so and binary.

Notes:
* `objdump -x <foo> | grep PATH` is supposed to show the rpath of an
  executable. I see none in fble-compile. I wonder how it finds libfble.so
  then?
* readelf -d <foo> shows the path of the library needed. In my case, it shows
  ./lib/libfble.so. That explains what it's looking for anyway. 
* ldd shows not found if the library wouldn't be found at runtime.
* If I do '-l fble' when compiling fble-compile instead of 'lib/libfble.so' as
  an object file, then it shows up as 'libfble.so' in the executable.
* ./sudo doesn't work in the build directory. It says it can't find
  libsudo_util.so. It has 'libsudo_util.so.1' listed as the library from
  readelf.

---

This shared library stuff sounds tedious and tricky.

Anyway, my goals are:
* Need to be able to build from tools before installing.
* Would like to be able to run tools before installing without passing special
  arguments or environment variables.
* Need to be able to run tools after installing without passing special
  arguments or environment variables.

Static libraries easily satisfy all these requirements. How should we handle
it for dynamic libraries?

The things we can manipulate:
* The absolute/relative path to the library when building the executable.
  e.g. gcc -o foo foo.o ../../libbar.so
* The LD_LIBRARY_PATH
* The DT_RUNPATH attribute of the in the readelf -d section.
* ldconfig cache.
* The default search path.

It sounds like the path to the directory and DT_RUNPATH are two separate
things. Let me confirm experimentally.

Yes. For example:

 0x0000000000000001 (NEEDED)             Shared library: [libfoo.so]
 0x000000000000001d (RUNPATH)            Library runpath: [/home/richard/scratch/so]

NEEDED comes from how I pass the .so to gcc
 - Either as a path to libbar.so, or using -lbar
RUNPATH comes from -Wl,-rpath,<path> option to gcc.
 - And can be specified multiple times.

Every installed binary I see on my system sets NEEDED without an / in it, so
using the -lbar option to gcc. It depends on the library being found in the 

There is potentially an option to modify the executable when installing. Let
me see if I can figure out how to do that.

Yes, via: chrpath or patchelf. But I don't have those installed on my system.

Or, we could create two separate versions of the binary, the 'local' one, and
the 'to be installed' one. Though that feels like a pain.

How does it work if I install a package and executable to a local directory?
Do I have to set LD_LIBRARY_PATH? I feel like I normally wouldn't? Let's try
that.

Note that when calling dlopen, I can pass an absolute path to the .so file
to open, so I can use my own search logic for that.

sqlite is interesting. It distributes a library and binary, but the installed
binary is statically linked with the library rather than dynamically linked.
Can I take that approach? How would that look? What needs to be dynamic vs.
shared in fble?

Libraries to consider:
* libfble, libfbletest, for lib for each package.

How about we statically link with libfble and libfbletest. We can make .so
files out of the package libraries and use explicit dlopen for all of them?
They can assume the binary has been statically linked with libfble, so we
avoid duplicates of that in process. It just means duplicate across binaries,
but I'm okay with that?

* libfble.a is 1.8M
* libfble.so is 1M
* fble-deps, fble-disassemble, etc are around 25K each when dynamically
  linked. They are around 200K to 600K when statically linked.

It would be really nice if we could dynamically link, to save on space.

If I'm willing to be hacky, how about build them with:
* rpath $ORIGIN/../lib

That way they always work, no need for LD_LIBRARY_PATH. The user can't specify
separate bindir and libdir anyway at the moment, so should be okay, right?

Yeah, that works. Just tedious to escape the mix of tcl, ninja, and shell:

  -Wl,-rpath,\\\$\$ORIGIN/../lib

\$ to escape tcl
$$ to escape ninja
\$ to escape shell

All combined: \\\$\$

Next step on the journey: get rid of libfble.a entirely. Switch everyone over
to libfble.so. Just so I don't have to worry about things anymore with regards
to that.

Trouble: pkgs/fbld/bin/fbld is installed to bin/fbld. Aside from that being
wrong in general (we should be installing the C implementation there), it's
problematic because $ORIGIN/../lib isn't the right value to use there. Should
be the same for any of the binaries built as part of fble packages.

---

Let's start with two relative runpaths: relative for build and relative for
installed. I don't think it matters what order we do them in.

