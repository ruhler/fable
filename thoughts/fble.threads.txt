Fble Threads
============
The goal is to redesign how threads are done in the fble implementation.

Recently we changed function calls to preserve the native stack, which makes
debugging more useful. I've done a proof of concept with getcontext/setcontext
for implementing threads which looks like it will work great.

The goal is to have each fble thread runs on its own native stack, rather than
all share the same native stack. The benefits:
* We can remove these pc jump tables everywhere in generated assembly.
* We open up the door for future optimizations of generated code where we
  store more context on the native stack instead of the managed stack.

I expect minor performance improvements, from no longer having to go through
pc jump tables and avoiding function call overheads when switching between
threads. But nothing significant performance wise.

As a side goal, it might be nice if we have a low level threading abstraction
that we can implement with getcontext/setcontext or pthreads, for example so
that we could reuse pthreads for gdb thread debugging.

Current Threading Design
------------------------
Each fble thread has a managed stack allocated on the heap that keeps track of
the chain of functions called and current pc within the function. An fble
function takes pc as an argument, making it possible to resume from within the
pc. The managed stack stores all the information necessary to run a thread.
When we want to suspend a thread, we unwind the native stack completely. When
we want to resume a thread, we execute functions from the top of the managed
stack one at a time, jumping to the recorded pc address.

Any thread can signal an abort. In this case, we abort the threads by calling
each function's 'abort' entry point passing the current pc.

The thread scheduler runs each thread in turn, recording whether a thread is
blocked or not. Then the thread scheduler runs the user supplied IO function.
A flag signalling whether all threads are blocked or not is passed to the IO
function, so it can block waiting for user input in that case. To track if
threads are blocked, we also ask each thread to update a variable if they
perform any io activity, in which case we say not all threads are blocked.

If all threads are blocked and the io function makes no progress, execution is
aborted with "deadlock".

The managed thread keeps track of how many child threads a thread is blocked
waiting on. The JOIN instruction waits until that number drops to zero. A
thread is responsible for decrementing the child count of its parent thread
when it exits.

Proposed Threading Design
-------------------------
* Threads should be responsible for aborting themselves. If a function needs
  to abort, it cleans itself up and returns ABORT to the caller. If a callee
  function returns ABORT, the caller aborts itself. If a sibling thread
  aborts, it should exit with ABORT, which other threads are notified of when
  they resume, and then they can abort.
* When a thread wishes to suspend, it Yields. It will need to indicate whether
  it is blocked or not. Yield will return if the thread should abort or not.

We can run IO on the main thread. That seems natural. So the main thread would
be something like:

  bool blocked = false;
  while (threads.size > 0) {
    if (!io->io(io, heap, blocked) && blocked) {
      // deadlock
      Abort notify threads and wait for them to clean up.
    }
   
    blocked = true;
    Yield();
  }

This assumes round robin scheduling, which is more than I would like. So how
about we keep track of a count of the number of threads blocked and compare
that to the number of current threads to know if all the threads are blocked.
And the total number of threads tells us if all the threads are done or not.
And a global flag to say if we are aborted or not.

Threads should remove themselves from the thread list when they are done
executing. Which means they will want to know where they are in the thread
list.

I think the current approach for blocking parent threads on children threads
is fine: keep a count of children threads on the managed stack, yield
control from the parent thread as long as there are children, and decrement
the children count when a child thread exits.

Potential Future Threading Design
---------------------------------
In the future I imagine we'll want better synchronization between threads. So
no busy loops, track exactly what each thread is blocked on. A thread could be
blocked on:
* A child thread running (from fork/join).
* A put on a port (from get).
* A get from a port (from put).

In that case it would be nice to store a thread id of some sort on a link to
know what threads are blocked waiting on it, and perhaps to move a thread to a
'suspended' state so it would not be scheduled. We could avoid scheduling
suspended threads this way, and remove them from the 'suspended' state when we
do a put/get or are the last child of the parent thread.

That suggests we could have two lists: running threads and suspended threads.
If we keep that up to date, we instantly know if all threads are blocked or
not, and how many threads have yet to complete.

Low Level Thread API Proposal
-----------------------------
Thinking about something we could do that would work for getcontext/setcontext
and pthreads and potentially a future where we implement proper synchonization
and want to support pre-emptive scheduling...

* Sched* NewScheduler()
  Allocate a new scheduler.
* void FreeScheduler(Sched*)
  Free resources associated with the given scheduler. Will return on the
  current thread. All other threads will be deleted and never resumed.
* void Create(Sched*, func, arg, stacksize)
  You give a function to run and a user argument. It spawns a new thread that
  will run that function with that user argument. Maybe you also give the size
  of the stack desired. It returns an id for the newly spawned thread. In the
  case of cooperative multithreading, the new thread is doesn't take over
  control until Yield() is called.
* void Yield(Sched*)
  Yield control of the current thread to some other runnable thread tracked by
  the scheduler. If there are no other runnable threads, a deadlock error is
  reported and the process is aborted. Scheduling is fair in the sense that
  all runnable threads will eventually be scheduled as long as the current
  thread continues to yield periodically.
* SuspendId Suspend(Sched*)
  Remove the current thread from the list of running threads and returns an
  identifier for the suspended thread. The thread will not be scheduled to run
  again until explicitly resumed with Resume. Suspend may return a different
  identifier for the same thread if invoked repeatedly.
* void Resume(Sched*, SuspendId id)
  Add the thread with the given suspend id back to the list of running
  threads. In the case of cooperative multithreading, the calling thread
  continues to run after this call. The resumed thread does not take control
  until it is scheduled from a call to Yield like normal.

Where Suspend/Resume can be implemented as part of V2. Note that the user of
the threading library is responsible for keeping track of the number of
threads currently running/suspended if it needs to know that information.
Threads clean up resources for themselves when they exit.

The Sched* tracks state and is required to avoid interference in the case when
you want reentrant thread schedulers. For example, if an fble IO function
calls Eval, which needs to spawn its own thread scheduler and run threads
without allowing threads sibling to the IO function to be run.

Proper Threading Proposal
-------------------------
I feel like there is a "proper" way to implement threading, which is what we
want in the longer term and solves the issue of knowing when all the threads
are blocked. It's just a bit annoying to implement. Let me write down the
proper threading proposal first and worry about the implementation later.

Threads can be blocked for the following reasons:
* Parent thread blocked on children to finish running.
* Blocked on Get from link.
* Blocked on Get from IO port.
* Blocked on Put to IO port.

In each of these cases, it is clear when the thread should be unblocked: when
the last child thread completes, or you do a corresponding put/get on the
link/port a thread is blocked on.

Conceptually we would like:
* A list of currently running (non-blocked) threads.
* A list of currently blocked threads (needed to easily track blocked threads
  in case we need to abort them).
* For each link, a FIFO list of threads blocked on get.
* For each IO port, a FIFO list of threads blocked on the port.
* A child thread knows what its parent thread is and how many children of the
  parent thread are running.

When a thread becomes blocked, it removes itself from the list of currently
running threads, adds itself to the list of blocked threads, adds itself to
whatever link or port it is blocked on if any, and suspends itself.

When a thread would unblock another thread, it moves that other thread from
the list of blocked threads to the list of running threads, removes it from
whatever link/port it is listed as blocked on, and resumes it.

We only ever schedule threads that are currently running. To avoid wasted
effort resuming blocked threads. This will help threading scale to a large
number of blocked threads.

We know if all threads are blocked by checking if there are any threads on the
list of currently running threads.

For data structures:
* We want to be able to remove from the middle of the list of running threads
  in constant time.
* We want to be able to remove from the middle of the list of currently
  blocked threads in constant time.
* We want FIFO order constant get and put from link and IO port thread lists.

There are two reasonable ways to support removing from the middle of a list of
threads: store them as a vector and swap with last element when removing, or
store as a doubly linked list.

To preserve FIFO order, it's not great to swap with the last element when
removing.

So this suggests we want threads to belong to a doubly linked list. We can say
they belong either to the list of running threads or the list of blocked
threads, and their order in blocked threads is FIFO and grouped together based
on the link/port they are blocked on.

Each link will have a pointer to the first and last of the threads blocked on
it. Each port will have a pointer to the first and last of the threads blocked
on it.

The memory cost is:
* Extra 2 pointers per link.
* Extra 2 pointers per port.
* Extra 2 pointers per thread.
* Less 1 pointer per thread for the thread vector.

That's really not so bad. Links and ports are presumably relatively rare, and
threads have the overhead of a stack allocated with them anyway.

We can use non-circular linked lists.

It's not clear to me there is any worthwhile incremental approach to
implementing the proper threading proposal described here. May as well just do
it all in one go. Note we can do this with our current native stack reuse
scheduler. We don't have to switch to separate native stacks per thread yet.

Review of Concerns
------------------
The big concern with switching to a different native stack per thread is
having to bound stack sizes ahead of time and the overall memory cost. For
example, if we pick 1 MB per thread, then some threads may run out of stack
space and we may end up allocating 1 GB memory for 1K threads that is mostly
unused.

We don't have this problem today, because all threads share the main stack.

The thought is, the kernel is already using virtual memory, so even if we need
a lot of virtual memory address space, we'll only be changed physical memory
for what we actually use. We may as well take advantage of what the kernel is
already giving us. Assuming that really works out in practice.

And I can't figure out a nicer way for different stacks to be interleaved in
memory, given that any standard C code we call assumes it has enough
continuous stack space to do whatever it wants.

Could we compromise? Have one shared main stack, but use getcontext/setcontext
to more efficiently unwind and rewind a thread's stack?

Upon reflection, I think it's better to reuse the main stack for all threads.
If the biggest implications are limited memory per thread and excessive
overall memory versus non-standard debugging/profiling support, I rather have
non-standard debugging/profiling support. I'm not convinced there are
significant performance motivations to switch.

We can get rid of the pc jump tables in the current design, we just need to
store native pc values instead of fble pc values. We'll also want to store
separate abort pc values in case of abort.

---

Note: this is obsolete now that we have removed processes from the language.
