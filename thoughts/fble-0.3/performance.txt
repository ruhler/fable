Performance
===========
Goal for fble-0.3 is to make fble programs run twice as fast.

Take fbld implementation for example, because that seems to go absurdly slow
compared to what I would expect if I coded the same thing in C code.

Let's start with a simpler example and see if we can really break down where
the time is going. Because presumably making a simple program run twice as
fast is easier than making a complex program, and anything we do to improve
performance we expect to work on the simple program too.

Starting with fble-cat, read from 'yes' and writing to /dev/null.

Let's aim for 60 seconds of runtime.

time yes | head -n 500000 | ./out/pkgs/core/fble-cat > /dev/null
real    0m38.297s
user    0m36.059s
sys     0m2.209s

time yes | head -n 500000 | cat > /dev/null  
real    0m0.104s
user    0m0.027s
sys     0m0.046s

That's what I'm talking about. fble is over 100 times slower than I would
like.

Summary from profiling:
  Memory Management     51.07
  Generated Code        16.99
  Value Utils           15.78
  Function Calls        14.33
  IO                     1.80

Memory Management 51.07
 5.05     8216  _int_free[0025X]
 4.47     7263  IncrGc.part.0[0021X]
 3.27     5326  malloc[0039X]
 3.87     6289  Refs[0028X]
 3.45     5610  FbleHeapRef[0022X]
 3.03     4929  FbleNewFuncValue[001fX]
 2.92     4741  cfree@GLIBC_2.17[0042X]
 4.11     6680  FbleReleaseHeapObject[0011X]
 2.96     4816  FbleRetainHeapObject[0015X]
 1.84     2997  FbleNewUnionValue[0010X]
 1.81     2951  FbleRetainValue[0016X]
 1.70     2765  FbleReleaseValue[0017X]
 1.55     2513  FbleNewStructValue[001cX]
 1.40     2284  OnFree[0048X]
 1.40     2271  _int_malloc[0040X]
 1.33     2157  FbleNewHeapObject[0020X]
 1.17     1902  FbleFreeExecutable[004fX]
 1.15     1867  FbleHeapObjectAddRef[0044X]
 1.14     1862  FbleReleaseValues[0043X]
 1.08     1754  FbleRawAlloc[0038X]
 0.57      924  FbleFree[0052X]
 0.44      715  FbleNewStructValue_[001bX]
 0.24      388  FbleNewIntValue[000fX]
 0.23      380  free@plt[0059X]
 0.20      331  malloc@plt[005aX]
 0.18      288  FbleNewStructValue_.constprop.7[0035X]
 0.13      215  FbleNewEnumValue[004cX]
 0.13      211  FbleNewListValue[0034X]
 0.08      126  FbleNewUnionValue.constprop.4[0049X]
 0.05       88  FbleNewStructValue_.constprop.6[0053X]
 0.12      203  MakeIntP[0045X]

Generated Code 16.99
 5.86     9532  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_.0010[000bX]
 1.90     3088  _2f_Core_2f_Map_2f_Map_25__2e_Lookup_21__2e_L_21_.005a[000aX]
 1.24     2020  _2f_Core_2f_Int_2f_Ord_25__2e_Ord_21_.0008[0012X]
 1.05     1714  _2f_Core_2f_Monad_2f_State_25__2e_Monad_21__21__21_.0011[0007X]
 0.68     1112  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_RoundLt_21_.0008[0029X]
 0.54      877  _2f_Core_2f_Monad_2f_State_25__2e_Monad_21_.000e[0036X]
 0.51      827  _2f_Core_2f_Monad_2f_State_25__2e_Monad_21__21_.0010[0026X]
 0.49      805  _2f_Core_2f_Stream_2f_IStream_25__21__2e_GetLine_21__21_.0016[001dX]
 0.44      716  _2f_Core_2f_Stream_2f_OStream_25__21__2e_PutStr_21_.000e[0023X]
 0.44      714  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_RoundGt_21_.000c[002bX]
 0.39      630  _2f_Core_2f_Char_2f_Ascii_25__2e_Ord_21_.000a[002dX]
 0.33      538  _2f_Core_2f_Stream_2f_IStream_25__21__2e_GetChar_21__21_.0011[0008X]
 0.32      519  _2f_Core_2f_Monad_2f_State_25__2e_Monad_21_.000b[0024X]
 0.31      508  _2f_Core_2f_Char_2f_Ascii_25__2e_Chr_21_.0075[0009X]
 0.27      438  _2f_Core_2f_Stream_2f_IStream_25__21__2e_GetLine_21_.0015[001eX]
 0.23      372  _2f_Core_2f_Stdio_2f_Cat_25__2e_Cat_21__2e_cat_21__21_.0017[002eX]
 0.22      357  _2f_Core_2f_Stream_2f_IStream_25__21__2e_GetChar_21_.0010[0037X]
 0.21      349  _2f_Core_2f_Monad_2f_State_25__2e_Monad_21__21_.000c[0027X]
 0.21      344  _2f_Core_2f_Stdio_2f_Cat_25__2e_Cat_21__2e_cat_21_.0016[0030X]
 0.19      315  _2f_Core_2f_Stream_2f_OStream_25__21__2e_PutStr_21__2e__3a__21_.0011[002fX]
 0.19      315  _2f_Core_2f_Maybe_25__2e_Just_21_.0007[003bX]
 0.18      297  _2f_Core_2f_Stream_2f_OStream_25__21__2e_PutChar_21_.000c[002cX]
 0.17      272  _2f_Core_2f_Stdio_2f_IO_25__2e_Run_21__21__21__21_.000f[0041X]
 0.15      248  _2f_Core_2f_Char_2f_Type_25__2e_IsNewline_21_.005a[004aX]
 0.14      233  _2f_Core_2f_Stdio_2f_IO_25__2e_Run_21__21__21_.000e[003aX]
 0.13      206  _2f_Core_2f_Stream_2f_IStream_25__21__2e_GetLine_21__21__2e__3a__2e__3a__21_.001b[003cX]
 0.08      132  _2f_Core_2f_Stdio_2f_Cat_25__2e_Cat_21__2e_cat_21__21__2e__3a__21_.001a[004eX]
 0.08      130  _2f_Core_2f_List_25__2e_Cons_21_.0008[003dX]
 0.04       68  _2f_Core_2f_List_25__2e_List_21_.000d[0051X]

Value Utils 15.78
 5.99     9741  FbleUnionValueTag[000cX]
 5.66     9212  FbleStrictValue[000dX]
 1.98     3217  FbleUnionValueAccess[0013X]
 1.68     2736  FbleStructValueAccess[0018X]
 0.24      390  FbleIntValueAccess[002aX]
 0.23      379  PackedValueLength[004bX]

Function Calls 14.33
11.58    18827  FbleThreadCall[0006X]
 2.75     4465  FbleFuncValueInfo[0014X]

IO 1.8
 0.32      525  _IO_fflush[001aX]
 0.21      339  __GI___libc_write[003fX]
 0.20      324  new_do_write[0033X]
 0.16      266  _IO_file_write@@GLIBC_2.17[003eX]
 0.15      245  OStreamImpl[0019X]
 0.14      227  _IO_getc[0047X]
 0.13      218  _IO_do_write@@GLIBC_2.17[0032X]
 0.13      209  fputc[0046X]
 0.11      184  IStreamImpl[000eX]
 0.11      171  _IO_file_sync@@GLIBC_2.17[0031X]
 0.05       76  fputc@plt[004dX]
 0.03       52  fflush@plt[0050X]
 0.03       41  0xffffffc010027968[0058X]
 0.02       27  fgetc@plt[005bX]
 0.01       20  0xffffffc01001041c[0060X]


Well, there you go. First thing to focus on is memory management.

In this case, it's all function allocations. Mostly from, I assume, the
State Monad.do call for the 'do' function for /Core/Monad/IO%.

There are multiple function allocations when you call do:
1. Do function is two argument function written as a function to a function.
Every time you call Do it allocates a function. This can be fixed by rewriting
it as a two argument function and adding monadic bind support to take
advantage of that.

2. The second argument to Do is a function that, when called, returns a
function which is the state monad.

3. The second argument to Do is itself a function that needs to be allocated,
and use of bind syntax causes it to be allocated every time the syntax is
executed.

Lots of function allocations coming out of monad bind syntax here. I could
rewrite fble-cat to avoid all use of monadic code. There should be no need for
any function allocations in the main loop. How do we reconcile that with the
desire to do monadic style programming?

The core loop is:

    Unit@ _ <- m.do(O.PutStr(stdio.out, line));
    cat(input);

Note:
* (Unit@ _) { cat(input); } is a constant function. No need to to reallocate
  it over and over.

Re-implementing cat without monads:

time yes | head -n 500000 | ./out/pkgs/core/fble-fast-cat > /dev/null

real    0m20.182s
user    0m18.387s
sys     0m1.825s

So, 38s ==> 20s. That's about our 50%.

I will note: the implementation isn't that bad:

  (World@) { R@<Bool@>; } Cat = (World@ w) {
    R@<Maybe@<Int@>> byte = stdin(w);
    byte.x.?(nothing: R@(w, True));

    Char@ char = /Core/Char/Ascii%.Chr(byte.x.just);
    R@<Unit@> out = stdout(/Core/Char/Ascii%.Ord(char), byte.s);
    Cat(out.s);
  };

Certainly way less obscure than going through IStream, GetLine, and PutStr
like my other version does.

Summary:
  Value Utils 29.37
  Memory Management 25.05
  Generated Code 23.78
  Function Calls 18.83
  IO 2.97

Value Utils 29.37
12.80    10803  FbleUnionValueTag[000aX]
 9.46     7985  FbleStrictValue[000cX]
 4.80     4051  FbleUnionValueAccess[000fX]
 1.84     1552  FbleStructValueAccess[001bX]
 0.47      397  FbleIntValueAccess[001eX]

Memory Management 25.05
 5.34     4502  FbleRetainHeapObject[000eX]
 4.31     3634  FbleReleaseHeapObject[0013X]
 3.61     3048  FbleNewUnionValue[0012X]
 3.08     2603  FbleRetainValue[0015X]
 2.56     2157  FbleReleaseValue[0016X]
 1.49     1255  FbleNewStructValue[0018X]
 1.03      873  FbleReleaseValues[001fX]
 0.84      708  FbleNewStructValue_[0017X]
 0.33      279  FbleNewEnumValue[002eX]
 0.33      279  _int_free[0024X]
 0.31      264  IncrGc.part.0[0023X]
 0.28      233  FbleHeapRef[002fX]
 0.27      230  FbleNewIntValue[0011X]
 0.23      192  Refs[002aX]
 0.21      178  malloc[002dX]
 0.19      159  cfree@GLIBC_2.17[0032X]
 0.16      135  MakeIntP[0028X]
 0.11       92  _int_malloc[0033X]
 0.10       88  FbleNewHeapObject[0022X]
 0.07       60  FbleRawAlloc[002cX]
 0.07       57  OnFree[0034X]
 0.05       40  FbleFree[003aX]
 0.03       29  FbleNewStructValue_.constprop.6[003eX]
 0.02       18  FbleHeapObjectAddRef[003dX]
 0.01        9  malloc@plt[0041X]
 0.02       13  free@plt[0040X]

Generated Code 23.78
11.77     9931  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_.0010[000bX]
 3.86     3257  _2f_Core_2f_Map_2f_Map_25__2e_Lookup_21__2e_L_21_.005a[0009X]
 3.65     3076  _2f_Core_2f_Int_2f_Ord_25__2e_Ord_21_.0008[000dX]
 1.33     1121  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_RoundLt_21_.0008[001aX]
 1.18      993  _2f_Core_2f_Stdio_2f_FastCat_25__2e_Main_21__2e_Cat_21_.0013[0007X]
 0.92      774  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_RoundGt_21_.000c[0019X]
 0.47      399  _2f_Core_2f_Char_2f_Ascii_25__2e_Chr_21_.0075[0008X]
 0.38      317  _2f_Core_2f_Char_2f_Ascii_25__2e_Ord_21_.000a[0020X]
 0.22      185  _2f_Core_2f_Maybe_25__2e_Just_21_.0007[0021X]

Function Calls 18.83
15.17    12800  FbleThreadCall[0006X]
 3.66     3088  FbleFuncValueInfo[0014X]

IO 2.97
 0.52      437  _IO_fflush[001dX]
 0.36      305  new_do_write[0027X]
 0.31      258  OStreamImpl[001cX]
 0.28      236  _IO_do_write@@GLIBC_2.17[0026X]
 0.27      229  IStreamImpl[0010X]
 0.23      191  __GI___libc_write[002bX]
 0.23      190  _IO_getc[0031X]
 0.22      188  _IO_file_write@@GLIBC_2.17[0029X]
 0.21      179  _IO_file_sync@@GLIBC_2.17[0025X]
 0.19      157  fputc[0030X]
 0.06       52  0xffffffc010027968[0039X]
 0.04       37  fputc@plt[003bX]
 0.03       24  fgetc@plt[003cX]
 0.02       15  fflush@plt[003fX]

Seems like understanding and eliminating this overhead of monadic style is the
first big step towards 2x performance improvement.

---

time perf record -F 299 -d -g ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld
./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld
> /dev/null

2m3.459s

* 39% time spent in FbleNewHeapObject.

How do I make that time go away?

First, check out where the allocations are coming from:

       1837     2438           FbleNewUnionValue[003aX]   
       3289     4393           FbleNewStructValue[0018X]   
       3400     4665           FbleNewFuncValue[0007X]   
**     8598    11572      180  FbleNewHeapObject[0008X] **

The biggest source of function allocations by far:
       1660     2374           _2f_Fbld_2f_Result_25__2e_Do_21_.0029[0006X]   

Which is from 'Do' being implemented as
  Result@<A@> -> (A@ -> Result@<B@>) -> Result@<B@>

Instead of:
  (Result@<A@>, A@ -> Result@<B@>) -> Result@<B@>

The biggest source of struct values:

        560      793           _2f_Core_2f_Map_2f_Map_25__2e_Rebalance_21_.0068[0023X]   
        699      881           _2f_Fbld_2f_Result_25__2e_Ok_21_.0014[0045X]   
        723      995           _2f_Core_2f_Map_2f_Map_25__2e_MapP_21_.0036[0034X]   
       1213     1626           _2f_Fbld_2f_Result_25__2e_Do_21__21_.002b[0001X]   
**     3573     4745      314  FbleNewStructValue[0018X] **

1. Allocating the Result@ to return from Do.
2. Allocating the non-empty map wrapper around Map@.
Almost entirely from rebalancing the map.
3. Allocating an 'OK' Result@.
4. Rebalancing the map.
Presumably allocating a bunch of @(delta, map) pairs, or maybe @(key, value)
pairs.

The use of Result@ is to help describe the program nicely. It should all be
constant time, not super expensive. Ideally free? This would be free in a C
implementation, I'm almost sure.

The rebalancing of the map is when we insert the arg values in the environment
before invoking an fbld command. This is more fundamentally difficult to
accomplish. But again, if data wasn't immutable, we could probably rebalance
in place and not need too many new allocations for this? Maybe, maybe not.


The biggest source of new union values:

        609      834           _2f_Core_2f_Map_2f_Map_25__2e_MapP_21_.0036[0034X]   
       1102     1435           _2f_Core_2f_Maybe_25__2e_Just_21_.0007[0040X]   
**     2046     2672      216  FbleNewUnionValue[003aX] **

Allocating Maybe@ value, sounds reasonable. But where from? Looks like
somewhere we are returning Result@<Maybe@> type.

Some of it is allocating Maybe@ for Maybe@<Impl@> to track if we have a valid
impl. My guess is HeadOf and TailOf returning Result@<Maybe@<Char@>>.

We could use a sentinel Char@ value, perhaps Unknown: 0 to avoid this extra
allocation for Maybe@<Char@>. In general, unions of unions could potentially
avoid layers of allocation if we merge tags.

Now, onto why those allocations are so expensive.

FbleNewHeapObject involves the following leaf calls, sorted into topic:

Gc 43%
**     2187     2976     2976  FbleHeapRef[001cX] **
**     3773     4980     1047  IncrGc.part.0[001bX] **
**     8598    11572      180  FbleNewHeapObject[0008X] **
**      674      809      809  Refs[0033X] **

FbleValueFree 16%
**     1219     1741     1741  OnFree[0024X] **
**      141      148      148  FbleFreeExecutable[005fX] **

Malloc 21%
**     1183     1924     1695  _int_malloc[000bX] **
**     1633     2434      516  malloc[000aX] **
**        8      229      229  malloc_consolidate[000cX] **
**       40       41       41  malloc@plt[0069X] **

Free 17%
**      930     1265     1264  _int_free[001dX] **
**      599      716      716  cfree@GLIBC_2.17[0035X] **
**       41       41       41  free@plt[006dX] **

FbleAlloc 1%
**     1669     2476      143  FbleAllocRaw[0009X] **

FbleFree 1%
**       86       93       93  FbleFree[0062X] **

The FbleAlloc/FbleFree wrappers cost us practically nothing. No need to worry
about those.

I'm a little surprised by the high cost of OnFree and FbleFreeExecutable.
Which OnFree call is that? Looks like it's OnFree for FbleValue.

That's crazy. OnFree does nothing but dispatch to FbleFreeExecutable. How can
it be so expensive compared to FbleFreeExecutable? I bet it's from trying to
access an object that is almost certainly not in the cache.

It feels to me like we spent 60% of the time for allocations traversing parts
of the heap that are not in the cache.

Consider two kinds of objects:

1. Short lived objects
The current GC has short lived objects stay around, putting pressure on the
heap, for an entire GC cycle. Then it traverses them to free them, smashing
the cache in the process.

2. Long lived objects.
The current GC traverses long lived objects every cycle, smashing the cache in
the process.

Imagine we had a stack-based allocation instead. We allocate at the top of the
stack. We free from the top of the stack. We copy objects down the stack as
needed. I claim, in this case, short lived objects stay in the cache the
entire time and don't put pressure on the heap once they are no longer used.
Long lived objects migrate down the stack and avoid smashing the cache.

I think stack based allocation is a good idea to try for those reasons:
* Avoid having short lived objects put pressure on the stack after they are
  done being used.
* Avoid smashing the cache to free short lived objects.
* Avoid repeatedly smashing the cache when traversing long lived objects.

Let's continue the discussion at stack_alloc.txt.

---

Minor optimization: avoiding a layer of indirection in the Obj list object
stored on FbleHeap:

time perf record -F 299 -d -g ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null

real    2m2.186s

Okay, so we save a second. Not much more. May as well. I think the code is a
little bit clearer this way anyway.

---

After switching to generational gc, down to 1m58s.

After fixing generational gc, down to 1m36s.

Allocations are still 27% of time in fble.html generation. But since the
improvement in generational GC, I want to look elsewhere for a little while
before coming back to that.

FbleStrictValue shows up as almost 10% of total time. Anything we can do about
that? Let's discuss at strict_value.txt.

---

Improvements to FbleStrictValue got us down to 1m29s.

What's next? I notice a lot of time spent releasing values. And, in
particular, FbleReleaseValues.

With some investigation, the top caller of FbleReleaseValues is in List Ord
function, releasing the tail call buffer.

Consider the following code:

    Ordering@ ordering = ord(a.cons.head, b.cons.head);
    ordering.?(eq: Ord_(a.cons.tail, b.cons.tail));

The first line is a normal function call. The second line has a tail call. The
first line will end up having to invoke the second.

What's sad here is that, in order to do the tail call, we do:
  retain Ord_
  retain a.cons.tail
  retain b.cons.tail

Then, after invoking the tail recursive call, we do ReleaseValues on those.

Any way to avoid this extra retain/release? I know they aren't needed, because
the caller is holding on to Ord_ and a and b. But I guess we don't know that
in general.

This is expensive, because a.cons.tail and b.cons.tail are likely not to be
roots otherwise. So we do the most expensive retain and release there is on
them.

You know what's interesting? If this was a non-recursive call, we wouldn't
have to pay the cost for the retain/release. I'm tempted to try changing that.

---

Next round of analysis since adding support for partial function application.

Some top contributors by self time:

Memory Management
 6.35     1918  FbleReleaseHeapObject[0004X]
 5.27     1594  IncrGc.part.0[0007X]
 5.16     1560  FbleReleaseValues[002eX]
 4.70     1421  _int_free[0008X]
 4.41     1333  _int_malloc[000fX]
 4.24     1283  FbleHeapObjectAddRef[0030X]
 3.42     1035  FbleRetainHeapObject[003bX]
 3.21      971  malloc[000eX]
 2.47      747  cfree@GLIBC_2.17[0009X]
 2.30      695  FbleRetainValue[0039X]
 2.23      675  __memcpy_generic[0031X]
 2.17      657  Refs[0025X]
 1.61      486  FbleNewStructValue[0005X]
 1.32      398  FbleNewHeapObject[0006X]
 0.91      275  FbleAllocRaw[000dX]
 0.90      272  FbleNewUnionValue[0034X]
 0.78      236  OnFree[0035X]
 0.73      222  FbleNewFuncValue[000cX]

Function Calls
 6.63     2005  FbleCall[0001X]
 6.27     1896  Call[0002X]

Value Manipulations
 4.10     1238  FbleStructValueField[000aX]
 3.66     1106  FbleUnionValueTag[0033X]
 3.62     1095  FbleFuncValueFunction[0038X]
 2.87      866  FbleUnionValueField[002cX]

Compiled Code
 3.93     1189  _2f_Core_2f_Int_2f_IntP_2f_Ord_25__2e_Ord_21_.0010[001dX]
 1.04      313  _2f_Core_2f_Char_2f_Ascii_25__2e_Ord_21_.000a[0037X]
 0.90      271  _2f_Core_2f_List_2f_Ord_25__2e_Ord_21__2e_Ord_5f__21_.000a[001cX]

Clearly memory management is still a big factor. Function calls after that.

Let's look at FbleCall and Call first, to see if anything obvious shows up in
the profile.

The one thing that stands out in the profile for FbleCall is the chained
memory load: func->executable->num_args.

We can see in this case that the functions being called the most are fbld
builtin @ifeq, Eval of sequence, defined command implementations.

Not too sure what we can do there, aside from reduce the number of function
calls made in the fble program in question.

Back to the memory. Where are all our allocations coming from now?

Breakdown by type:
       1472     1993           FbleNewUnionValue[0034X]   
       1325     2200           FbleNewFuncValue[000cX]   
       2807     4509           FbleNewStructValue[0005X]   

Structs:
        130      192           _2f_Core_2f_List_25__2e_Cons_21_.0006[005cX]   
        137      217           _2f_Fbld_2f_Parse_2f_M_25__2e_Return_21_.0021[0054X]   
        274      386           _2f_Fbld_2f_Parse_2f_M_25__2e_Do_21_.0027[0024X]   
        365      597           _2f_Core_2f_Map_2f_Map_25__2e_Rebalance_21_.0052[002aX]   
        456      701           _2f_Core_2f_Map_2f_Map_25__2e_MapP_21_.0030[0032X]   
        663      892           _2f_Fbld_2f_Result_25__2e_Ok_21_.0012[0044X]   
        759     1494           _2f_Fbld_2f_Result_25__2e_Do_21_.0020[0003X]   
**     3183     5055      486  FbleNewStructValue[0005X] **

Mostly Result@ allocations.

Functions:
         42      102           Call[0002X]   
         71      103           _2f_Core_2f_Map_2f_Map_25__2e_Insert_21__21_.009d[0055X]   
         57      124           _2f_Fbld_2f_Builtin_2f_If_25__2e_IfEq_21__21__21_.0021[0019X]   
         73      128           _2f_Fbld_2f_Builtin_2f_If_25__2e_Equals_21__21_.0016[003dX]   
         70      135           _2f_Fbld_2f_Builtin_2f_If_25__2e_IfEq_21_.001f[0022X]   
         49      136           _2f_Fbld_2f_Builtin_2f_Define_25__2e_Define_21__21__21__21__2e_impl_21_.003c[000bX]   
         70      138           _2f_Fbld_2f_Builtin_2f_If_25__2e_IfEq_21__21_.0020[002bX]   
        100      141           FbleCall[0001X]   
         85      173           _2f_Fbld_2f_Builtin_2f_If_25__2e_Equals_21_.0015[0018X]   
        147      187           _2f_Fbld_2f_Builtin_2f_Define_25__2e_VarImpl_21_.0019[0063X]   
**     1485     2443      222  FbleNewFuncValue[000cX] **

Functions are very evenly spread out. VarImpl captures cmd to check arg count
is correct. Equals is monadic bind code that captures the previous monadic
result. A lot of these look like functions allocated for monadic style coding
that capture the bind result.

Unions:
        137      187           _2f_Core_2f_List_25__2e_Cons_21_.0006[005cX]   
        372      537           _2f_Core_2f_Map_2f_Map_25__2e_MapP_21_.0030[0032X]   
        997     1313           _2f_Core_2f_Maybe_25__2e_Just_21_.0005[0045X]   
**     1688     2294      272  FbleNewUnionValue[0034X] **

The unions one is Maybe@ I think I saw before: HeadOf, TailOf returning
Result@<Maybe@<Char@>>. Does that make sense? Don't we expect Maybe@<Char@> to
fit in a packed value? It must be the Maybe@ from Result@, whatever value is
being returned.

You see the common theme in all these allocations? Result@ monad. Structs
because it is a struct. Functions for bind. Unions because it is a union. The
approach of monadic code is to allocate an object for every 'statement' in the
program. We execute a lot of statements, so we allocate a lot of objects.

In a language like C, we wouldn't have these glue allocations at all.

I would say all these allocations are probably short lived. Just look at the
implementation of Do:

<@ A@>(Result@<A@>)<@ B@>((A@) { Result@<B@>; }) { Result@<B@>; }
Do = <@ A@>(Result@<A@> ra)<@ B@>((A@) { Result@<B@>; } f) {
  ra.value.?(nothing: Raise<B@>(ra));

  Result@<B@> rb = f(ra.value.just);
  Result@(Append(ra.errors, rb.errors), Or(ra.failed, rb.failed), rb.value);
};

Call f, throw it a way. Take allocated Result@, access its fields and throw
away the wrapper. Check if value is nothing, then throw it away.

Maybe the question to ask is: can I have 0 allocation overhead to monadic
style programming? Is there something fundamental about information hiding
that makes monadic programming useful but need to make allocations?

If we were smart, could we realize that Result@ rb is no longer used, so we
can reuse that existing allocation for the return result? Instead of
allocating a new Result@ value?

If we inlined Do, could we see that we have the function and it's being used
only once, so we could turn it into a let and not allocate any function value
at all?

Let's try inlining Do.

  Markup@ a <- r.do(g(...));
  Markup@ b <- r.do(h(...));
  f(a, b)

  Do(g(...), (Markup@ a) {
    Do(h(...), (Markup@ b) {
      i(a, b);
    });
  });

When we inline a function, we end up with the construct 
func_apply (func_value arg)

This can be turned into a let. Let's inline it as a let for clarity.

  Result@ ra = g(...);
  (Markup@) { Result@; } f = (Markup@ a) {
    Result@ ra1 = h(...);
    (Markup@) { Result@; } f1 = (Markup@ b) {
      i(a, b);
    };
    ra1.value.?(nothing: Raise(ra1));

    Result@ rb1 = f1(ra1.value.just);
    Result@(Append(ra1.errors, rb1.errors), Or(ra1.failed, rb1.failed), rb1.value);
  };
  ra.value.?(nothing: Raise(ra));

  Result@ rb = f(ra.value.just);
  Result@(Append(ra.errors, rb.errors), Or(ra.failed, rb.failed), rb.value);

If we are smart, we can see that f and f1 are only used once. We can inline
them, and turn them back into lets.
    
  Result@ ra = g(...);
  ra.value.?(nothing: Raise(ra));

  Markup@ a = ra.value.just;
  Result@ ra1 = h(...);
  ra1.value.?(nothing: Raise(ra1));

  Markup@ b = ra1.value.just;
  Result@ rb1 = i(a, b);
  Result@ rb = Result@(Append(ra1.errors, rb1.errors), Or(ra1.failed, rb1.failed), rb1.value);
  Result@(Append(ra.errors, rb.errors), Or(ra.failed, rb.failed), rb.value);

Not surprisingly, if we get rid of use of bind and inline Do, we avoid the
need for the extra function allocations in the glue logic.

For the struct value, look at rb. We allocate a struct result, access its
fields, then otherwise no longer use the struct result. In that case, we could
turn the fields into individual values.

  Result@ ra = g(...);
  ra.value.?(nothing: Raise(ra));

  Markup@ a = ra.value.just;
  Result@ ra1 = h(...);
  ra1.value.?(nothing: Raise(ra1));

  Markup@ b = ra1.value.just;
  Result@ rb1 = i(a, b);
  List@ rb_errors = Append(ra1.errors, rb1.errors);
  Bool@ rb_failed = Or(ra1.failed, rb1.failed)
  Markup@ rb_value = rb1.value;
  Result@(Append(ra.errors, rb_errors), Or(ra.failed, rb_failed), rb_value);

I don't see an obvious way to get rid of the Maybe@ allocation. Those are
allocated inside g, h, i. We would have to inline those too. At some point
that starts to be excessive inlining.

What we observed:
* function allocations can be avoided by replacing with let if you know the
  args at time of function value allocation and its only used once.
* struct allocations can be avoided if you know all uses of its fields at time
  of allocation.

This 'inline and optimize' approach doesn't feel great to me though. It feels
fundamentally limited in terms of how much inlining you can do and how much
visibility you have into the functions you are inlining.

Any way we could make the allocations significantly cheaper? As in... allocate
on the stack and not the heap?

Let's take the discussion back to stack_alloc.txt. Time to revisit that again.

---

Double checking performance since upgrading OS and refactoring some things.

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null

User time (seconds): 99.81
Maximum resident set size (kbytes): 103300

The perf profile looks similar to before.

For the fun of it, let's look at md5 perf profile. See if there's anything
interesting there.

Self time:

18.37    30668  FbleStructValueField[000bX]
15.85    26460  PackedValueLength[0013X]
14.67    24494  FbleNewStructValue[000cX]
 8.39    14015  FbleCall[0001X]
 4.19     7001  FbleReleaseValues[0015X]
 3.10     5174  FbleFuncValueFunction[0017X]

Not surprising I guess. Lots of time spent packing and unpacking. Not as much
time in allocations. Most the allocations are coming from BitXY.Addx. Another
example of a short-lived struct being returned from a function.

For the record to measure against:

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5

User time (seconds): 49.54
Maximum resident set size (kbytes): 2448

We were hovering around 1m10s last time I ran this, so that's cool.

---

Random idea: what if we change the tail call protocol to return a
zero-argument function value? We Have some way to quickly check if that's what
we are doing. Maybe a zero-arg function value pointer with bit 1 set in the
pointer.

What we get from this:
* Avoid having to retain/release function and args. It uses AddRef instead.
* Avoid need for allocating a tail_call_buffer.
* Feels like it's more natural for work on switching to stack allocator for
  the heap.
* Simpler implementation of FbleCall, Call.

The cost:
* An extra partial function allocation on the heap for tail calls that we
  didn't need before.

I think it's worth a try, to see if it's cleaner and what impact, if any, it
has on performance. It feels like it should be cleaner. It's hard for me to
predict performance. I feel like it could be a small performance improvement
due to less complicated function calling code.

Let's try this. It's slightly involved.

Across the board the code is cleaner this way. I really hope it does go
noticeably slower, because I would love to keep this change.

First need to work through bugs.

* Error unwinding tail recursion.
* Memory leak in TailRecursion.

Those are the only ones so far (haven't implemented aarch64 backend yet).

The unwind must be the abort logic for tail recursion that I didn't implement
right. Yeah. That's fixed now.

Memory leak in TailRecursion?

Maybe it's not a real memory leak, it's just that we are allocating during the
call and we don't get a chance to GC soon enough?

No. It looks like a real memory leak. But a slow one. How could that be?

I bet it's because PartialApply uses FbleCall, which will then try to resolve
tail calls from within itself instead of being willing to return tail call
directly.

---

Seems to be a bug with debug info in the aarch64 back end. Otherwise things
appear to be working.

Let's check performance. I worry it's not good.

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.54 ==> 60.85
  Maximum resident set size (kbytes): 2448 ==> 22268

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 99.81 ==> 166.96
  Maximum resident set size (kbytes): 103300 ==> 148720

Yeah, not good. A lot more memory pressure. I assume perf profiling shows more
time in NewHeapObject, malloc, free and IncrGC. Maybe a little less time in
ReleaseValues. Yeah, that's what perf profiling shows.

That's too big a slowdown.

I wonder if we can keep the API cleanup somehow, but avoid the extra
allocation. Maybe get the best of both worlds.

The idea is to keep a global side buffer for passing tail call data to the
caller. One should be sufficient. Use that in place of tail_call_buffer.

We do retain/release on all the arguments again, but avoid the heap allocation
and call to InitGc. Hopefully it's like the previous implementation, except
the extra simplicity in FbleCall speeds things up.

yes | head -n 60000 | /usr/bin/time -v ./pkgs/md5/fble-md5
  User time (seconds): 49.54 ==> 60.85 ==> 50.98
  Maximum resident set size (kbytes): 2448 ==> 22268 ==> 22092

/usr/bin/time -v ./pkgs/fbld/bin/fbld ../fbld/nobuildstamp.fbld ./fbld/version.fbld ../fbld/html.fbld ../spec/fble.lib.fbld ../spec/fble.fbld > /dev/null
  User time (seconds): 99.81 ==> 166.96 ==> 98.60
  Maximum resident set size (kbytes): 103300 ==> 148720 ==> 133432

Looks like md5 is leaking memory though.

Performance of this approach is fine compared to before the change. That means
we can keep it and the nice cleanliness that goes with it.

But I need to figure out the issues first:

* debug test is failing.
* md5 appears to be leaking memory.
