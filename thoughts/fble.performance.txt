Fble Performance
================
Right now is pretty bad. How can we make things better?

Some things I notice from profiling:
* A lot of GC related things.
* A lot of call/return instruction related things.
* Note that we have profile arcs and no optimization turned on, though that
  seems to only be a 2x speedup.

It's a lot of little things being called a ton, rather than a few big calls.
That's a bit annoying, because it makes me want to do low level optimizations
that aren't very general, and it makes it hard to figure out what to focus on.

Remember the disadvantages we expect versus, say, C:
- no primitive types or operations
- automatic reference vs. value
- immutable data types
- automatic memory management
- closures that capture variables from local scope
- light weight multithreading

Add to that the fact that we are currently running an interpreter instead of
compiled code. The space invaders game looks to be running over 100x slower
than it needs to be. How can I get a factor of 100x improvement?

It's got to come from big, high level changes. I don't think it will come from
lots of little low level hacks. I don't want it to come from lots of little
low level hacks.

Some obvious things to try:
* Compile instead of interpret.
* Inline function calls.
* Have a variation of the language with zero cost abstraction, mutable data
  types, and explicit memory management.
* Try to do lots of tiny little cleanup/optimizations and see if they can add
  up anywhere.

---

Here's a proposal for a different programming language that I believe I could
implement efficiently, without garbage collection or function calls at run
time.

We have struct and union types as in fble, except non-recursive.
We have a reference type T@&, which is a reference type values of type T@.
We have a program.

You have the following operations in the program:

Access:
  Given a reference to a struct and a field, returns a reference to the field.
  S@& -> Field -> F@&

Assign:
  Given two references of the same type, copy the data in one reference to
  another.

  T@& -> T@& -> Prog@
Condition:
  Select a program to run based on the tag of a referenced union value.

  U@& -> Prog@, Prog@, ... Prog@ -> Prog@

Loop:
  Run a program repeatedly as long as the tag of a referenced union value is
  0.

  U@& -> Prog@ -> Prog@

Var:
  Create a reference to a variable of a given type, and use that in a given
  program.

  T@ -> Prog@ -> Prog@

Sequence:
  Given two programs, run one program after the other.

  Prog@ -> Prog@ -> Prog@

Parallel:
  Given two programs, run both programs in parallel and wait for them both to
  finish.

  Prog@ -> Prog@ -> Prog@

For multithreading, we may also want:

Swap:
  Given two references of the same type, swap their values. Possibly needed
  for multithreaded synchronization.

Block:
  Wait for a referenced value to have a given tag before continuing.


Why this can be done efficiently:
* There are no function calls, so no cost of call/return.
* The total amount of memory used by a program is constant. So we can
  pre-allocate all memory and don't need runtime GC.
* Non-recursive and non-sharing values mean we can pack values into bits for
  efficient copying.

We still have to support multithreading, but the number of threads is constant
and the burden of synchronization is on the programmer, so it shouldn't be too
hard I hope. Just round robin execute commands among the currently running
threads.

The key bits are: working with finite, non-shared values. Using constant
memory for the entire program. Anything unbounded has to be done broken up
into bits by time.

Now we have two paths we can pursue: My newly proposed language, or fble with
elaborate.

For fble elaborate to be useful, you practically have to limit yourself to
non-recursive values. I don't have a great story for loops though.

For my newly proposed language to be useful, we need programming level
abstractions (e.g. functions, polymorphic types, program modules).

If I truly want something to go fast, then I'll allow you to define primitive
program modules in different languages (e.g. C), and build apps on top of
those.

---

I am going to retire the efforts on fble elaborate. What I've learned from
fblf is that full inlining of all functions, even if they are finite, is going
to blow up code size far beyond what's practical. For elaborate to be useful,
then, it would have to be up to the implementation to choose how much gets
inlined. That's completely opaque to the developer. Then the question is what
is the spec for elaborate? You call it and it takes an arbitrary amount of
time to return a value that takes an arbitrary different amount of time to run
than the input? How can you do anything with that?

It had a good run, but I no longer think fble elaborate is the answer. I don't
see any reason not to keep the internal changes to the interpreter that we
made for symbolic elaboration, where a type checked expression is a value.
Unless it gets in the way.

---

I now compile fble instructions to c code, to hopefully avoid the overheads of
instruction dispatch. It doesn't look like that's the big bottleneck though.
The big bottleneck appears to be doing a lot of function calls, each of which
requires allocating a stack frame. And for every stack frame allocated we do
incremental gc, which means traversing more objects.

I bet, if we could avoid doing so many allocations for function calls, we
would spend much less time doing gc, which would improve performance a bunch.

---

The profile for fble bench says:
* 107,519,013 new objects allocated, of which 30,928,572, or almost 30% are
  FbleStackValues. That's more than the number of union values we allocate for
  the entire program. So, 108M total allocations, 31M stack values, 31M ref
  values (for the stack result), 26M union values, 16M struct values, and 4M
  function values.
* We call MarkRef 134M times. That's maybe 30% more than once per allocated
  object. MarkRef is the top entry on the profile. AddRef is second, IncrGc is
  third.

The point is, we are spending a lot of time doing GC, because we have a lot of
allocations, the majority of which are not even a visible part to the end
users program. Here's what I propose to fix this:

1. StackValue should not be an FbleValue. It should not have garbage
collection. It's not longer mixed up with thunk values, so I don't see any
issue with this.

2. StackValue's result should be stored as a pointer to something that expects
to receive a strong reference to the value. Not as a RefValue.

Assuming I haven't missed something that prevents (1) and (2) from working,
this will cut down number of gc object allocations by over 50%, which will cut
down gc by over 50%. Which should give, I would estimate a 10% improvement in
performance of the fble benchmark. In other words, from 5m45s to ... about
5m0s? Okay, doesn't feel like a lot. Especially when before the split of
ThunkValue into RefValue and StackValue we were at 5m10s. But on the flip
side, if you think adding 31M StackValue's slowed down the program from 5m10s
by a minute, then removing 31M would speed it up by a minute, leading to a
runtime around 4m10s, which is much better. Anyway, it's worth a shot.

Step 1: Confirm that the references to stack values we need are strong
references, and the only references to stack value results we need are strong
references.

Yeah. Looks like that's the case. So we'll want locals to be an array of
strongly retained values or NULL. We have to set it to NULL when we release
it, if we do. We'll have to release any time we overwrite a value.

Note: this effectively will undo a change we did to make stacks into gc values
in the first place. But I don't think things need to be as complicated as they
were before. We can track live values on the stack using NULL versus non-NULL.
Clean up values when we overwrite a non-NULL and when we free the stack. We
saw a 1 minute increase in fble bench time when we made that switch. I'm
hopeful we can see a 2 minute improvement in fble bench time when we switch
back.

Yes. As expected, we're down to a 4 minute runtime with this change.
